{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Fraud Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PaySim simulates mobile money transactions based on a sample of real transacions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world. The objective of the project is to predict if a transaction is fraudulent or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries: mathematical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# libraries: sklearn\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# libraries: pyspark SparkContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# libraries: pyspark sql\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from  pyspark.sql.functions import monotonically_increasing_id, desc, row_number\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# libraries: pyspark machine learning\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, DecisionTreeClassifier, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel, TrainValidationSplit\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, RFormula\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "\n",
    "# libraries: visualization\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as mpt\n",
    "import functools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 21:20:48 WARN Utils: Your hostname, Alexangels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.4 instead (on interface en0)\n",
      "24/05/24 21:20:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/24 21:20:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating a Spark-Context\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Builder\n",
    "\n",
    "spark = SparkSession.builder.appName(\"fraudDetection\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x16699b890>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "global df_bank, results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ll use PySpark to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the SparkSession\n",
    "\n",
    "##spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n",
    "##spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark dataframe \n",
    "\n",
    "df = spark.read.csv('fraudDetection.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 21:20:58 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll convert this \"df\" dataframe into a parquet file using the following method of pyspark. The file will be named \"fraudDetection.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 21:21:00 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/05/24 21:21:00 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/24 21:21:00 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/24 21:21:00 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/24 21:21:10 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/24 21:21:10 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/24 21:21:10 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet(\"fraudDetection.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll read the file as a parquet file. The calculation will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = spark.read.parquet(\"fraudDetection.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s take a look to the data with the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|   type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|  35|CASH_IN|312070.89| C154541954|     334944.3|     647015.19|C1995182035|     1030393.8|     718322.91|      0|             0|\n",
      "|  35|CASH_IN|244107.21|C1988196004|    647015.19|      891122.4| C877334652|     792091.74|     547984.53|      0|             0|\n",
      "|  35|CASH_IN| 17849.53|C1469762907|     891122.4|     908971.93| C733481207|     107400.33|       89550.8|      0|             0|\n",
      "|  35|CASH_IN|204719.93| C842268344|    908971.93|    1113691.86| C702268498|     531408.31|     326688.37|      0|             0|\n",
      "|  35|CASH_IN|281004.16| C188755315|   1113691.86|    1394696.02|C1358158097|     640496.15|     359491.99|      0|             0|\n",
      "|  35|CASH_IN| 16114.11|C1980321513|   1394696.02|    1410810.13| C834369930|    2642508.03|    2626393.92|      0|             0|\n",
      "|  35|CASH_IN|  65320.6|C2012783974|   1410810.13|    1476130.73|C1837613240|    3709177.65|    3643857.05|      0|             0|\n",
      "|  35|CASH_IN|197260.06|C1511215903|   1476130.73|    1673390.79|C1053726450|     389463.19|     192203.13|      0|             0|\n",
      "|  35|CASH_IN|  17589.2|C1345504923|   1673390.79|    1690979.99| C573946296|      76894.38|      59305.18|      0|             0|\n",
      "|  35|CASH_IN|225958.23|C1819325554|   1690979.99|    1916938.22|C1615795666|     997413.03|     771454.81|      0|             0|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: string (nullable = true)\n",
      " |-- newbalanceOrig: string (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: string (nullable = true)\n",
      " |-- newbalanceDest: string (nullable = true)\n",
      " |-- isFraud: string (nullable = true)\n",
      " |-- isFlaggedFraud: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 11 columns, none of them is numerical (they are categorical). Let´s count the number of registers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of registers is: 6362620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of registers is:\",df_bank_par.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have more than six miliions of transactions in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firstly, we´ll create a function to create a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.1.- creation of a new variable: type2\n",
    "\n",
    "df_type2 = df_bank_par.withColumn(\"type2\",f.concat(f.substring(\"nameOrig\",1,1),f.substring(\"nameDest\",1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+\n",
      "|step|   type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+\n",
      "|  35|CASH_IN|312070.89| C154541954|     334944.3|     647015.19|C1995182035|     1030393.8|     718322.91|      0|             0|   CC|\n",
      "|  35|CASH_IN|244107.21|C1988196004|    647015.19|      891122.4| C877334652|     792091.74|     547984.53|      0|             0|   CC|\n",
      "|  35|CASH_IN| 17849.53|C1469762907|     891122.4|     908971.93| C733481207|     107400.33|       89550.8|      0|             0|   CC|\n",
      "|  35|CASH_IN|204719.93| C842268344|    908971.93|    1113691.86| C702268498|     531408.31|     326688.37|      0|             0|   CC|\n",
      "|  35|CASH_IN|281004.16| C188755315|   1113691.86|    1394696.02|C1358158097|     640496.15|     359491.99|      0|             0|   CC|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_type2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve created a new column named \"type2\" which is composed by the first character of the column \"nameOrig\" and the first character of the column \"nameDest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+\n",
      "|step|   type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+\n",
      "|  35|CASH_IN|312070.89| C154541954|     334944.3|     647015.19|C1995182035|     1030393.8|     718322.91|      0|             0|   CC|\n",
      "|  35|CASH_IN|244107.21|C1988196004|    647015.19|      891122.4| C877334652|     792091.74|     547984.53|      0|             0|   CC|\n",
      "|  35|CASH_IN| 17849.53|C1469762907|     891122.4|     908971.93| C733481207|     107400.33|       89550.8|      0|             0|   CC|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 2.1.2.1.- One Hot Encoding: column \"type\"\n",
    "\n",
    "df_type2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ll use some libraries of Spark for Machine Learning (SparkML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### StringIndexer Initialization\n",
    "### column: type\n",
    "\n",
    "indexer_type = StringIndexer(inputCol=\"type\",outputCol=\"types_indexed\")\n",
    "indexerModel_type = indexer_type.fit(df_type2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+\n",
      "|step|   type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|types_indexed|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+\n",
      "|  35|CASH_IN|312070.89| C154541954|     334944.3|     647015.19|C1995182035|     1030393.8|     718322.91|      0|             0|   CC|          2.0|\n",
      "|  35|CASH_IN|244107.21|C1988196004|    647015.19|      891122.4| C877334652|     792091.74|     547984.53|      0|             0|   CC|          2.0|\n",
      "|  35|CASH_IN| 17849.53|C1469762907|     891122.4|     908971.93| C733481207|     107400.33|       89550.8|      0|             0|   CC|          2.0|\n",
      "|  35|CASH_IN|204719.93| C842268344|    908971.93|    1113691.86| C702268498|     531408.31|     326688.37|      0|             0|   CC|          2.0|\n",
      "|  35|CASH_IN|281004.16| C188755315|   1113691.86|    1394696.02|C1358158097|     640496.15|     359491.99|      0|             0|   CC|          2.0|\n",
      "|  35|CASH_IN| 16114.11|C1980321513|   1394696.02|    1410810.13| C834369930|    2642508.03|    2626393.92|      0|             0|   CC|          2.0|\n",
      "|  35|CASH_IN|  65320.6|C2012783974|   1410810.13|    1476130.73|C1837613240|    3709177.65|    3643857.05|      0|             0|   CC|          2.0|\n",
      "|  35|CASH_IN|197260.06|C1511215903|   1476130.73|    1673390.79|C1053726450|     389463.19|     192203.13|      0|             0|   CC|          2.0|\n",
      "|  35|CASH_IN|  17589.2|C1345504923|   1673390.79|    1690979.99| C573946296|      76894.38|      59305.18|      0|             0|   CC|          2.0|\n",
      "|  35|CASH_IN|225958.23|C1819325554|   1690979.99|    1916938.22|C1615795666|     997413.03|     771454.81|      0|             0|   CC|          2.0|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Transform the DataFrame using the fitted StringIndexer model\n",
    "\n",
    "indexed_df_type2 = indexerModel_type.transform(df_type2)\n",
    "indexed_df_type2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we´ve set each of the elements of the \"type\" column into indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+\n",
      "|step|type    |amount   |nameOrig   |oldbalanceOrg|newbalanceOrig|nameDest   |oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|types_indexed|types_onehot |\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+\n",
      "|35  |CASH_IN |312070.89|C154541954 |334944.3     |647015.19     |C1995182035|1030393.8     |718322.91     |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_IN |244107.21|C1988196004|647015.19    |891122.4      |C877334652 |792091.74     |547984.53     |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_IN |17849.53 |C1469762907|891122.4     |908971.93     |C733481207 |107400.33     |89550.8       |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_IN |204719.93|C842268344 |908971.93    |1113691.86    |C702268498 |531408.31     |326688.37     |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_IN |281004.16|C188755315 |1113691.86   |1394696.02    |C1358158097|640496.15     |359491.99     |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_IN |16114.11 |C1980321513|1394696.02   |1410810.13    |C834369930 |2642508.03    |2626393.92    |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_IN |65320.6  |C2012783974|1410810.13   |1476130.73    |C1837613240|3709177.65    |3643857.05    |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_IN |197260.06|C1511215903|1476130.73   |1673390.79    |C1053726450|389463.19     |192203.13     |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_IN |17589.2  |C1345504923|1673390.79   |1690979.99    |C573946296 |76894.38      |59305.18      |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_IN |225958.23|C1819325554|1690979.99   |1916938.22    |C1615795666|997413.03     |771454.81     |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_IN |137987.66|C1578237711|10889.0      |148876.66     |C1395949792|421743.44     |283755.78     |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |PAYMENT |2550.67  |C509265017 |148876.66    |146325.99     |M1486544695|0.0           |0.0           |0      |0             |CM   |1.0          |(5,[1],[1.0])|\n",
      "|35  |PAYMENT |10807.74 |C832582514 |153908.0     |143100.26     |M1612209102|0.0           |0.0           |0      |0             |CM   |1.0          |(5,[1],[1.0])|\n",
      "|35  |PAYMENT |788.2    |C682173112 |23862.0      |23073.8       |M650392573 |0.0           |0.0           |0      |0             |CM   |1.0          |(5,[1],[1.0])|\n",
      "|35  |CASH_IN |26016.33 |C382713844 |136678.0     |162694.33     |C1441810518|80955.03      |54938.7       |0      |0             |CC   |2.0          |(5,[2],[1.0])|\n",
      "|35  |CASH_OUT|176914.1 |C883638472 |17796.0      |0.0           |C1693177571|487562.09     |664476.19     |0      |0             |CC   |0.0          |(5,[0],[1.0])|\n",
      "|35  |PAYMENT |13175.49 |C1442191367|62135.0      |48959.51      |M1378517663|0.0           |0.0           |0      |0             |CM   |1.0          |(5,[1],[1.0])|\n",
      "|35  |TRANSFER|101701.01|C972138289 |19988.0      |0.0           |C1485034389|837516.47     |939217.48     |0      |0             |CC   |3.0          |(5,[3],[1.0])|\n",
      "|35  |CASH_OUT|146052.99|C1462902215|0.0          |0.0           |C1935757345|1121971.73    |1268024.72    |0      |0             |CC   |0.0          |(5,[0],[1.0])|\n",
      "|35  |CASH_OUT|104230.91|C845222150 |495295.0     |391064.09     |C2049813033|9392809.57    |9497040.48    |0      |0             |CC   |0.0          |(5,[0],[1.0])|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### apply One-Hot-Encoding to the indexed column, that is, \n",
    "### \"types_indexed\"\n",
    "\n",
    "encoder_type = OneHotEncoder(dropLast=False, inputCol=\"types_indexed\", outputCol=\"types_onehot\")\n",
    "encoder_type_df = encoder_type.fit(indexed_df_type2).transform(indexed_df_type2)\n",
    "encoder_type_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: string (nullable = true)\n",
      " |-- newbalanceOrig: string (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: string (nullable = true)\n",
      " |-- newbalanceDest: string (nullable = true)\n",
      " |-- isFraud: string (nullable = true)\n",
      " |-- isFlaggedFraud: string (nullable = true)\n",
      " |-- type2: string (nullable = true)\n",
      " |-- types_indexed: double (nullable = false)\n",
      " |-- types_onehot: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_type_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+\n",
      "|step|   type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|types_indexed| types_onehot|  types_onehot_split|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+\n",
      "|  35|CASH_IN|312070.89| C154541954|     334944.3|     647015.19|C1995182035|     1030393.8|     718322.91|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|\n",
      "|  35|CASH_IN|244107.21|C1988196004|    647015.19|      891122.4| C877334652|     792091.74|     547984.53|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|\n",
      "|  35|CASH_IN| 17849.53|C1469762907|     891122.4|     908971.93| C733481207|     107400.33|       89550.8|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|\n",
      "|  35|CASH_IN|204719.93| C842268344|    908971.93|    1113691.86| C702268498|     531408.31|     326688.37|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|\n",
      "|  35|CASH_IN|281004.16| C188755315|   1113691.86|    1394696.02|C1358158097|     640496.15|     359491.99|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_type_df_split = encoder_type_df.select('*',vector_to_array('types_onehot').alias('types_onehot_split'))\n",
    "encoder_type_df_split.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now, we´ll split the \"types_onehot_split\" into five columns, one per category\n",
    "\n",
    "num_categories = len(encoder_type_df_split.first()['types_onehot_split'])\n",
    "cols_expanded = [(f.col('types_onehot_split')[i].alias(f\"{indexerModel_type.labels[i]}\")) for i in range(num_categories)]\n",
    "type_df = encoder_type_df_split.select('*',*cols_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+\n",
      "|step|    type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|types_indexed| types_onehot|  types_onehot_split|CASH_OUT|PAYMENT|CASH_IN|TRANSFER|DEBIT|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+\n",
      "|  35| CASH_IN|312070.89| C154541954|     334944.3|     647015.19|C1995182035|     1030393.8|     718322.91|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|244107.21|C1988196004|    647015.19|      891122.4| C877334652|     792091.74|     547984.53|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 17849.53|C1469762907|     891122.4|     908971.93| C733481207|     107400.33|       89550.8|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|204719.93| C842268344|    908971.93|    1113691.86| C702268498|     531408.31|     326688.37|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|281004.16| C188755315|   1113691.86|    1394696.02|C1358158097|     640496.15|     359491.99|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 16114.11|C1980321513|   1394696.02|    1410810.13| C834369930|    2642508.03|    2626393.92|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|  65320.6|C2012783974|   1410810.13|    1476130.73|C1837613240|    3709177.65|    3643857.05|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|197260.06|C1511215903|   1476130.73|    1673390.79|C1053726450|     389463.19|     192203.13|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|  17589.2|C1345504923|   1673390.79|    1690979.99| C573946296|      76894.38|      59305.18|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|225958.23|C1819325554|   1690979.99|    1916938.22|C1615795666|     997413.03|     771454.81|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|137987.66|C1578237711|      10889.0|     148876.66|C1395949792|     421743.44|     283755.78|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  2550.67| C509265017|    148876.66|     146325.99|M1486544695|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 10807.74| C832582514|     153908.0|     143100.26|M1612209102|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|    788.2| C682173112|      23862.0|       23073.8| M650392573|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 26016.33| C382713844|     136678.0|     162694.33|C1441810518|      80955.03|       54938.7|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35|CASH_OUT| 176914.1| C883638472|      17796.0|           0.0|C1693177571|     487562.09|     664476.19|      0|             0|   CC|          0.0|(5,[0],[1.0])|[1.0, 0.0, 0.0, 0...|     1.0|    0.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 13175.49|C1442191367|      62135.0|      48959.51|M1378517663|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35|TRANSFER|101701.01| C972138289|      19988.0|           0.0|C1485034389|     837516.47|     939217.48|      0|             0|   CC|          3.0|(5,[3],[1.0])|[0.0, 0.0, 0.0, 1...|     0.0|    0.0|    0.0|     1.0|  0.0|\n",
      "|  35|CASH_OUT|146052.99|C1462902215|          0.0|           0.0|C1935757345|    1121971.73|    1268024.72|      0|             0|   CC|          0.0|(5,[0],[1.0])|[1.0, 0.0, 0.0, 0...|     1.0|    0.0|    0.0|     0.0|  0.0|\n",
      "|  35|CASH_OUT|104230.91| C845222150|     495295.0|     391064.09|C2049813033|    9392809.57|    9497040.48|      0|             0|   CC|          0.0|(5,[0],[1.0])|[1.0, 0.0, 0.0, 0...|     1.0|    0.0|    0.0|     0.0|  0.0|\n",
      "|  35|CASH_OUT|110251.46| C878265263|        538.0|           0.0|C1430463714|    4203595.49|    4313846.96|      0|             0|   CC|          0.0|(5,[0],[1.0])|[1.0, 0.0, 0.0, 0...|     1.0|    0.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 18766.09|  C10859066|     159924.0|     141157.91|M1195269068|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35|CASH_OUT|359468.59|C2104331020|      55496.0|           0.0|C1838303932|    2204689.84|    2564158.43|      0|             0|   CC|          0.0|(5,[0],[1.0])|[1.0, 0.0, 0.0, 0...|     1.0|    0.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  3925.85|  C56097589|          0.0|           0.0| M785296647|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 45092.98| C454779689|          0.0|           0.0| M171945645|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 23537.84| C906846860|          0.0|           0.0|M1836079071|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  7144.25| C758276979|          0.0|           0.0|M1510996670|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  20442.1| C122419825|          0.0|           0.0|M1760201704|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  9306.54|C1459438717|          0.0|           0.0|M1937935894|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  4643.75| C734161509|          0.0|           0.0| M399180094|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 16004.48| C724136214|          0.0|           0.0|M1287436379|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  4945.76|C1955947243|          0.0|           0.0| M991449014|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 16230.65|C1540962306|          0.0|           0.0|M1913535810|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  5439.43| C768838610|          0.0|           0.0|M1422857249|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|   851.46|C1187262354|          0.0|           0.0|M1159982629|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|   9482.8|C1948217416|          0.0|           0.0|M1482497270|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  3539.99|C1065484496|          0.0|           0.0|M1703576515|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  1124.47|C1779970570|          0.0|           0.0|M1519532307|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  2676.64|C1975068177|          0.0|           0.0|M1151084542|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  1763.35|C1866454277|          0.0|           0.0| M607007568|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  2568.07| C404422762|          0.0|           0.0|M1033729856|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 14780.42| C739973643|          0.0|           0.0|M2038044779|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  5695.76| C966938494|          0.0|           0.0|M1867716733|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 21428.57| C460098782|          0.0|           0.0| M855406232|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 88348.49| C420353542|      51985.0|     140333.49| C126978053|    1451025.04|    1362676.55|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 14321.49|C1860793770|      15538.0|       1216.51| M295440085|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35|CASH_OUT|161270.26|C1074131656|     165947.0|       4676.74| C247803393|           0.0|     161270.26|      0|             0|   CC|          0.0|(5,[0],[1.0])|[1.0, 0.0, 0.0, 0...|     1.0|    0.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT| 14496.64| C701657832|      4676.74|           0.0| M213059165|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  3901.81|C2108185528|          0.0|           0.0| M315913591|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 151679.7| C682299639|      45544.0|      197223.7|C1674217439|           0.0|           0.0|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35|CASH_OUT| 90960.33|C1010723409|      25561.0|           0.0| C287560801|     2623310.3|    2714270.63|      0|             0|   CC|          0.0|(5,[0],[1.0])|[1.0, 0.0, 0.0, 0...|     1.0|    0.0|    0.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|258137.47|  C57971701|       5255.0|     263392.47| C909295153| 2.195781352E7| 2.169967605E7|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  1210.05| C842674465|    263392.47|     262182.42|M1122411321|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35|CASH_OUT| 35684.14| C231024086|      15450.0|           0.0|C2006795469|      94890.58|     130574.71|      0|             0|   CC|          0.0|(5,[0],[1.0])|[1.0, 0.0, 0.0, 0...|     1.0|    0.0|    0.0|     0.0|  0.0|\n",
      "|  35|CASH_OUT| 85472.06| C705295520|       5029.0|           0.0| C876373474|      18621.61|     104093.67|      0|             0|   CC|          0.0|(5,[0],[1.0])|[1.0, 0.0, 0.0, 0...|     1.0|    0.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  17003.5|C1238989817|      21621.0|        4617.5|M1045554832|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35|CASH_OUT| 62553.07| C446603033|     427235.0|     364681.93| C559154038|      97402.49|     159955.56|      0|             0|   CC|          0.0|(5,[0],[1.0])|[1.0, 0.0, 0.0, 0...|     1.0|    0.0|    0.0|     0.0|  0.0|\n",
      "|  35| PAYMENT|  19199.2|C2136458270|      12868.0|           0.0|M2111012542|           0.0|           0.0|      0|             0|   CM|          1.0|(5,[1],[1.0])|[0.0, 1.0, 0.0, 0...|     0.0|    1.0|    0.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|186057.66|C1275552498|          0.0|     186057.66| C143770259|     485805.55|      299747.9|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|232993.44| C579820092|    186057.66|     419051.09|C1327728482|     766632.66|     533639.23|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|185319.59|C1664683335|    419051.09|     604370.68|C1936074663|     188247.06|       2927.47|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|151446.99|  C91859216|    604370.68|     755817.67|C1572164601|    1841949.24|    1690502.25|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 59167.78|C1914588095|    755817.67|     814985.46|C1187717512|     152302.59|      93134.81|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|266814.92| C714645882|    814985.46|    1081800.38| C972381775|     526796.02|      259981.1|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 49011.95|  C23122523|   1081800.38|    1130812.32| C564802404|       53516.0|       4504.05|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|104665.53|C1234815566|   1130812.32|    1235477.86|C1508400096|     169325.93|       64660.4|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|316099.38|C1230644081|   1235477.86|    1551577.24| C258311078|     931992.12|     615892.74|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 21812.65| C648464465|   1551577.24|    1573389.89|C1315841811|    1242690.59|    1220877.95|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 14803.07| C559101140|   1573389.89|    1588192.95|C2026644517|     531240.67|      516437.6|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|133054.86| C647971082|   1588192.95|    1721247.81|C2099893482|    1718158.41|    1585103.55|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 82892.91|C1044819715|   1721247.81|    1804140.73| C903798224|     251647.68|     168754.77|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|103179.07| C783424730|   1804140.73|     1907319.8|C1372520401|    6149572.93|    6046393.86|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|135319.76| C385837132|    1907319.8|    2042639.55|C1139004975|     4927681.8|    4792362.04|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|121452.35|C1253371016|   2042639.55|     2164091.9|C1857182924|     455692.55|      334240.2|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 50691.72|C1090616426|    2164091.9|    2214783.62| C344245633|     355018.35|     304326.62|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 12339.37|C2137779292|   2214783.62|    2227122.99| C677321856|     193503.01|     181163.64|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 89613.42| C305878773|   2227122.99|    2316736.41|C1299864770|    1070057.27|     980443.86|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|312547.65|C2139502742|   2316736.41|    2629284.06|   C8857666|     960227.65|      647680.0|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|399235.26| C892266784|   2629284.06|    3028519.32| C176304417|     958081.34|     558846.08|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 89457.97| C352844501|   3028519.32|    3117977.29|C1787556170|    4178304.13|    4088846.15|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|290786.32|C1573100158|   3117977.29|    3408763.62|   C7347113|    2317610.95|    2026824.63|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|151698.45| C323963864|   3408763.62|    3560462.07|C1433756384|    1031949.48|     880251.03|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 91179.46|C1374340984|   3560462.07|    3651641.53| C834808790| 2.942981751E7| 2.933863804E7|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 12679.87|C2073124546|   3651641.53|     3664321.4|C1653041616|    1093193.39|    1080513.53|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|209488.22| C977284004|    3664321.4|    3873809.62| C640672953|     446934.85|     237446.63|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|259319.98|C1396221631|   3873809.62|     4133129.6|C1311515272|    8843196.67|    8583876.69|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|430290.47|C1416522407|    4133129.6|    4563420.07| C287588933|    2082936.04|    1652645.57|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 329477.2| C637028774|   4563420.07|    4892897.27| C294951184|    1840732.26|    1511255.06|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 49834.77| C509496246|   4892897.27|    4942732.04| C690821844|     655673.96|     605839.19|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|252039.65| C138746780|   4942732.04|    5194771.69|  C23017640|     561134.75|      309095.1|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|159848.58| C794391393|   5194771.69|    5354620.27|C1838171162|    1005561.72|     845713.14|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|  9741.07|C1217116757|   5354620.27|    5364361.34| C273761368|     308781.26|     299040.19|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|392270.58| C610552585|   5364361.34|    5756631.92|C1778890613|    1912997.99|    1520727.41|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|150327.73|  C81608162|   5756631.92|    5906959.65|C1094277319|     398534.45|      33134.92|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|106669.02|C1627100259|   5906959.65|    6013628.67| C720252194|    2517317.66|    2410648.64|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|319952.85|C1974981435|   6013628.67|    6333581.52| C800643630|    1012320.29|     692367.43|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|  7934.79|C1930289799|   6333581.52|    6341516.31|C1308979018|     609134.15|     601199.36|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|249733.01| C508748585|   6341516.31|    6591249.33|C1699046512|     648556.05|     398823.04|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN| 130319.6| C582362124|   6591249.33|    6721568.93| C576654614|    1661242.15|    1530922.55|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35| CASH_IN|382528.38|C1767048554|   6721568.93|    7104097.31|C1434179268|     848230.83|      477634.3|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "type_df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve applied One-Hot-Encoding to the column \"type\" resulting in five new columns:\n",
    "+ CASH_OUT\n",
    "+ CASH_IN\n",
    "+ PAYMENT\n",
    "+ TRANSFER \n",
    "+ DEBIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll apply this procedure to the column \"type2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+\n",
      "|step|   type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|types_indexed| types_onehot|  types_onehot_split|CASH_OUT|PAYMENT|CASH_IN|TRANSFER|DEBIT|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+\n",
      "|  35|CASH_IN|312070.89| C154541954|     334944.3|     647015.19|C1995182035|     1030393.8|     718322.91|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35|CASH_IN|244107.21|C1988196004|    647015.19|      891122.4| C877334652|     792091.74|     547984.53|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35|CASH_IN| 17849.53|C1469762907|     891122.4|     908971.93| C733481207|     107400.33|       89550.8|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35|CASH_IN|204719.93| C842268344|    908971.93|    1113691.86| C702268498|     531408.31|     326688.37|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "|  35|CASH_IN|281004.16| C188755315|   1113691.86|    1394696.02|C1358158097|     640496.15|     359491.99|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 2.1.2.2.- One Hot Encoding: column \"type2\"\n",
    "\n",
    "type_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### StringIndexer Initialization\n",
    "### column: type2\n",
    "\n",
    "indexer_type = StringIndexer(inputCol=\"type2\",outputCol=\"types_indexed2\")\n",
    "indexerModel_type = indexer_type.fit(type_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+--------------+\n",
      "|step|   type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|types_indexed| types_onehot|  types_onehot_split|CASH_OUT|PAYMENT|CASH_IN|TRANSFER|DEBIT|types_indexed2|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+--------------+\n",
      "|  35|CASH_IN|312070.89| C154541954|     334944.3|     647015.19|C1995182035|     1030393.8|     718322.91|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|\n",
      "|  35|CASH_IN|244107.21|C1988196004|    647015.19|      891122.4| C877334652|     792091.74|     547984.53|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|\n",
      "|  35|CASH_IN| 17849.53|C1469762907|     891122.4|     908971.93| C733481207|     107400.33|       89550.8|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|\n",
      "|  35|CASH_IN|204719.93| C842268344|    908971.93|    1113691.86| C702268498|     531408.31|     326688.37|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|\n",
      "|  35|CASH_IN|281004.16| C188755315|   1113691.86|    1394696.02|C1358158097|     640496.15|     359491.99|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|\n",
      "|  35|CASH_IN| 16114.11|C1980321513|   1394696.02|    1410810.13| C834369930|    2642508.03|    2626393.92|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|\n",
      "|  35|CASH_IN|  65320.6|C2012783974|   1410810.13|    1476130.73|C1837613240|    3709177.65|    3643857.05|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|\n",
      "|  35|CASH_IN|197260.06|C1511215903|   1476130.73|    1673390.79|C1053726450|     389463.19|     192203.13|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|\n",
      "|  35|CASH_IN|  17589.2|C1345504923|   1673390.79|    1690979.99| C573946296|      76894.38|      59305.18|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|\n",
      "|  35|CASH_IN|225958.23|C1819325554|   1690979.99|    1916938.22|C1615795666|     997413.03|     771454.81|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Transform the DataFrame using the fitted StringIndexer model\n",
    "\n",
    "indexed_df_type = indexerModel_type.transform(type_df)\n",
    "indexed_df_type.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+-------------------------+--------+-------+-------+--------+-----+--------------+-------------+\n",
      "|step|type    |amount   |nameOrig   |oldbalanceOrg|newbalanceOrig|nameDest   |oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|types_indexed|types_onehot |types_onehot_split       |CASH_OUT|PAYMENT|CASH_IN|TRANSFER|DEBIT|types_indexed2|types_onehot2|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+-------------------------+--------+-------+-------+--------+-----+--------------+-------------+\n",
      "|35  |CASH_IN |312070.89|C154541954 |334944.3     |647015.19     |C1995182035|1030393.8     |718322.91     |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_IN |244107.21|C1988196004|647015.19    |891122.4      |C877334652 |792091.74     |547984.53     |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_IN |17849.53 |C1469762907|891122.4     |908971.93     |C733481207 |107400.33     |89550.8       |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_IN |204719.93|C842268344 |908971.93    |1113691.86    |C702268498 |531408.31     |326688.37     |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_IN |281004.16|C188755315 |1113691.86   |1394696.02    |C1358158097|640496.15     |359491.99     |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_IN |16114.11 |C1980321513|1394696.02   |1410810.13    |C834369930 |2642508.03    |2626393.92    |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_IN |65320.6  |C2012783974|1410810.13   |1476130.73    |C1837613240|3709177.65    |3643857.05    |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_IN |197260.06|C1511215903|1476130.73   |1673390.79    |C1053726450|389463.19     |192203.13     |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_IN |17589.2  |C1345504923|1673390.79   |1690979.99    |C573946296 |76894.38      |59305.18      |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_IN |225958.23|C1819325554|1690979.99   |1916938.22    |C1615795666|997413.03     |771454.81     |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_IN |137987.66|C1578237711|10889.0      |148876.66     |C1395949792|421743.44     |283755.78     |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |PAYMENT |2550.67  |C509265017 |148876.66    |146325.99     |M1486544695|0.0           |0.0           |0      |0             |CM   |1.0          |(5,[1],[1.0])|[0.0, 1.0, 0.0, 0.0, 0.0]|0.0     |1.0    |0.0    |0.0     |0.0  |1.0           |(2,[1],[1.0])|\n",
      "|35  |PAYMENT |10807.74 |C832582514 |153908.0     |143100.26     |M1612209102|0.0           |0.0           |0      |0             |CM   |1.0          |(5,[1],[1.0])|[0.0, 1.0, 0.0, 0.0, 0.0]|0.0     |1.0    |0.0    |0.0     |0.0  |1.0           |(2,[1],[1.0])|\n",
      "|35  |PAYMENT |788.2    |C682173112 |23862.0      |23073.8       |M650392573 |0.0           |0.0           |0      |0             |CM   |1.0          |(5,[1],[1.0])|[0.0, 1.0, 0.0, 0.0, 0.0]|0.0     |1.0    |0.0    |0.0     |0.0  |1.0           |(2,[1],[1.0])|\n",
      "|35  |CASH_IN |26016.33 |C382713844 |136678.0     |162694.33     |C1441810518|80955.03      |54938.7       |0      |0             |CC   |2.0          |(5,[2],[1.0])|[0.0, 0.0, 1.0, 0.0, 0.0]|0.0     |0.0    |1.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_OUT|176914.1 |C883638472 |17796.0      |0.0           |C1693177571|487562.09     |664476.19     |0      |0             |CC   |0.0          |(5,[0],[1.0])|[1.0, 0.0, 0.0, 0.0, 0.0]|1.0     |0.0    |0.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |PAYMENT |13175.49 |C1442191367|62135.0      |48959.51      |M1378517663|0.0           |0.0           |0      |0             |CM   |1.0          |(5,[1],[1.0])|[0.0, 1.0, 0.0, 0.0, 0.0]|0.0     |1.0    |0.0    |0.0     |0.0  |1.0           |(2,[1],[1.0])|\n",
      "|35  |TRANSFER|101701.01|C972138289 |19988.0      |0.0           |C1485034389|837516.47     |939217.48     |0      |0             |CC   |3.0          |(5,[3],[1.0])|[0.0, 0.0, 0.0, 1.0, 0.0]|0.0     |0.0    |0.0    |1.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_OUT|146052.99|C1462902215|0.0          |0.0           |C1935757345|1121971.73    |1268024.72    |0      |0             |CC   |0.0          |(5,[0],[1.0])|[1.0, 0.0, 0.0, 0.0, 0.0]|1.0     |0.0    |0.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "|35  |CASH_OUT|104230.91|C845222150 |495295.0     |391064.09     |C2049813033|9392809.57    |9497040.48    |0      |0             |CC   |0.0          |(5,[0],[1.0])|[1.0, 0.0, 0.0, 0.0, 0.0]|1.0     |0.0    |0.0    |0.0     |0.0  |0.0           |(2,[0],[1.0])|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+-------------------------+--------+-------+-------+--------+-----+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### apply One-Hot-Encoding to the indexed column, that is, \n",
    "### \"types_indexed2\"\n",
    "\n",
    "encoder_type2 = OneHotEncoder(dropLast=False, inputCol=\"types_indexed2\", outputCol=\"types_onehot2\")\n",
    "encoder_type2_df = encoder_type2.fit(indexed_df_type).transform(indexed_df_type)\n",
    "encoder_type2_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: string (nullable = true)\n",
      " |-- newbalanceOrig: string (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: string (nullable = true)\n",
      " |-- newbalanceDest: string (nullable = true)\n",
      " |-- isFraud: string (nullable = true)\n",
      " |-- isFlaggedFraud: string (nullable = true)\n",
      " |-- type2: string (nullable = true)\n",
      " |-- types_indexed: double (nullable = false)\n",
      " |-- types_onehot: vector (nullable = true)\n",
      " |-- types_onehot_split: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- CASH_OUT: double (nullable = true)\n",
      " |-- PAYMENT: double (nullable = true)\n",
      " |-- CASH_IN: double (nullable = true)\n",
      " |-- TRANSFER: double (nullable = true)\n",
      " |-- DEBIT: double (nullable = true)\n",
      " |-- types_indexed2: double (nullable = false)\n",
      " |-- types_onehot2: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_type2_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+--------------+-------------+-------------------+\n",
      "|step|   type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|types_indexed| types_onehot|  types_onehot_split|CASH_OUT|PAYMENT|CASH_IN|TRANSFER|DEBIT|types_indexed2|types_onehot2|types_onehot_split2|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+--------------+-------------+-------------------+\n",
      "|  35|CASH_IN|312070.89| C154541954|     334944.3|     647015.19|C1995182035|     1030393.8|     718322.91|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|(2,[0],[1.0])|         [1.0, 0.0]|\n",
      "|  35|CASH_IN|244107.21|C1988196004|    647015.19|      891122.4| C877334652|     792091.74|     547984.53|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|(2,[0],[1.0])|         [1.0, 0.0]|\n",
      "|  35|CASH_IN| 17849.53|C1469762907|     891122.4|     908971.93| C733481207|     107400.33|       89550.8|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|(2,[0],[1.0])|         [1.0, 0.0]|\n",
      "|  35|CASH_IN|204719.93| C842268344|    908971.93|    1113691.86| C702268498|     531408.31|     326688.37|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|(2,[0],[1.0])|         [1.0, 0.0]|\n",
      "|  35|CASH_IN|281004.16| C188755315|   1113691.86|    1394696.02|C1358158097|     640496.15|     359491.99|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|(2,[0],[1.0])|         [1.0, 0.0]|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+--------------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_type2_df_split = encoder_type2_df.select('*',vector_to_array('types_onehot2').alias('types_onehot_split2'))\n",
    "encoder_type2_df_split.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now, we´ll split the \"types_onehot_split2\" into two columns, one per category\n",
    "\n",
    "num_categories = len(encoder_type2_df_split.first()['types_onehot_split2'])\n",
    "cols_expanded = [(f.col('types_onehot_split2')[i].alias(f\"{indexerModel_type.labels[i]}\")) for i in range(num_categories)]\n",
    "encoder_type2_df_split = encoder_type2_df_split.select('*',*cols_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+--------------+-------------+-------------------+---+---+\n",
      "|step|   type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|type2|types_indexed| types_onehot|  types_onehot_split|CASH_OUT|PAYMENT|CASH_IN|TRANSFER|DEBIT|types_indexed2|types_onehot2|types_onehot_split2| CC| CM|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+--------------+-------------+-------------------+---+---+\n",
      "|  35|CASH_IN|312070.89| C154541954|     334944.3|     647015.19|C1995182035|     1030393.8|     718322.91|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|(2,[0],[1.0])|         [1.0, 0.0]|1.0|0.0|\n",
      "|  35|CASH_IN|244107.21|C1988196004|    647015.19|      891122.4| C877334652|     792091.74|     547984.53|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|(2,[0],[1.0])|         [1.0, 0.0]|1.0|0.0|\n",
      "|  35|CASH_IN| 17849.53|C1469762907|     891122.4|     908971.93| C733481207|     107400.33|       89550.8|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|(2,[0],[1.0])|         [1.0, 0.0]|1.0|0.0|\n",
      "|  35|CASH_IN|204719.93| C842268344|    908971.93|    1113691.86| C702268498|     531408.31|     326688.37|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|(2,[0],[1.0])|         [1.0, 0.0]|1.0|0.0|\n",
      "|  35|CASH_IN|281004.16| C188755315|   1113691.86|    1394696.02|C1358158097|     640496.15|     359491.99|      0|             0|   CC|          2.0|(5,[2],[1.0])|[0.0, 0.0, 1.0, 0...|     0.0|    0.0|    1.0|     0.0|  0.0|           0.0|(2,[0],[1.0])|         [1.0, 0.0]|1.0|0.0|\n",
      "+----+-------+---------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+-----+-------------+-------------+--------------------+--------+-------+-------+--------+-----+--------------+-------------+-------------------+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_type2_df_split.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve split the \"type2\" column into two columns based on One-Hot-Encoding. Now, we´ll eliminate some unnecessaruy columns. Let´s check out all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: string (nullable = true)\n",
      " |-- newbalanceOrig: string (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: string (nullable = true)\n",
      " |-- newbalanceDest: string (nullable = true)\n",
      " |-- isFraud: string (nullable = true)\n",
      " |-- isFlaggedFraud: string (nullable = true)\n",
      " |-- type2: string (nullable = true)\n",
      " |-- types_indexed: double (nullable = false)\n",
      " |-- types_onehot: vector (nullable = true)\n",
      " |-- types_onehot_split: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- CASH_OUT: double (nullable = true)\n",
      " |-- PAYMENT: double (nullable = true)\n",
      " |-- CASH_IN: double (nullable = true)\n",
      " |-- TRANSFER: double (nullable = true)\n",
      " |-- DEBIT: double (nullable = true)\n",
      " |-- types_indexed2: double (nullable = false)\n",
      " |-- types_onehot2: vector (nullable = true)\n",
      " |-- types_onehot_split2: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      " |-- CC: double (nullable = true)\n",
      " |-- CM: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_type2_df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### StringIndexer Initialization\n",
    "### column: step\n",
    "\n",
    "#indexer_type = StringIndexer(inputCol=\"step\",outputCol=\"types_indexed3\")\n",
    "#indexerModel_type = indexer_type.fit(type_df)\n",
    "\n",
    "\n",
    "### Transform the DataFrame using the fitted StringIndexer model\n",
    "\n",
    "#indexed_df_type = indexerModel_type.transform(type_df)\n",
    "#indexed_df_type.show(10)\n",
    "\n",
    "\n",
    "### apply One-Hot-Encoding to the indexed column, that is, \n",
    "### \"types_indexed2\"\n",
    "\n",
    "#encoder_type2 = OneHotEncoder(dropLast=False, inputCol=\"types_indexed3\", outputCol=\"types_onehot3\")\n",
    "#encoder_type2_df = encoder_type2.fit(indexed_df_type).transform(indexed_df_type)\n",
    "#encoder_type2_df.show(truncate=False)\n",
    "\n",
    "#encoder_type2_df_split = encoder_type2_df.select('*',vector_to_array('types_onehot3').alias('types_onehot_split3'))\n",
    "#encoder_type2_df_split.show(5)\n",
    "\n",
    "### now, we´ll split the \"types_onehot_split2\" into two columns, one per category\n",
    "\n",
    "#num_categories = len(encoder_type2_df_split.first()['types_onehot_split3'])\n",
    "#cols_expanded = [(f.col('types_onehot_split3')[i].alias(f\"{indexerModel_type.labels[i]}\")) for i in range(num_categories)]\n",
    "#encoder_type2_df_split = encoder_type2_df_split.select('*',*cols_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll eliminate the unnecessary columns:\n",
    "+ nameOrig\n",
    "+ nameDest\n",
    "+ isFlaggedFraud\n",
    "+ newbalanceDest\n",
    "+ oldbalanceDest\n",
    "+ oldbalanceOrg\n",
    "+ newbalanceOrig \n",
    "+ types_indexed\n",
    "+ types_onehot\n",
    "+ types_onehot_split\n",
    "+ types_indexed2\n",
    "+ types_onehot2\n",
    "+ types_onehot_split2\n",
    "+ type\n",
    "+ type2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+--------+-------+-------+--------+-----+---+---+\n",
      "|step|   amount|isFraud|CASH_OUT|PAYMENT|CASH_IN|TRANSFER|DEBIT| CC| CM|\n",
      "+----+---------+-------+--------+-------+-------+--------+-----+---+---+\n",
      "|  35|312070.89|      0|     0.0|    0.0|    1.0|     0.0|  0.0|1.0|0.0|\n",
      "|  35|244107.21|      0|     0.0|    0.0|    1.0|     0.0|  0.0|1.0|0.0|\n",
      "|  35| 17849.53|      0|     0.0|    0.0|    1.0|     0.0|  0.0|1.0|0.0|\n",
      "|  35|204719.93|      0|     0.0|    0.0|    1.0|     0.0|  0.0|1.0|0.0|\n",
      "|  35|281004.16|      0|     0.0|    0.0|    1.0|     0.0|  0.0|1.0|0.0|\n",
      "+----+---------+-------+--------+-------+-------+--------+-----+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank_par = encoder_type2_df_split.drop(\"nameOrig\",\"nameDest\",\"isFlaggedFraud\",\"newbalanceDest\",\"oldbalanceDest\",\n",
    "                       \"oldbalanceOrg\",\"newbalanceOrig\",\"type\",\"types_indexed\",\"types_onehot\",\n",
    "                       \"types_onehot_split\",\"type2\",\"types_indexed2\",\"types_onehot2\",\"types_onehot_split2\",\"types_indexed3\",\"types_onehot3\",\"types_onehot_split3\" )\n",
    "df_bank_par.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['step',\n",
       " 'amount',\n",
       " 'isFraud',\n",
       " 'CASH_OUT',\n",
       " 'PAYMENT',\n",
       " 'CASH_IN',\n",
       " 'TRANSFER',\n",
       " 'DEBIT',\n",
       " 'CC',\n",
       " 'CM']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bank_par.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6362620"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bank_par.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_bank_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are the same quantity of registers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6362620"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 2.2.1.- Eliminate duplicated\n",
    "\n",
    "num_all_rows = df_bank_par.count()\n",
    "num_all_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_duplicated_rows = df_bank_par.distinct().count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of duplicated rows is: 7597\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of duplicated rows is:\",num_all_rows - num_duplicated_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are 7597 duplicated rows. Let´s remove the null values and duplicated values from the df_bank_par dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.dropna()\n",
    "\n",
    "df_bank_par = df_bank_par.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:21:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6355023"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bank_par.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the duplicated registers have been removed because there are fewer registers than before. Let´s take a look at the \"clean\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/05/24 21:22:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 45:=====>                                                   (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+--------+-------+-------+--------+-----+---+---+\n",
      "|step|   amount|isFraud|CASH_OUT|PAYMENT|CASH_IN|TRANSFER|DEBIT| CC| CM|\n",
      "+----+---------+-------+--------+-------+-------+--------+-----+---+---+\n",
      "|  35| 23148.99|      0|     0.0|    1.0|    0.0|     0.0|  0.0|0.0|1.0|\n",
      "|  35| 21158.22|      0|     0.0|    1.0|    0.0|     0.0|  0.0|0.0|1.0|\n",
      "|  35|  7326.08|      0|     0.0|    1.0|    0.0|     0.0|  0.0|0.0|1.0|\n",
      "|  35|248000.14|      0|     0.0|    0.0|    1.0|     0.0|  0.0|1.0|0.0|\n",
      "|  35|   227.92|      0|     0.0|    1.0|    0.0|     0.0|  0.0|0.0|1.0|\n",
      "|  35| 74827.29|      0|     0.0|    0.0|    1.0|     0.0|  0.0|1.0|0.0|\n",
      "|  35| 36205.75|      0|     1.0|    0.0|    0.0|     0.0|  0.0|1.0|0.0|\n",
      "|  35| 25359.28|      0|     1.0|    0.0|    0.0|     0.0|  0.0|1.0|0.0|\n",
      "|  35|  3579.31|      0|     0.0|    1.0|    0.0|     0.0|  0.0|0.0|1.0|\n",
      "|  35| 11931.85|      0|     1.0|    0.0|    0.0|     0.0|  0.0|1.0|0.0|\n",
      "+----+---------+-------+--------+-------+-------+--------+-----+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The visualization will be done using a functions which leverages the method histogram() of pyspark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the \"histogram\" function\n",
    "\n",
    "def histogram(df, col, bins=10, xname=None, yname=None):\n",
    "    \n",
    "    '''\n",
    "    This function makes a histogram from spark dataframe named \n",
    "    df for column name col. \n",
    "    '''\n",
    "    \n",
    "    # Calculating histogram in Spark \n",
    "    vals = df.select(col).rdd.flatMap(lambda x: x).histogram(bins)\n",
    "    \n",
    "    # Preprocessing histogram points and locations \n",
    "    width = vals[0][1] - vals[0][0]\n",
    "    loc = [vals[0][0] + (i+1) * width for i in range(len(vals[1]))]\n",
    "    \n",
    "    # Making a bar plot \n",
    "    mpt.bar(loc, vals[1], width=width)\n",
    "    mpt.xlabel(col)\n",
    "    mpt.ylabel(yname)\n",
    "    mpt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are some features that need to be converted to integers such as \"step\",\"amount\" and \"isFraud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string columns into integer columns\n",
    "\n",
    "df_bank_par = df_bank_par.withColumn(\"step\",df_bank_par[\"step\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.withColumn(\"amount\",df_bank_par[\"amount\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.withColumn(\"isFraud\",df_bank_par[\"isFraud\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- CASH_OUT: double (nullable = true)\n",
      " |-- PAYMENT: double (nullable = true)\n",
      " |-- CASH_IN: double (nullable = true)\n",
      " |-- TRANSFER: double (nullable = true)\n",
      " |-- DEBIT: double (nullable = true)\n",
      " |-- CC: double (nullable = true)\n",
      " |-- CM: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_bank_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve seen that all the features are \"integer\" types now. Therefore, we´re able to perform various visualizations with the histogram method. That´s what we´ll do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"step\"\n",
    "\n",
    "##histogram(df_bank_par, 'step', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"amount\"\n",
    "\n",
    "##histogram(df_bank_par, 'amount', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"Debit\"\n",
    "\n",
    "##histogram(df_bank_par, 'Debit', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"Payment\"\n",
    "\n",
    "##histogram(df_bank_par, 'Payment', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CASH_OUT\"\n",
    "\n",
    "##histogram(df_bank_par, 'CASH_OUT', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CASH_IN\"\n",
    "\n",
    "##histogram(df_bank_par, 'CASH_IN', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"TRANSFER\"\n",
    "\n",
    "##histogram(df_bank_par, 'TRANSFER', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CC\"\n",
    "\n",
    "##histogram(df_bank_par, 'CC', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CM\"\n",
    "\n",
    "##histogram(df_bank_par, 'CM', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"isFraud\"\n",
    "\n",
    "##histogram(df_bank_par, 'isFraud', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember that our label is \"isFraud\", therefore, we can see that this class is unbalanced as we can see from the previous graphic. We need to perform an **Oversampling** through ***Data Balancing*** using *pyspark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################## Oversampling with PySpark #########################################################\n",
    "\n",
    "# Create undersampling function\n",
    "#def oversample_minority(df, ratio=1):\n",
    "#    '''\n",
    "#    ratio is the ratio of majority to minority\n",
    "#    Eg. ratio 1 is equivalent to majority:minority = 1:1\n",
    "#    ratio 5 is equivalent to majority:minority = 5:1\n",
    "#    '''\n",
    "#    minority_count = df.filter(f.col('isFraud')==1).count()\n",
    "#    majority_count = df.filter(f.col('isFraud')==0).count()\n",
    "#    \n",
    "#    balance_ratio = majority_count / minority_count\n",
    "#    \n",
    "#    print(f\"Initial Majority:Minority ratio is {balance_ratio:.2f}:1\")\n",
    "#    if ratio >= balance_ratio:\n",
    "#        print(\"No oversampling of minority was done as the input ratio was more than or equal to the initial ratio.\")\n",
    "#    else:\n",
    "#        print(f\"Oversampling of minority done such that Majority:Minority ratio is {ratio}:1\")\n",
    "#    \n",
    "#    oversampled_minority = df.filter(f.col('isFraud')==1).sample(withReplacement=True, fraction=(balance_ratio/ratio),seed=88)\n",
    "#    oversampled_df = df.filter(f.col('isFraud')==0).union(oversampled_minority)\n",
    "#    \n",
    "#    return oversampled_df\n",
    "\n",
    "#oversampled_df = oversample_minority(df_bank_par,ratio=1)\n",
    "\n",
    "#minority_count = oversampled_df.filter(f.col('isFraud')==1).count()\n",
    "#majority_count = oversampled_df.filter(f.col('isFraud')==0).count()\n",
    "#minority_count, majority_count\n",
    "#oversampled_df = oversampled_df.dropna()\n",
    "#oversampled_df = oversampled_df.dropDuplicates()\n",
    "#df_bank_par = oversampled_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want to transform this pyspark \"dataframe\" df_bank_par into a pandas dataframe we can use the method to_pandas_on_spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_pandas = pd.read_csv('fraudDetection.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_bank_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def procesar_datos():\n",
    "  global df_banco, resultados\n",
    "  df_banco=df_bank_pandas.copy()\n",
    "  # Crea la nueva variable type2 con la combinación de la primera letra de las columnas nameOrig y nameDest\n",
    "  df_banco['type2'] = df_banco['nameOrig'].str[0] + df_banco['nameDest'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "      <th>type2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>9839.64</td>\n",
       "      <td>C1231006815</td>\n",
       "      <td>170136.00</td>\n",
       "      <td>160296.36</td>\n",
       "      <td>M1979787155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>1864.28</td>\n",
       "      <td>C1666544295</td>\n",
       "      <td>21249.00</td>\n",
       "      <td>19384.72</td>\n",
       "      <td>M2044282225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C1305486145</td>\n",
       "      <td>181.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C553264065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C840083671</td>\n",
       "      <td>181.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C38997010</td>\n",
       "      <td>21182.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11668.14</td>\n",
       "      <td>C2048537720</td>\n",
       "      <td>41554.00</td>\n",
       "      <td>29885.86</td>\n",
       "      <td>M1230701703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>7817.71</td>\n",
       "      <td>C90045638</td>\n",
       "      <td>53860.00</td>\n",
       "      <td>46042.29</td>\n",
       "      <td>M573487274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>7107.77</td>\n",
       "      <td>C154988899</td>\n",
       "      <td>183195.00</td>\n",
       "      <td>176087.23</td>\n",
       "      <td>M408069119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>7861.64</td>\n",
       "      <td>C1912850431</td>\n",
       "      <td>176087.23</td>\n",
       "      <td>168225.59</td>\n",
       "      <td>M633326333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>4024.36</td>\n",
       "      <td>C1265012928</td>\n",
       "      <td>2671.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>M1176932104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>DEBIT</td>\n",
       "      <td>5337.77</td>\n",
       "      <td>C712410124</td>\n",
       "      <td>41720.00</td>\n",
       "      <td>36382.23</td>\n",
       "      <td>C195600860</td>\n",
       "      <td>41898.0</td>\n",
       "      <td>40348.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
       "0     1   PAYMENT   9839.64  C1231006815      170136.00       160296.36   \n",
       "1     1   PAYMENT   1864.28  C1666544295       21249.00        19384.72   \n",
       "2     1  TRANSFER    181.00  C1305486145         181.00            0.00   \n",
       "3     1  CASH_OUT    181.00   C840083671         181.00            0.00   \n",
       "4     1   PAYMENT  11668.14  C2048537720       41554.00        29885.86   \n",
       "5     1   PAYMENT   7817.71    C90045638       53860.00        46042.29   \n",
       "6     1   PAYMENT   7107.77   C154988899      183195.00       176087.23   \n",
       "7     1   PAYMENT   7861.64  C1912850431      176087.23       168225.59   \n",
       "8     1   PAYMENT   4024.36  C1265012928        2671.00            0.00   \n",
       "9     1     DEBIT   5337.77   C712410124       41720.00        36382.23   \n",
       "\n",
       "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud type2  \n",
       "0  M1979787155             0.0            0.00        0               0    CM  \n",
       "1  M2044282225             0.0            0.00        0               0    CM  \n",
       "2   C553264065             0.0            0.00        1               0    CC  \n",
       "3    C38997010         21182.0            0.00        1               0    CC  \n",
       "4  M1230701703             0.0            0.00        0               0    CM  \n",
       "5   M573487274             0.0            0.00        0               0    CM  \n",
       "6   M408069119             0.0            0.00        0               0    CM  \n",
       "7   M633326333             0.0            0.00        0               0    CM  \n",
       "8  M1176932104             0.0            0.00        0               0    CM  \n",
       "9   C195600860         41898.0        40348.79        0               0    CC  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "procesar_datos()\n",
    "df_banco.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "      <th>type_CASH_IN</th>\n",
       "      <th>type_CASH_OUT</th>\n",
       "      <th>type_DEBIT</th>\n",
       "      <th>type_PAYMENT</th>\n",
       "      <th>type_TRANSFER</th>\n",
       "      <th>type2_CC</th>\n",
       "      <th>type2_CM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4484297</th>\n",
       "      <td>324</td>\n",
       "      <td>13446.95</td>\n",
       "      <td>C509087326</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1402213524</td>\n",
       "      <td>1156927.73</td>\n",
       "      <td>1170374.68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897438</th>\n",
       "      <td>349</td>\n",
       "      <td>148142.70</td>\n",
       "      <td>C452949359</td>\n",
       "      <td>9324518.87</td>\n",
       "      <td>9472661.57</td>\n",
       "      <td>C712878492</td>\n",
       "      <td>1398652.10</td>\n",
       "      <td>1250509.40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056198</th>\n",
       "      <td>96</td>\n",
       "      <td>769.12</td>\n",
       "      <td>C105247912</td>\n",
       "      <td>19913.00</td>\n",
       "      <td>19143.88</td>\n",
       "      <td>M1972533505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3867227</th>\n",
       "      <td>283</td>\n",
       "      <td>142920.56</td>\n",
       "      <td>C1602679819</td>\n",
       "      <td>15470.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1369248966</td>\n",
       "      <td>49794.68</td>\n",
       "      <td>192715.24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6160257</th>\n",
       "      <td>548</td>\n",
       "      <td>208882.31</td>\n",
       "      <td>C1341557859</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1369390187</td>\n",
       "      <td>310461.68</td>\n",
       "      <td>519344.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818101</th>\n",
       "      <td>281</td>\n",
       "      <td>57515.66</td>\n",
       "      <td>C1960017515</td>\n",
       "      <td>166.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1700524560</td>\n",
       "      <td>4849916.91</td>\n",
       "      <td>4907432.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3968226</th>\n",
       "      <td>297</td>\n",
       "      <td>18482.63</td>\n",
       "      <td>C1342698466</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>M2132483009</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597263</th>\n",
       "      <td>156</td>\n",
       "      <td>2030.15</td>\n",
       "      <td>C1602652519</td>\n",
       "      <td>788237.58</td>\n",
       "      <td>786207.43</td>\n",
       "      <td>M51920892</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536251</th>\n",
       "      <td>205</td>\n",
       "      <td>114485.38</td>\n",
       "      <td>C44332445</td>\n",
       "      <td>21882.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C806678283</td>\n",
       "      <td>0.00</td>\n",
       "      <td>114485.38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678720</th>\n",
       "      <td>276</td>\n",
       "      <td>3199.41</td>\n",
       "      <td>C612315473</td>\n",
       "      <td>19952.00</td>\n",
       "      <td>16752.59</td>\n",
       "      <td>M1372916846</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         step     amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
       "4484297   324   13446.95   C509087326           0.00            0.00   \n",
       "4897438   349  148142.70   C452949359     9324518.87      9472661.57   \n",
       "1056198    96     769.12   C105247912       19913.00        19143.88   \n",
       "3867227   283  142920.56  C1602679819       15470.00            0.00   \n",
       "6160257   548  208882.31  C1341557859           0.00            0.00   \n",
       "3818101   281   57515.66  C1960017515         166.00            0.00   \n",
       "3968226   297   18482.63  C1342698466           0.00            0.00   \n",
       "1597263   156    2030.15  C1602652519      788237.58       786207.43   \n",
       "2536251   205  114485.38    C44332445       21882.00            0.00   \n",
       "3678720   276    3199.41   C612315473       19952.00        16752.59   \n",
       "\n",
       "            nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \\\n",
       "4484297  C1402213524      1156927.73      1170374.68        0               0   \n",
       "4897438   C712878492      1398652.10      1250509.40        0               0   \n",
       "1056198  M1972533505            0.00            0.00        0               0   \n",
       "3867227  C1369248966        49794.68       192715.24        0               0   \n",
       "6160257  C1369390187       310461.68       519344.00        0               0   \n",
       "3818101  C1700524560      4849916.91      4907432.58        0               0   \n",
       "3968226  M2132483009            0.00            0.00        0               0   \n",
       "1597263    M51920892            0.00            0.00        0               0   \n",
       "2536251   C806678283            0.00       114485.38        0               0   \n",
       "3678720  M1372916846            0.00            0.00        0               0   \n",
       "\n",
       "         type_CASH_IN  type_CASH_OUT  type_DEBIT  type_PAYMENT  type_TRANSFER  \\\n",
       "4484297             0              1           0             0              0   \n",
       "4897438             1              0           0             0              0   \n",
       "1056198             0              0           0             1              0   \n",
       "3867227             0              1           0             0              0   \n",
       "6160257             0              1           0             0              0   \n",
       "3818101             0              0           0             0              1   \n",
       "3968226             0              0           0             1              0   \n",
       "1597263             0              0           0             1              0   \n",
       "2536251             0              1           0             0              0   \n",
       "3678720             0              0           0             1              0   \n",
       "\n",
       "         type2_CC  type2_CM  \n",
       "4484297         1         0  \n",
       "4897438         1         0  \n",
       "1056198         0         1  \n",
       "3867227         1         0  \n",
       "6160257         1         0  \n",
       "3818101         1         0  \n",
       "3968226         0         1  \n",
       "1597263         0         1  \n",
       "2536251         1         0  \n",
       "3678720         0         1  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realiza one-hot encoding de las columnas type y type2\n",
    "df_encoded = pd.get_dummies(df_banco, columns=['type', 'type2'], dtype=int)\n",
    "df_encoded.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>type_CASH_IN</th>\n",
       "      <th>type_CASH_OUT</th>\n",
       "      <th>type_DEBIT</th>\n",
       "      <th>type_PAYMENT</th>\n",
       "      <th>type_TRANSFER</th>\n",
       "      <th>type2_CC</th>\n",
       "      <th>type2_CM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9839.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1864.28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>181.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>181.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>11668.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362615</th>\n",
       "      <td>743</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362616</th>\n",
       "      <td>743</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362617</th>\n",
       "      <td>743</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362618</th>\n",
       "      <td>743</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362619</th>\n",
       "      <td>743</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6362620 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         step      amount  isFraud  type_CASH_IN  type_CASH_OUT  type_DEBIT  \\\n",
       "0           1     9839.64        0             0              0           0   \n",
       "1           1     1864.28        0             0              0           0   \n",
       "2           1      181.00        1             0              0           0   \n",
       "3           1      181.00        1             0              1           0   \n",
       "4           1    11668.14        0             0              0           0   \n",
       "...       ...         ...      ...           ...            ...         ...   \n",
       "6362615   743   339682.13        1             0              1           0   \n",
       "6362616   743  6311409.28        1             0              0           0   \n",
       "6362617   743  6311409.28        1             0              1           0   \n",
       "6362618   743   850002.52        1             0              0           0   \n",
       "6362619   743   850002.52        1             0              1           0   \n",
       "\n",
       "         type_PAYMENT  type_TRANSFER  type2_CC  type2_CM  \n",
       "0                   1              0         0         1  \n",
       "1                   1              0         0         1  \n",
       "2                   0              1         1         0  \n",
       "3                   0              0         1         0  \n",
       "4                   1              0         0         1  \n",
       "...               ...            ...       ...       ...  \n",
       "6362615             0              0         1         0  \n",
       "6362616             0              1         1         0  \n",
       "6362617             0              0         1         0  \n",
       "6362618             0              1         1         0  \n",
       "6362619             0              0         1         0  \n",
       "\n",
       "[6362620 rows x 10 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista de columnas a eliminar\n",
    "columns_to_drop = ['nameOrig', 'nameDest', 'isFlaggedFraud', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "# Elimina las columnas del DataFrame\n",
    "df_encoded.drop(columns=columns_to_drop, inplace=True)\n",
    "# Resetea el índice\n",
    "df_encoded.reset_index(drop=True, inplace=True)\n",
    "df_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina registros duplicados y guarda el resultado en df_banco\n",
    "df_banco = df_encoded.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/zwbn7zz13l7c9kw1ccdb5qb80000gn/T/ipykernel_80684/2381833487.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_banco.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Elimina registros con valores nulos y restablece el índice\n",
    "df_banco.dropna(inplace=True)\n",
    "df_banco.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>type_CASH_IN</th>\n",
       "      <th>type_CASH_OUT</th>\n",
       "      <th>type_DEBIT</th>\n",
       "      <th>type_PAYMENT</th>\n",
       "      <th>type_TRANSFER</th>\n",
       "      <th>type2_CC</th>\n",
       "      <th>type2_CM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9839.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1864.28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>181.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>181.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>11668.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6355018</th>\n",
       "      <td>743</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6355019</th>\n",
       "      <td>743</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6355020</th>\n",
       "      <td>743</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6355021</th>\n",
       "      <td>743</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6355022</th>\n",
       "      <td>743</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6355023 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         step      amount  isFraud  type_CASH_IN  type_CASH_OUT  type_DEBIT  \\\n",
       "0           1     9839.64        0             0              0           0   \n",
       "1           1     1864.28        0             0              0           0   \n",
       "2           1      181.00        1             0              0           0   \n",
       "3           1      181.00        1             0              1           0   \n",
       "4           1    11668.14        0             0              0           0   \n",
       "...       ...         ...      ...           ...            ...         ...   \n",
       "6355018   743   339682.13        1             0              1           0   \n",
       "6355019   743  6311409.28        1             0              0           0   \n",
       "6355020   743  6311409.28        1             0              1           0   \n",
       "6355021   743   850002.52        1             0              0           0   \n",
       "6355022   743   850002.52        1             0              1           0   \n",
       "\n",
       "         type_PAYMENT  type_TRANSFER  type2_CC  type2_CM  \n",
       "0                   1              0         0         1  \n",
       "1                   1              0         0         1  \n",
       "2                   0              1         1         0  \n",
       "3                   0              0         1         0  \n",
       "4                   1              0         0         1  \n",
       "...               ...            ...       ...       ...  \n",
       "6355018             0              0         1         0  \n",
       "6355019             0              1         1         0  \n",
       "6355020             0              0         1         0  \n",
       "6355021             0              1         1         0  \n",
       "6355022             0              0         1         0  \n",
       "\n",
       "[6355023 rows x 10 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6355023 entries, 0 to 6355022\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   step           int64  \n",
      " 1   amount         float64\n",
      " 2   isFraud        int64  \n",
      " 3   type_CASH_IN   int64  \n",
      " 4   type_CASH_OUT  int64  \n",
      " 5   type_DEBIT     int64  \n",
      " 6   type_PAYMENT   int64  \n",
      " 7   type_TRANSFER  int64  \n",
      " 8   type2_CC       int64  \n",
      " 9   type2_CM       int64  \n",
      "dtypes: float64(1), int64(9)\n",
      "memory usage: 484.8 MB\n"
     ]
    }
   ],
   "source": [
    "df_banco.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contar los valores de la columna isFraud\n",
    "conteo_isfraud = df_banco['isFraud'].value_counts()\n",
    "\n",
    "# Crear el gráfico de barras verticales\n",
    "mpt.figure(figsize=(8, 6))\n",
    "conteo_isfraud.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "mpt.title('Distribución de la columna isFraud')\n",
    "mpt.xlabel('isFraud')\n",
    "mpt.ylabel('Cantidad')\n",
    "mpt.xticks([0, 1], ['No Fraude', 'Fraude'], rotation=0)\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>amount</th>\n",
       "      <th>type_CASH_IN</th>\n",
       "      <th>type_CASH_OUT</th>\n",
       "      <th>type_DEBIT</th>\n",
       "      <th>type_PAYMENT</th>\n",
       "      <th>type_TRANSFER</th>\n",
       "      <th>type2_CC</th>\n",
       "      <th>type2_CM</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9839.640000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1864.280000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>11668.140000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11316844</th>\n",
       "      <td>72</td>\n",
       "      <td>59132.865542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11316845</th>\n",
       "      <td>262</td>\n",
       "      <td>213293.869235</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11316846</th>\n",
       "      <td>592</td>\n",
       "      <td>468613.275827</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11316847</th>\n",
       "      <td>195</td>\n",
       "      <td>295040.631306</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11316848</th>\n",
       "      <td>202</td>\n",
       "      <td>21751.397045</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11316849 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          step         amount  type_CASH_IN  type_CASH_OUT  type_DEBIT  \\\n",
       "0            1    9839.640000             0              0           0   \n",
       "1            1    1864.280000             0              0           0   \n",
       "2            1     181.000000             0              0           0   \n",
       "3            1     181.000000             0              1           0   \n",
       "4            1   11668.140000             0              0           0   \n",
       "...        ...            ...           ...            ...         ...   \n",
       "11316844    72   59132.865542             0              0           0   \n",
       "11316845   262  213293.869235             0              1           0   \n",
       "11316846   592  468613.275827             0              1           0   \n",
       "11316847   195  295040.631306             0              0           0   \n",
       "11316848   202   21751.397045             0              0           0   \n",
       "\n",
       "          type_PAYMENT  type_TRANSFER  type2_CC  type2_CM  isFraud  \n",
       "0                    1              0         0         1        0  \n",
       "1                    1              0         0         1        0  \n",
       "2                    0              1         1         0        1  \n",
       "3                    0              0         1         0        1  \n",
       "4                    1              0         0         1        0  \n",
       "...                ...            ...       ...       ...      ...  \n",
       "11316844             0              0         1         0        1  \n",
       "11316845             0              0         1         0        1  \n",
       "11316846             0              0         1         0        1  \n",
       "11316847             0              1         1         0        1  \n",
       "11316848             0              0         1         0        1  \n",
       "\n",
       "[11316849 rows x 10 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title\n",
    "\n",
    "def balanceo_clases():\n",
    "    global df_banco, resultados\n",
    "\n",
    "    # Instancia SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Balanceo de clases\n",
    "    X_res, y_res = smote.fit_resample(df_banco.drop(columns=['isFraud']), df_banco['isFraud'])\n",
    "\n",
    "    # Reconstrucción del DataFrame balanceado\n",
    "    df_banco = pd.DataFrame(X_res, columns=df_banco.drop(columns=['isFraud']).columns)\n",
    "    df_banco['isFraud'] = y_res\n",
    "\n",
    "    # Elimina registros duplicados\n",
    "    df_banco.drop_duplicates(inplace=True)\n",
    "    df_banco.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Llama a la función balanceo_clases\n",
    "balanceo_clases()\n",
    "\n",
    "# Imprime el resultado final\n",
    "df_banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_banco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contar los valores de la columna isFraud\n",
    "conteo_isfraud = df_banco['isFraud'].value_counts()\n",
    "\n",
    "# Crear el gráfico de barras verticales\n",
    "mpt.figure(figsize=(8, 6))\n",
    "conteo_isfraud.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "mpt.title('Distribución de la columna isFraud')\n",
    "mpt.xlabel('isFraud')\n",
    "mpt.ylabel('Cantidad')\n",
    "mpt.xticks([0, 1], ['No Fraude', 'Fraude'], rotation=0)\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_banco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll convert this pandas dataframe into a PySpark dataframe to leverage, but first to a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_banco.to_parquet('df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = spark.read.parquet('df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_bank_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------+-------------+----------+------------+-------------+--------+--------+-------+\n",
      "|step|  amount|type_CASH_IN|type_CASH_OUT|type_DEBIT|type_PAYMENT|type_TRANSFER|type2_CC|type2_CM|isFraud|\n",
      "+----+--------+------------+-------------+----------+------------+-------------+--------+--------+-------+\n",
      "|   1| 9839.64|           0|            0|         0|           1|            0|       0|       1|      0|\n",
      "|   1| 1864.28|           0|            0|         0|           1|            0|       0|       1|      0|\n",
      "|   1|   181.0|           0|            0|         0|           0|            1|       1|       0|      1|\n",
      "|   1|   181.0|           0|            1|         0|           0|            0|       1|       0|      1|\n",
      "|   1|11668.14|           0|            0|         0|           1|            0|       0|       1|      0|\n",
      "|   1| 7817.71|           0|            0|         0|           1|            0|       0|       1|      0|\n",
      "|   1| 7107.77|           0|            0|         0|           1|            0|       0|       1|      0|\n",
      "|   1| 7861.64|           0|            0|         0|           1|            0|       0|       1|      0|\n",
      "|   1| 4024.36|           0|            0|         0|           1|            0|       0|       1|      0|\n",
      "|   1| 5337.77|           0|            0|         1|           0|            0|       1|       0|      0|\n",
      "+----+--------+------------+-------------+----------+------------+-------------+--------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- type_CASH_IN: long (nullable = true)\n",
      " |-- type_CASH_OUT: long (nullable = true)\n",
      " |-- type_DEBIT: long (nullable = true)\n",
      " |-- type_PAYMENT: long (nullable = true)\n",
      " |-- type_TRANSFER: long (nullable = true)\n",
      " |-- type2_CC: long (nullable = true)\n",
      " |-- type2_CM: long (nullable = true)\n",
      " |-- isFraud: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11316849"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bank_par.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string columns into integer columns\n",
    "\n",
    "df_bank_par = df_bank_par.withColumn(\"isFraud\",df_bank_par[\"isFraud\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- type_CASH_IN: long (nullable = true)\n",
      " |-- type_CASH_OUT: long (nullable = true)\n",
      " |-- type_DEBIT: long (nullable = true)\n",
      " |-- type_PAYMENT: long (nullable = true)\n",
      " |-- type_TRANSFER: long (nullable = true)\n",
      " |-- type2_CC: long (nullable = true)\n",
      " |-- type2_CM: long (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = df_bank_par.filter(f.col(\"isFraud\")==0)\n",
    "class_1 = df_bank_par.filter(f.col(\"isFraud\")==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6346920"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4969929"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## Convert parquet file into Pandas ##########################\n",
    "\n",
    "##df_bank_par_pandas = df_bank_par.to_pandas_on_spark()\n",
    "##df_bank_par_pandas.head(10)\n",
    "##df_bank_par_pandas.describe()\n",
    "##type(df_bank_par_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s create a function to find a correlation between the target variable \"isFraud\" and the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the function \"correlation_df\"\n",
    "\n",
    "def correlation_df(df,target_var,feature_cols, method):\n",
    "    # assemble features into a vector\n",
    "    target_var = [target_var]\n",
    "    feature_cols = feature_cols\n",
    "    df_cor = df.select(target_var + feature_cols)\n",
    "    assembler = VectorAssembler(inputCols=target_var + feature_cols, outputCol=\"features\")\n",
    "    df_cor = assembler.transform(df_cor)\n",
    "\n",
    "    # calculate correlation matrix\n",
    "    correlation_matrix = Correlation.corr(df_cor, \"features\", method =method).head()[0]\n",
    "\n",
    "    # extract the correlation coefficient between target and each feature\n",
    "    target_corr_list = [correlation_matrix[i,0] for i in range(len(feature_cols)+1)][1:]\n",
    "\n",
    "    # create a Dataframe with target variable, feature names and correlation coefficients\n",
    "    correlation_data = [(feature_cols[i],float(target_corr_list[i])) for i in range(len(feature_cols))]\n",
    "\n",
    "    correlation_df = spark.createDataFrame(correlation_data, [\"feature\",\"correlation\"] )\n",
    "\n",
    "    correlation_df = correlation_df.withColumn(\"abs_correlation\",f.abs(\"correlation\"))\n",
    "\n",
    "    # print the result\n",
    "    return correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- type_CASH_IN: long (nullable = true)\n",
      " |-- type_CASH_OUT: long (nullable = true)\n",
      " |-- type_DEBIT: long (nullable = true)\n",
      " |-- type_PAYMENT: long (nullable = true)\n",
      " |-- type_TRANSFER: long (nullable = true)\n",
      " |-- type2_CC: long (nullable = true)\n",
      " |-- type2_CM: long (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:37 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:38 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:38 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:38 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:38 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:41 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:41 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:41 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:41 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:41 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corelation between isFraud and the other features is: \n",
      "+-------------+--------------------+-------------------+\n",
      "|      feature|         correlation|    abs_correlation|\n",
      "+-------------+--------------------+-------------------+\n",
      "|         step| 0.34684009076918143|0.34684009076918143|\n",
      "|       amount|  0.3604992933016074| 0.3604992933016074|\n",
      "| type_CASH_IN| -0.3323385816575818| 0.3323385816575818|\n",
      "|type_CASH_OUT|-0.10691980783674657|0.10691980783674657|\n",
      "|   type_DEBIT|-0.05363819819216311|0.05363819819216311|\n",
      "| type_PAYMENT| -0.4282609844322808| 0.4282609844322808|\n",
      "|type_TRANSFER| 0.22944948951215355|0.22944948951215355|\n",
      "|     type2_CC| 0.42826098443228067|0.42826098443228067|\n",
      "|     type2_CM| -0.4282609844322808| 0.4282609844322808|\n",
      "+-------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"isFraud\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: long (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- type_CASH_IN: long (nullable = true)\n",
      " |-- type_CASH_OUT: long (nullable = true)\n",
      " |-- type_DEBIT: long (nullable = true)\n",
      " |-- type_PAYMENT: long (nullable = true)\n",
      " |-- type_TRANSFER: long (nullable = true)\n",
      " |-- type2_CC: long (nullable = true)\n",
      " |-- type2_CM: long (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:57 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:57 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:57 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:57 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:57 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:22:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corelation between amount and the other features is: \n",
      "+-------------+--------------------+--------------------+\n",
      "|      feature|         correlation|     abs_correlation|\n",
      "+-------------+--------------------+--------------------+\n",
      "|         step| 0.12429095205369503| 0.12429095205369503|\n",
      "| type_CASH_IN|-0.12114623126047482| 0.12114623126047482|\n",
      "|type_CASH_OUT|  -0.041412031021294|   0.041412031021294|\n",
      "|   type_DEBIT|-0.02682257141276...|0.026822571412764645|\n",
      "| type_PAYMENT| -0.2114653262805776|  0.2114653262805776|\n",
      "|type_TRANSFER| 0.15105593069727094| 0.15105593069727094|\n",
      "|     type2_CC| 0.21146532628056097| 0.21146532628056097|\n",
      "|     type2_CM| -0.2114653262805776|  0.2114653262805776|\n",
      "|      isFraud|  0.3604992933016071|  0.3604992933016071|\n",
      "+-------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"amount\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:03 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:07 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:08 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:08 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:08 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:08 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:11 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:11 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:11 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:11 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:11 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:23:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corelation between step and the other features is: \n",
      "+-------------+--------------------+--------------------+\n",
      "|      feature|         correlation|     abs_correlation|\n",
      "+-------------+--------------------+--------------------+\n",
      "|       amount| 0.12429095205369503| 0.12429095205369503|\n",
      "| type_CASH_IN|-0.11255059686687098| 0.11255059686687098|\n",
      "|type_CASH_OUT|-0.04565434924107...|0.045654349241078744|\n",
      "|   type_DEBIT|-0.01678650820881...|0.016786508208811435|\n",
      "| type_PAYMENT|-0.14515981565813296| 0.14515981565813296|\n",
      "|type_TRANSFER| 0.08329816241940949| 0.08329816241940949|\n",
      "|     type2_CC|  0.1451598156581346|  0.1451598156581346|\n",
      "|     type2_CM|-0.14515981565813296| 0.14515981565813296|\n",
      "|      isFraud| 0.34684009076918176| 0.34684009076918176|\n",
      "+-------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"step\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construction of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = df_bank_par.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s assemble these datasets \"train\" and \"test\" into a single feature vector using VectorAssembler class per each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+\n",
      "|step|amount|type_CASH_IN|type_CASH_OUT|type_DEBIT|type_PAYMENT|type_TRANSFER|type2_CC|type2_CM|isFraud|         my_features|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+\n",
      "|   1|  6.42|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1|  6.93|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1|  8.73|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 13.54|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 23.31|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 25.12|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 27.55|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 38.66|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 58.21|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 79.26|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# let´s assemble the train dataset as a single feature vector using VectorAssembler class\n",
    "\n",
    "columns = ['step','amount','type_CASH_OUT','type_PAYMENT','type_CASH_IN','type_TRANSFER','type_DEBIT','type2_CC','type2_CM','isFraud']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "\n",
    "train = assembler.transform(train).withColumnRenamed(\"features\", \"my_features\")\n",
    "\n",
    "train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+\n",
      "|step|amount|type_CASH_IN|type_CASH_OUT|type_DEBIT|type_PAYMENT|type_TRANSFER|type2_CC|type2_CM|isFraud|         my_features|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+\n",
      "|   1| 15.06|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1|  42.0|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 53.35|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 96.32|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1|137.88|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1|150.07|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 151.2|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1| 181.0|           0|            1|         0|           0|            0|       1|       0|      1|(10,[0,1,2,7,9],[...|\n",
      "|   1|232.74|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "|   1|244.87|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# let´s assemble the test dataset as a single feature vector using VectorAssembler class\n",
    "\n",
    "columns = ['step','amount','type_CASH_OUT','type_PAYMENT','type_CASH_IN','type_TRANSFER','type_DEBIT','type2_CC','type2_CM','isFraud']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "\n",
    "test = assembler.transform(test).withColumnRenamed(\"features\", \"my_features\")\n",
    "\n",
    "test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Models\n",
    "\n",
    "We´ll use several machine learning algorithms to evaluate all of them and to select the best one. We´ll start with Random Forest. However, it´s important to create some lists where to store the results of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = []\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "precision = []\n",
    "\n",
    "recall = []\n",
    "\n",
    "auc_roc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model \"random forest\" (rf)\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='isFraud',featuresCol='features')\n",
    "##model_RF = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor().setLabelCol(\"isFraud\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Params.explainParam of RandomForestRegressor_4dd5ccd5d457>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.explainParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_a5df87f773eb"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rForm = RFormula(formula = \"lab\")\n",
    "assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "\n",
    "stages = [assembler,rf]\n",
    "\n",
    "pipeline = Pipeline().setStages(stages) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParamGridBuilder()\\\n",
    "         .addGrid(rf.numTrees, [1,2,3,4,5])\\\n",
    "         .addGrid(rf.maxDepth, [1,2,3,4,5])\\\n",
    "         .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "            .setMetricName(\"areaUnderROC\")\\\n",
    "            .setRawPredictionCol(\"prediction\")\\\n",
    "            .setLabelCol(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainValidationSplit\n",
    "\n",
    "tvs = TrainValidationSplit()\\\n",
    "      .setTrainRatio(0.75)\\\n",
    "      .setEstimatorParamMaps(params)\\\n",
    "      .setEstimator(pipeline)\\\n",
    "      .setEvaluator(evaluator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 20:21:42 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 20:21:42 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 20:21:48 WARN MemoryStore: Not enough space to cache rdd_241_8 in memory! (computed 24.4 MiB so far)\n",
      "24/05/24 20:21:48 WARN MemoryStore: Not enough space to cache rdd_241_9 in memory! (computed 24.4 MiB so far)\n",
      "24/05/24 20:21:48 WARN MemoryStore: Not enough space to cache rdd_241_3 in memory! (computed 23.6 MiB so far)\n",
      "24/05/24 20:21:48 WARN MemoryStore: Not enough space to cache rdd_241_2 in memory! (computed 23.6 MiB so far)\n",
      "24/05/24 20:21:48 WARN MemoryStore: Not enough space to cache rdd_241_5 in memory! (computed 24.4 MiB so far)\n",
      "24/05/24 20:21:48 WARN BlockManager: Persisting block rdd_241_3 to disk instead.\n",
      "24/05/24 20:21:48 WARN BlockManager: Persisting block rdd_241_8 to disk instead.\n",
      "24/05/24 20:21:48 WARN BlockManager: Persisting block rdd_241_2 to disk instead.\n",
      "24/05/24 20:21:48 WARN BlockManager: Persisting block rdd_241_9 to disk instead.\n",
      "24/05/24 20:21:48 WARN BlockManager: Persisting block rdd_241_5 to disk instead.\n",
      "24/05/24 20:21:48 WARN MemoryStore: Not enough space to cache rdd_241_1 in memory! (computed 23.6 MiB so far)\n",
      "24/05/24 20:21:48 WARN BlockManager: Persisting block rdd_241_1 to disk instead.\n",
      "24/05/24 20:21:48 WARN MemoryStore: Not enough space to cache rdd_241_4 in memory! (computed 45.9 MiB so far)\n",
      "24/05/24 20:21:48 WARN BlockManager: Persisting block rdd_241_4 to disk instead.\n",
      "24/05/24 20:21:48 WARN MemoryStore: Not enough space to cache rdd_241_0 in memory! (computed 45.8 MiB so far)\n",
      "24/05/24 20:21:49 WARN BlockManager: Persisting block rdd_241_0 to disk instead.\n",
      "24/05/24 20:22:20 WARN BlockManager: Block rdd_241_4 could not be removed as it was not found on disk or in memory\n",
      "24/05/24 20:22:26 ERROR Executor: Exception in task 4.0 in stage 91.0 (TID 307)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$5673/1949591185.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$75/1651162064.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5716/1554266102.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2260/1892064910.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "24/05/24 20:22:36 WARN BlockManager: Block rdd_241_6 could not be removed as it was not found on disk or in memory\n",
      "24/05/24 20:22:37 ERROR Executor: Exception in task 6.0 in stage 91.0 (TID 309)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$5673/1949591185.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$75/1651162064.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2260/1892064910.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "24/05/24 20:22:38 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 91.0 (TID 307),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$5673/1949591185.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$75/1651162064.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5716/1554266102.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2260/1892064910.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "24/05/24 20:22:38 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 6.0 in stage 91.0 (TID 309),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$5673/1949591185.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$75/1651162064.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2260/1892064910.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "24/05/24 20:22:39 WARN TaskSetManager: Lost task 6.0 in stage 91.0 (TID 309) (192.168.1.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$5673/1949591185.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$75/1651162064.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2260/1892064910.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\n",
      "24/05/24 20:22:39 ERROR TaskSetManager: Task 6 in stage 91.0 failed 1 times; aborting job\n",
      "24/05/24 20:22:40 ERROR Instrumentation: org.apache.spark.SparkException: Job 73 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/05/24 20:22:41 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@225be19e rejected from java.util.concurrent.ThreadPoolExecutor@134421c2[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 305]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/24 20:22:41 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@51a58f6e rejected from java.util.concurrent.ThreadPoolExecutor@134421c2[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 305]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/24 20:22:42 WARN BlockManager: Putting block rdd_241_8 failed due to exception java.lang.AssertionError: assertion failed.\n",
      "24/05/24 20:22:42 WARN BlockManager: Putting block rdd_241_5 failed due to exception java.lang.AssertionError: assertion failed.\n",
      "24/05/24 20:22:42 WARN BlockManager: Block rdd_241_5 was not removed normally.\n",
      "24/05/24 20:22:42 WARN BlockManager: Block rdd_241_8 was not removed normally.\n",
      "24/05/24 20:22:42 ERROR Executor: Exception in task 3.0 in stage 91.0 (TID 306): Block rdd_241_3 does not exist\n",
      "24/05/24 20:22:42 ERROR Executor: Exception in task 5.0 in stage 91.0 (TID 308): Block rdd_241_5 does not exist\n",
      "24/05/24 20:22:42 ERROR Executor: Exception in task 8.0 in stage 91.0 (TID 311): Block rdd_241_8 does not exist\n",
      "24/05/24 20:22:42 ERROR Executor: Exception in task 2.0 in stage 91.0 (TID 305): Block rdd_241_2 does not exist\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50704)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/0n/zwbn7zz13l7c9kw1ccdb5qb80000gn/T/ipykernel_69544/3326208726.py\", line 1, in <module>\n",
      "    tvsFitted = tvs.fit(train)\n",
      "                ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 1464, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 873, in next\n",
      "    raise value\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 1464, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "                                                             ^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tvsFitted \u001b[38;5;241m=\u001b[39m tvs\u001b[38;5;241m.\u001b[39mfit(train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:1464\u001b[0m, in \u001b[0;36mTrainValidationSplit._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1463\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m numModels\n\u001b[0;32m-> 1464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m   1465\u001b[0m     metrics[j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:1464\u001b[0m, in \u001b[0;36mTrainValidationSplit._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m   1463\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m numModels\n\u001b[0;32m-> 1464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m   1465\u001b[0m     metrics[j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitSingleModel(index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39mfit(dataset, paramMaps[index])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_showtraceback(etype, value, stb)\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(evalue),\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m gateway_client\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "tvsFitted = tvs.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tvsFitted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluator\u001b[38;5;241m.\u001b[39mevaluate(tvsFitted\u001b[38;5;241m.\u001b[39mtransform(train))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tvsFitted' is not defined"
     ]
    }
   ],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap: Whether bootstrap samples are used when building trees. (default: True)\n",
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name. (default: label, current: isFraud)\n",
      "leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)\n",
      "numTrees: Number of trees to train (>= 1). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: -7418932063336898719)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(rf.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.getMaxDepth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 00:30:29 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:30:45 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:30:45 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 2.5 MiB so far)\n",
      "24/05/20 00:30:45 WARN MemoryStore: Not enough space to cache rdd_320_9 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:30:45 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:30:45 WARN BlockManager: Persisting block rdd_320_2 to disk instead.\n",
      "24/05/20 00:30:45 WARN BlockManager: Persisting block rdd_320_3 to disk instead.\n",
      "24/05/20 00:30:45 WARN BlockManager: Persisting block rdd_320_5 to disk instead.\n",
      "24/05/20 00:30:45 WARN BlockManager: Persisting block rdd_320_9 to disk instead.\n",
      "24/05/20 00:30:45 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 13.7 MiB so far)\n",
      "24/05/20 00:30:45 WARN BlockManager: Persisting block rdd_320_8 to disk instead.\n",
      "24/05/20 00:30:45 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 9.0 MiB so far)\n",
      "24/05/20 00:30:45 WARN BlockManager: Persisting block rdd_320_6 to disk instead.\n",
      "24/05/20 00:30:46 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:30:46 WARN BlockManager: Persisting block rdd_320_7 to disk instead.\n",
      "24/05/20 00:30:46 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 46.4 MiB so far)\n",
      "24/05/20 00:30:46 WARN BlockManager: Persisting block rdd_320_0 to disk instead.\n",
      "24/05/20 00:30:48 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 110.8 MiB so far)\n",
      "24/05/20 00:30:48 WARN BlockManager: Persisting block rdd_320_4 to disk instead.\n",
      "24/05/20 00:30:48 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 46.4 MiB so far)\n",
      "24/05/20 00:30:48 WARN BlockManager: Persisting block rdd_320_1 to disk instead.\n",
      "24/05/20 00:30:51 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:30:52 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:30:52 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:30:52 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 46.4 MiB so far)\n",
      "24/05/20 00:30:52 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:30:52 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 46.4 MiB so far)\n",
      "24/05/20 00:30:53 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 72.4 MiB so far)\n",
      "24/05/20 00:30:53 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 72.4 MiB so far)\n",
      "24/05/20 00:30:58 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 166.2 MiB so far)\n",
      "24/05/20 00:31:04 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:04 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:04 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:04 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:04 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:04 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:04 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:04 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:04 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:08 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:08 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:08 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:08 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:08 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:08 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:08 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:08 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:08 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:13 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:13 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:13 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:13 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:13 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:13 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:13 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:13 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:13 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:17 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:17 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:17 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:17 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:17 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:17 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:17 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:17 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:17 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:20 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:21 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:21 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:21 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:21 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:21 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:21 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:21 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:21 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:24 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:24 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:24 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:24 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:24 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:24 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:24 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:24 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:26 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 166.2 MiB so far)\n",
      "24/05/20 00:31:28 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:28 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:28 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:28 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:28 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:28 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:29 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 72.4 MiB so far)\n",
      "24/05/20 00:31:29 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 72.4 MiB so far)\n",
      "24/05/20 00:31:29 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 72.4 MiB so far)\n",
      "24/05/20 00:31:31 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:31 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:31 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:31 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:31 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:31 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:31 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:31 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:33 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 166.2 MiB so far)\n",
      "24/05/20 00:31:35 WARN MemoryStore: Not enough space to cache rdd_320_1 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:35 WARN MemoryStore: Not enough space to cache rdd_320_8 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:35 WARN MemoryStore: Not enough space to cache rdd_320_0 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:35 WARN MemoryStore: Not enough space to cache rdd_320_2 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:35 WARN MemoryStore: Not enough space to cache rdd_320_7 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:35 WARN MemoryStore: Not enough space to cache rdd_320_3 in memory! (computed 20.5 MiB so far)\n",
      "24/05/20 00:31:35 WARN MemoryStore: Not enough space to cache rdd_320_5 in memory! (computed 30.8 MiB so far)\n",
      "24/05/20 00:31:36 WARN MemoryStore: Not enough space to cache rdd_320_6 in memory! (computed 110.8 MiB so far)\n",
      "24/05/20 00:31:36 WARN MemoryStore: Not enough space to cache rdd_320_4 in memory! (computed 110.8 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_RF = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.getNumTrees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.RandomForestRegressor"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we put our simple, two-stage workflow into an ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler,rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.Pipeline"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "            .addGrid(rf.numTrees, [int(x) for x in np.linspace(start=10, stop=12, num=3)])\\\n",
    "            .addGrid(rf.maxDepth,[int(x) for x in np.linspace(start=5, stop=7, num=3)])\\\n",
    "            .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossValidator\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \\\n",
    "                          evaluator=RegressionEvaluator().setLabelCol(\"isFraud\"),numFolds=3\n",
    "                         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 21:26:02 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:26:02 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/24 21:26:09 WARN MemoryStore: Not enough space to cache rdd_241_9 in memory! (computed 24.4 MiB so far)\n",
      "24/05/24 21:26:09 WARN MemoryStore: Not enough space to cache rdd_241_1 in memory! (computed 23.7 MiB so far)\n",
      "24/05/24 21:26:09 WARN MemoryStore: Not enough space to cache rdd_241_2 in memory! (computed 23.7 MiB so far)\n",
      "24/05/24 21:26:09 WARN MemoryStore: Not enough space to cache rdd_241_6 in memory! (computed 24.4 MiB so far)\n",
      "24/05/24 21:26:09 WARN MemoryStore: Not enough space to cache rdd_241_8 in memory! (computed 24.4 MiB so far)\n",
      "24/05/24 21:26:09 WARN MemoryStore: Not enough space to cache rdd_241_5 in memory! (computed 24.3 MiB so far)\n",
      "24/05/24 21:26:09 WARN BlockManager: Persisting block rdd_241_2 to disk instead.\n",
      "24/05/24 21:26:09 WARN BlockManager: Persisting block rdd_241_1 to disk instead.\n",
      "24/05/24 21:26:09 WARN BlockManager: Persisting block rdd_241_8 to disk instead.\n",
      "24/05/24 21:26:09 WARN BlockManager: Persisting block rdd_241_6 to disk instead.\n",
      "24/05/24 21:26:09 WARN BlockManager: Persisting block rdd_241_9 to disk instead.\n",
      "24/05/24 21:26:09 WARN BlockManager: Persisting block rdd_241_5 to disk instead.\n",
      "24/05/24 21:26:11 WARN MemoryStore: Not enough space to cache rdd_241_0 in memory! (computed 45.8 MiB so far)\n",
      "24/05/24 21:26:11 WARN MemoryStore: Not enough space to cache rdd_241_4 in memory! (computed 45.9 MiB so far)\n",
      "24/05/24 21:26:11 WARN MemoryStore: Not enough space to cache rdd_241_3 in memory! (computed 45.9 MiB so far)\n",
      "24/05/24 21:26:11 WARN BlockManager: Persisting block rdd_241_0 to disk instead.\n",
      "24/05/24 21:26:11 WARN BlockManager: Persisting block rdd_241_3 to disk instead.\n",
      "24/05/24 21:26:11 WARN BlockManager: Persisting block rdd_241_4 to disk instead.\n",
      "24/05/24 21:26:12 WARN MemoryStore: Not enough space to cache rdd_241_7 in memory! (computed 47.3 MiB so far)\n",
      "24/05/24 21:26:13 WARN BlockManager: Persisting block rdd_241_7 to disk instead.\n",
      "24/05/24 21:26:36 WARN BlockManager: Block rdd_241_2 could not be removed as it was not found on disk or in memory\n",
      "24/05/24 21:26:37 ERROR Executor: Exception in task 2.0 in stage 91.0 (TID 305)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$6177/1759800813.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$75/1651162064.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6225/175392074.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2260/198369421.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "24/05/24 21:26:38 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 91.0 (TID 305),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$6177/1759800813.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$75/1651162064.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6225/175392074.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2260/198369421.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "24/05/24 21:26:39 WARN TaskSetManager: Lost task 2.0 in stage 91.0 (TID 305) (192.168.1.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$6177/1759800813.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$75/1651162064.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6225/175392074.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2260/198369421.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\n",
      "24/05/24 21:26:39 ERROR TaskSetManager: Task 2 in stage 91.0 failed 1 times; aborting job\n",
      "24/05/24 21:26:40 WARN MemoryStore: Not enough space to cache rdd_241_7 in memory! (computed 47.3 MiB so far)\n",
      "24/05/24 21:26:40 WARN BlockManager: Putting block rdd_241_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/24 21:26:40 WARN BlockManager: Putting block rdd_241_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/24 21:26:40 WARN BlockManager: Block rdd_241_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/24 21:26:40 WARN MemoryStore: Not enough space to cache rdd_241_5 in memory! (computed 24.3 MiB so far)\n",
      "24/05/24 21:26:40 WARN BlockManager: Block rdd_241_8 could not be removed as it was not found on disk or in memory\n",
      "24/05/24 21:26:40 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 91.0 failed 1 times, most recent failure: Lost task 2.0 in stage 91.0 (TID 305) (192.168.1.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$6177/1759800813.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$75/1651162064.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6225/175392074.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2260/198369421.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.build(ColumnBuilder.scala:81)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build(NullableColumnBuilder.scala:67)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.build$(NullableColumnBuilder.scala:66)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.build(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.$anonfun$next$4(InMemoryRelation.scala:115)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1$$Lambda$6177/1759800813.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike$$Lambda$75/1651162064.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:114)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$6225/175392074.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2260/198369421.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51477)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/0n/zwbn7zz13l7c9kw1ccdb5qb80000gn/T/ipykernel_80684/170167318.py\", line 3, in <module>\n",
      "    cvModel = crossval.fit(train)\n",
      "              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 847, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 873, in next\n",
      "    raise value\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 847, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "                                                             ^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[102], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cross Validation Model: train\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m crossval\u001b[38;5;241m.\u001b[39mfit(train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitSingleModel(index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39mfit(dataset, paramMaps[index])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_showtraceback(etype, value, stb)\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(evalue),\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m gateway_client\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Cross Validation Model: train\n",
    "\n",
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 16, 23, 30]\n"
     ]
    }
   ],
   "source": [
    "#print([int(x) for x in np.linspace(start=10, stop=30, num=4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 10, 12, 15, 17, 20, 22, 25]\n"
     ]
    }
   ],
   "source": [
    "#print([int(x) for x in np.linspace(start=5, stop=25, num=9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Model: predictions on test dataset\n",
    "\n",
    "predictions = cvModel.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:44:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:44:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:44:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:44:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:44:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:44:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:44:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:44:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:44:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:44:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:45:53 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks \n",
      "java.io.IOException: Connecting to /192.168.1.3:51082 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:151)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:102)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1172)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/23 00:45:53 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks \n",
      "java.io.IOException: Connecting to /192.168.1.3:51082 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:151)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:102)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1172)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/23 00:45:53 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks \n",
      "java.io.IOException: Connecting to /192.168.1.3:51082 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:151)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:102)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1172)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/23 00:45:53 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks \n",
      "java.io.IOException: Failed to connect to /192.168.1.3:51082\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:151)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:102)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1172)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1116)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1116)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1256)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Operation timed out: /192.168.1.3:51082\n",
      "Caused by: java.net.ConnectException: Operation timed out\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:715)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/23 00:47:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\n",
      "java.io.IOException: Connecting to /192.168.1.3:51082 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/23 00:47:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\n",
      "java.io.IOException: Connecting to /192.168.1.3:51082 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/23 00:47:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\n",
      "java.io.IOException: Connecting to /192.168.1.3:51082 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/23 00:47:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\n",
      "java.io.IOException: Failed to connect to /192.168.1.3:51082\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Operation timed out: /192.168.1.3:51082\n",
      "Caused by: java.net.ConnectException: Operation timed out\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:715)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 10) / 10]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py\", line 280, in _collect_as_arrow\n",
      "    results = list(batch_stream)\n",
      "              ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/pandas/serializers.py\", line 69, in load_stream\n",
      "    for batch in self.serializer.load_stream(stream):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/pandas/serializers.py\", line 111, in load_stream\n",
      "    reader = pa.ipc.open_stream(stream)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyarrow/ipc.py\", line 190, in open_stream\n",
      "    return RecordBatchStreamReader(source, options=options,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyarrow/ipc.py\", line 52, in __init__\n",
      "    self._open(source, options=options, memory_pool=memory_pool)\n",
      "  File \"pyarrow/ipc.pxi\", line 929, in pyarrow.lib._RecordBatchStreamReader._open\n",
      "  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/types.pxi\", line 88, in pyarrow.lib._datatype_to_pep3118\n",
      "  File \"/opt/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:280\u001b[0m, in \u001b[0;36mPandasConversionMixin._collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(batch_stream)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/pandas/serializers.py:69\u001b[0m, in \u001b[0;36mArrowCollectSerializer.load_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# load the batches\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserializer\u001b[38;5;241m.\u001b[39mload_stream(stream):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m batch\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/pandas/serializers.py:111\u001b[0m, in \u001b[0;36mArrowStreamSerializer.load_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m reader \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mipc\u001b[38;5;241m.\u001b[39mopen_stream(stream)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m reader:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/ipc.py:190\u001b[0m, in \u001b[0;36mopen_stream\u001b[0;34m(source, options, memory_pool)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mCreate reader for Arrow streaming format.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    A reader for the given source\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RecordBatchStreamReader(source, options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    191\u001b[0m                                memory_pool\u001b[38;5;241m=\u001b[39mmemory_pool)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/ipc.py:52\u001b[0m, in \u001b[0;36mRecordBatchStreamReader.__init__\u001b[0;34m(self, source, options, memory_pool)\u001b[0m\n\u001b[1;32m     51\u001b[0m options \u001b[38;5;241m=\u001b[39m _ensure_default_ipc_read_options(options)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(source, options\u001b[38;5;241m=\u001b[39moptions, memory_pool\u001b[38;5;241m=\u001b[39mmemory_pool)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/ipc.pxi:929\u001b[0m, in \u001b[0;36mpyarrow.lib._RecordBatchStreamReader._open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/types.pxi:88\u001b[0m, in \u001b[0;36mpyarrow.lib._datatype_to_pep3118\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m rmse \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions)\n\u001b[1;32m      9\u001b[0m rfPred \u001b[38;5;241m=\u001b[39m cvModel\u001b[38;5;241m.\u001b[39mtransform(df_bank_par)\n\u001b[0;32m---> 11\u001b[0m rfResult \u001b[38;5;241m=\u001b[39m rfPred\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m     13\u001b[0m mpt\u001b[38;5;241m.\u001b[39mplot(rfResult\u001b[38;5;241m.\u001b[39misFraud, rfResult\u001b[38;5;241m.\u001b[39mprediction,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m mpt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misFraud\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:131\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\n\u001b[1;32m    130\u001b[0m self_destruct \u001b[38;5;241m=\u001b[39m jconf\u001b[38;5;241m.\u001b[39marrowPySparkSelfDestructEnabled()\n\u001b[0;32m--> 131\u001b[0m batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collect_as_arrow(split_batches\u001b[38;5;241m=\u001b[39mself_destruct)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batches) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    133\u001b[0m     table \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(batches)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:284\u001b[0m, in \u001b[0;36mPandasConversionMixin._collect_as_arrow\u001b[0;34m(self, split_batches)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_spark_exception():\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;66;03m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m         jsocket_auth_server\u001b[38;5;241m.\u001b[39mgetResult()\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Separate RecordBatches from batch order indices in results\u001b[39;00m\n\u001b[1;32m    287\u001b[0m batches \u001b[38;5;241m=\u001b[39m results[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:48:34 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)\n",
      "java.io.IOException: Connecting to /192.168.1.3:51082 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/23 00:48:34 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)\n",
      "java.io.IOException: Connecting to /192.168.1.3:51082 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/23 00:48:34 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)\n",
      "java.io.IOException: Connecting to /192.168.1.3:51082 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/23 00:48:34 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 2 retries)\n",
      "java.io.IOException: Failed to connect to /192.168.1.3:51082\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:131)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Operation timed out: /192.168.1.3:51082\n",
      "Caused by: java.net.ConnectException: Operation timed out\n",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:715)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Model: evaluate\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"isFraud\", \\\n",
    "                                predictionCol=\"prediction\", \\\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "rfPred = cvModel.transform(df_bank_par)\n",
    "\n",
    "rfResult = rfPred.toPandas()\n",
    "\n",
    "mpt.plot(rfResult.isFraud, rfResult.prediction,'bo')\n",
    "mpt.xlabel('isFraud')\n",
    "mpt.ylabel('Prediction')\n",
    "mpt.suptitle(\"Model Performance RMSE\")\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Feature Importances')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cross Validation Model: feature importance \n",
    "\n",
    "bestPipeline = cvModel.bestModel\n",
    "bestModel = bestPipeline.stages[1] # type: ignore\n",
    "\n",
    "importances = bestModel.featureImportances\n",
    "\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "mpt.bar(x_values,importances, orientation='vertical')\n",
    "mpt.xticks(x_values, columns, rotation = 40)\n",
    "mpt.ylabel('Importance')\n",
    "mpt.xlabel('Feature')\n",
    "mpt.title('Feature Importances')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel_ =  cvModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='CrossValidatorModel_2748ca735c2e', name='seed', doc='random seed.'): 8945853511248409359,\n",
       " Param(parent='CrossValidatorModel_2748ca735c2e', name='numFolds', doc='number of folds for cross validation'): 3,\n",
       " Param(parent='CrossValidatorModel_2748ca735c2e', name='foldCol', doc=\"Param for the column name of user specified fold number. Once this is specified, :py:class:`CrossValidator` won't do random k-fold split. Note that this column should be integer type with range [0, numFolds) and Spark will throw exception on out-of-range fold numbers.\"): '',\n",
       " Param(parent='CrossValidatorModel_2748ca735c2e', name='estimator', doc='estimator to be cross-validated'): Pipeline_0701df93e45d,\n",
       " Param(parent='CrossValidatorModel_2748ca735c2e', name='estimatorParamMaps', doc='estimator param maps'): [{Param(parent='RandomForestRegressor_83c753edd4e5', name='numTrees', doc='Number of trees to train (>= 1).'): 10,\n",
       "   Param(parent='RandomForestRegressor_83c753edd4e5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5},\n",
       "  {Param(parent='RandomForestRegressor_83c753edd4e5', name='numTrees', doc='Number of trees to train (>= 1).'): 10,\n",
       "   Param(parent='RandomForestRegressor_83c753edd4e5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6},\n",
       "  {Param(parent='RandomForestRegressor_83c753edd4e5', name='numTrees', doc='Number of trees to train (>= 1).'): 10,\n",
       "   Param(parent='RandomForestRegressor_83c753edd4e5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 7},\n",
       "  {Param(parent='RandomForestRegressor_83c753edd4e5', name='numTrees', doc='Number of trees to train (>= 1).'): 11,\n",
       "   Param(parent='RandomForestRegressor_83c753edd4e5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5},\n",
       "  {Param(parent='RandomForestRegressor_83c753edd4e5', name='numTrees', doc='Number of trees to train (>= 1).'): 11,\n",
       "   Param(parent='RandomForestRegressor_83c753edd4e5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6},\n",
       "  {Param(parent='RandomForestRegressor_83c753edd4e5', name='numTrees', doc='Number of trees to train (>= 1).'): 11,\n",
       "   Param(parent='RandomForestRegressor_83c753edd4e5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 7},\n",
       "  {Param(parent='RandomForestRegressor_83c753edd4e5', name='numTrees', doc='Number of trees to train (>= 1).'): 12,\n",
       "   Param(parent='RandomForestRegressor_83c753edd4e5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5},\n",
       "  {Param(parent='RandomForestRegressor_83c753edd4e5', name='numTrees', doc='Number of trees to train (>= 1).'): 12,\n",
       "   Param(parent='RandomForestRegressor_83c753edd4e5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6},\n",
       "  {Param(parent='RandomForestRegressor_83c753edd4e5', name='numTrees', doc='Number of trees to train (>= 1).'): 12,\n",
       "   Param(parent='RandomForestRegressor_83c753edd4e5', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 7}],\n",
       " Param(parent='CrossValidatorModel_2748ca735c2e', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'): RegressionEvaluator_c46b3c5be0ab}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bestModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cross Validation Model: best hyperparameters\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumTrees - \u001b[39m\u001b[38;5;124m'\u001b[39m, bestModel\u001b[38;5;241m.\u001b[39mgetNumTrees)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxDepth - \u001b[39m\u001b[38;5;124m'\u001b[39m, bestModel\u001b[38;5;241m.\u001b[39mgetOrDefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxDepth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bestModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Cross Validation Model: best hyperparameters\n",
    "\n",
    "print('numTrees - ', bestModel.getNumTrees)\n",
    "print('maxDepth - ', bestModel.getOrDefault('maxDepth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04154387943814526,\n",
       " 0.04154387943814526,\n",
       " 0.04154387943814526,\n",
       " 0.04154387943814526,\n",
       " 0.04154387943814526,\n",
       " 0.04154387943814526,\n",
       " 0.04154387943814526,\n",
       " 0.04154387943814526,\n",
       " 0.04154387943814526]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x14f8af680>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(cvModel.avgMetrics, paramGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the best hyperparameters in this case is:\n",
    "+ numTrees : 20\n",
    "+ maxDepth : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll use these one to build our Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train the model \"random forest\" (rf)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misFraud\u001b[39m\u001b[38;5;124m'\u001b[39m,featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_features\u001b[39m\u001b[38;5;124m'\u001b[39m, maxDepth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, numTrees\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# train the model \"random forest\" (rf)\n",
    "\n",
    "rf = RandomForestRegressor(labelCol='isFraud',featuresCol='my_features', maxDepth=8, numTrees=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:54:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:33 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:33 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:33 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:33 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:33 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:33 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:33 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:33 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:33 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:33 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:54:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_RF = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of the random forest model using the test dataset\n",
    "\n",
    "predictions = model_RF.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1477:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+\n",
      "|step|amount|type_CASH_IN|type_CASH_OUT|type_DEBIT|type_PAYMENT|type_TRANSFER|type2_CC|type2_CM|isFraud|         my_features|          prediction|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+\n",
      "|   1|  6.42|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|  6.93|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|  8.73|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 25.12|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|  42.0|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 58.21|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 96.32|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|106.81|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|137.88|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|150.07|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|154.87|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|163.72|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|186.22|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|207.75|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|233.76|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|244.87|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|264.93|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 272.8|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|320.53|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|351.26|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|402.54|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 413.4|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|450.55|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|452.57|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|0.057770776565610714|\n",
      "|   1|461.65|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|466.97|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|577.01|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|622.15|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 624.1|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|635.55|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|642.26|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|0.057770776565610714|\n",
      "|   1|671.64|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|681.05|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|700.86|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|708.82|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|727.94|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|748.42|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|749.34|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|795.29|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|804.16|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|818.99|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|0.057770776565610714|\n",
      "|   1|846.91|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|848.74|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|881.26|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|895.24|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|911.76|           1|            0|         0|           0|            0|       1|       0|      0|(10,[0,1,4,7],[1....|                 0.0|\n",
      "|   1|942.95|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|969.76|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|993.46|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|997.34|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|                 0.0|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are three more columns: rawPrediction, probability and prediction. We can clearly compare the actual values and predicted values with the output below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+--------------------+\n",
      "|step|amount|type_CASH_IN|type_CASH_OUT|type_DEBIT|type_PAYMENT|type_TRANSFER|type2_CC|type2_CM|isFraud|             feature|            features|          prediction|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+--------------------+\n",
      "|   1|  6.42|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|  8.73|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 13.54|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 15.06|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 25.12|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 27.55|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 38.66|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 53.35|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 58.21|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 79.26|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 84.22|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 96.32|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|106.81|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|112.56|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|121.27|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|132.42|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|137.88|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|150.07|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 151.2|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|163.72|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 181.0|           0|            0|         0|           0|            1|       1|       0|      1|(10,[0,1,5,7,9],[...|(10,[0,1,5,7,9],[...|  0.9270226032715354|\n",
      "|   1| 181.0|           0|            1|         0|           0|            0|       1|       0|      1|(10,[0,1,2,7,9],[...|(10,[0,1,2,7,9],[...|  0.8777536528390898|\n",
      "|   1|186.33|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 194.4|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|(10,[0,1,6,7],[1....|0.040354931981134025|\n",
      "|   1|207.75|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|226.29|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|232.74|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|233.76|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|244.87|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|261.95|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|264.93|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 265.8|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|266.88|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|270.78|           1|            0|         0|           0|            0|       1|       0|      0|(10,[0,1,4,7],[1....|(10,[0,1,4,7],[1....|                 0.0|\n",
      "|   1|271.32|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|282.42|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|286.87|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|(10,[0,1,6,7],[1....|0.040354931981134025|\n",
      "|   1|300.41|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|(10,[0,1,6,7],[1....|0.040354931981134025|\n",
      "|   1|320.53|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|328.07|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 331.4|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|353.62|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 353.9|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|360.13|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 365.0|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|374.36|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|(10,[0,1,6,7],[1....|0.040354931981134025|\n",
      "|   1|399.66|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|402.54|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1| 413.4|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "|   1|413.84|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|(10,[0,1,3,8],[1....|                 0.0|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At a glance we can see that the predicted values are the same of the actual values, at least for the first fifty registers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC - ROC Curve (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 01:04:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 01:04:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 01:04:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 01:04:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 01:04:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 01:04:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 01:04:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 01:04:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 01:04:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 01:04:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "DoubleType column cannot be cast to Vector",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m lr_eval2 \u001b[38;5;241m=\u001b[39m MulticlassClassificationEvaluator(probabilityCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misFraud\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m lr_AUC \u001b[38;5;241m=\u001b[39m lr_eval\u001b[38;5;241m.\u001b[39mevaluate(predictions) \u001b[38;5;66;03m# AUC performance\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m lr_ACC \u001b[38;5;241m=\u001b[39m lr_eval2\u001b[38;5;241m.\u001b[39mevaluate(predictions, {lr_eval2\u001b[38;5;241m.\u001b[39mmetricName:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m}) \u001b[38;5;66;03m# Accuracy performance\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ROC graphic\u001b[39;00m\n\u001b[1;32m      9\u001b[0m preds_and_labels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misFraud\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/evaluation.py:109\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mevaluate(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: DoubleType column cannot be cast to Vector"
     ]
    }
   ],
   "source": [
    "lr_eval = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "lr_eval2 = MulticlassClassificationEvaluator(probabilityCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "lr_AUC = lr_eval.evaluate(predictions) # AUC performance\n",
    "lr_ACC = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"accuracy\"}) # Accuracy performance\n",
    "\n",
    "# ROC graphic\n",
    "\n",
    "preds_and_labels = predictions.select(\"prediction\", \"isFraud\")\n",
    "preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "# Area under ROC\n",
    "\n",
    "print(\"Logistic Regression Area Under ROC:\")\n",
    "\n",
    "metrics.areaUnderROC\n",
    "\n",
    "# Visualization\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "y_test = [i[1] for i in preds_and_labels_list]\n",
    "y_score = [i[0] for i in preds_and_labels_list]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "mpt.figure(figsize=(5,4))\n",
    "mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "mpt.plot([0,1],[0,1],'k--')\n",
    "mpt.xlim([0.0,1.0])\n",
    "mpt.ylim([0.0,1.05])\n",
    "mpt.xlabel('False Positive Rate')\n",
    "mpt.ylabel('True Positive Rate')\n",
    "mpt.title('ROC Curve - Logistic Regression')\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to evaluate our random forest machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:56:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:56:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:56:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:56:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:56:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:56:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:56:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:56:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:56:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:56:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\")\n",
    "accuracy_ = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe accuracy is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"The accuracy is {accuracy_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Error is 0.5960906736266807\n"
     ]
    }
   ],
   "source": [
    "Test_Error = (1 - accuracy_)\n",
    "print(f\"The Test Error is {Test_Error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out the Consufion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|prediction|isFraud|\n",
      "+----------+-------+\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_and_labels.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:57:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:57:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:57:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:57:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:57:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:57:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:57:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:57:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:57:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:57:55 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:10 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:10 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:10 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:10 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:10 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:10 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:10 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:10 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:10 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:10 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:25 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:25 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:25 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:25 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:25 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:25 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:25 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:25 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:25 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:25 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:58:40 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "[Stage 1496:=================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC:  1.0\n",
      "Accuracy:  0.3136234694614361\n",
      "Precsion:  1.0\n",
      "Recall:  0.3136234694614361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# AUC - ROC\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\",)\n",
    "\n",
    "accuracy_rf = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_rf = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_rf = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "# let´s store the results of this model: Random Forest\n",
    "\n",
    "accuracy.append(accuracy_rf)\n",
    "\n",
    "precision.append(precision_rf)\n",
    "\n",
    "recall.append(recall_rf)\n",
    "\n",
    "auc_roc.append(auc_rf)\n",
    "\n",
    "# let´s store the name of the model: Random Forest \n",
    "name_model_ = \"Random Forest\"\n",
    "\n",
    "name_model.append(name_model_)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_rf)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_rf)\n",
    "\n",
    "print(f\"Precsion: \", precision_rf)\n",
    "\n",
    "print(f\"Recall: \", recall_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "24/05/23 00:59:23 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confusion Matrix is:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:59:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 00:59:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1059512.,       0.],\n",
       "       [      0.,    4614.]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s use cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##grid = ParamGridBuilder().addGrid(lr.)\n",
    "\n",
    "##cv = CrossValidator(estimator=rf, evaluator=evaluator, estimatorParamMaps=)\n",
    "\n",
    "##cvModel = cv.fit(df_bank_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to the confusion matrix, all the actual values will be correctly predicted. It may mean an Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A. Summary of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model Logistic Regression (lr)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"my_features\", labelCol=\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: my_features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: isFraud)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 16:43:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:32 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/05/23 16:43:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:32 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:43:37 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:37 WARN MemoryStore: Not enough space to cache rdd_213_3 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:37 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:37 WARN BlockManager: Persisting block rdd_213_5 to disk instead.\n",
      "24/05/23 16:43:37 WARN BlockManager: Persisting block rdd_213_6 to disk instead.\n",
      "24/05/23 16:43:37 WARN BlockManager: Persisting block rdd_213_3 to disk instead.\n",
      "24/05/23 16:43:38 WARN MemoryStore: Not enough space to cache rdd_213_4 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:38 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:38 WARN MemoryStore: Not enough space to cache rdd_213_0 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:38 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:38 WARN BlockManager: Persisting block rdd_213_0 to disk instead.\n",
      "24/05/23 16:43:38 WARN BlockManager: Persisting block rdd_213_7 to disk instead.\n",
      "24/05/23 16:43:38 WARN BlockManager: Persisting block rdd_213_4 to disk instead.\n",
      "24/05/23 16:43:38 WARN BlockManager: Persisting block rdd_213_8 to disk instead.\n",
      "24/05/23 16:43:38 WARN MemoryStore: Not enough space to cache rdd_213_2 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:38 WARN BlockManager: Persisting block rdd_213_2 to disk instead.\n",
      "24/05/23 16:43:38 WARN MemoryStore: Not enough space to cache rdd_213_9 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:38 WARN BlockManager: Persisting block rdd_213_9 to disk instead.\n",
      "24/05/23 16:43:42 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:42 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:42 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:42 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:46 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:46 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:46 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:46 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:48 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:48 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:48 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:48 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:49 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:49 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:49 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:49 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:51 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:51 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:51 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:51 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:53 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:53 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:53 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:53 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:55 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:55 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:55 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:55 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:57 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:57 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:57 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:57 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:59 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:59 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:43:59 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:43:59 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:01 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:01 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:01 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:01 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:03 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:03 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:03 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:03 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:05 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:05 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:05 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:05 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:07 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:07 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:07 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:07 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:09 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:09 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:09 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:09 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:11 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:11 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:11 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:11 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:12 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:12 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:12 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:12 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:14 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:14 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:14 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:14 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:16 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:16 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:16 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:16 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:18 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:18 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:18 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:18 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:20 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:20 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:20 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:20 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:22 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:22 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:22 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:22 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:24 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:24 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:24 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:24 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:26 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:26 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:26 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:26 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:28 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:28 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:28 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:28 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:29 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:29 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:29 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:29 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:31 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:31 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:31 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:31 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:33 WARN MemoryStore: Not enough space to cache rdd_213_5 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:33 WARN MemoryStore: Not enough space to cache rdd_213_8 in memory! (computed 17.0 MiB so far)\n",
      "24/05/23 16:44:33 WARN MemoryStore: Not enough space to cache rdd_213_6 in memory! (computed 33.0 MiB so far)\n",
      "24/05/23 16:44:33 WARN MemoryStore: Not enough space to cache rdd_213_7 in memory! (computed 33.0 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# model Logistic Regression (lr): train\n",
    "\n",
    "model_LR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coefficients are: [0.0017840862461090978,-4.904200691325669e-09,-1.794951436855724,-1.3438215028369287,-2.976242141230391,-1.1510835511863873,-2.942112930866071,1.3438215028365235,-1.3438215028369287,36.88531824595333]\n",
      "The independent term is: -18.670206043122782\n"
     ]
    }
   ],
   "source": [
    "# coefficients and intercept\n",
    "\n",
    "print('The coefficients are:', model_LR.coefficients)\n",
    "print('The independent term is:', model_LR.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 16:53:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:53:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:53:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:53:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:53:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:53:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:53:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:53:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:53:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:53:53 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:54:28 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_344_4 in memory.\n",
      "24/05/23 16:54:28 WARN MemoryStore: Not enough space to cache rdd_344_4 in memory! (computed 384.0 B so far)\n",
      "24/05/23 16:54:28 WARN BlockManager: Block rdd_344_4 could not be removed as it was not found on disk or in memory\n",
      "24/05/23 16:54:28 WARN BlockManager: Putting block rdd_344_4 failed\n",
      "24/05/23 16:54:28 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_344_5 in memory.\n",
      "24/05/23 16:54:28 WARN MemoryStore: Not enough space to cache rdd_344_5 in memory! (computed 384.0 B so far)\n",
      "24/05/23 16:54:28 WARN BlockManager: Block rdd_344_5 could not be removed as it was not found on disk or in memory\n",
      "24/05/23 16:54:28 WARN BlockManager: Putting block rdd_344_5 failed\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC: 0.999999834776539\n",
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.006700844286839...|\n",
      "|0.0| 0.01154040679797463|\n",
      "|0.0|  0.0159563890501342|\n",
      "|0.0|0.020459156402843792|\n",
      "|0.0| 0.02468288954153328|\n",
      "|0.0| 0.02910749283017133|\n",
      "|0.0| 0.03329789129447736|\n",
      "|0.0|0.037323915329927064|\n",
      "|0.0|0.041504255918341545|\n",
      "|0.0| 0.04545843798313725|\n",
      "|0.0| 0.04916232262217445|\n",
      "|0.0|0.052762180087704676|\n",
      "|0.0| 0.05620657152873966|\n",
      "|0.0| 0.05973544912726374|\n",
      "|0.0| 0.06312466593483645|\n",
      "|0.0| 0.06659606995683734|\n",
      "|0.0| 0.06998499939652744|\n",
      "|0.0| 0.07327335007730196|\n",
      "|0.0| 0.07663383009661308|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The ROC is: None\n",
      "+--------------------+---------+\n",
      "|              recall|precision|\n",
      "+--------------------+---------+\n",
      "|                 0.0|      1.0|\n",
      "|0.006700844286839...|      1.0|\n",
      "| 0.01154040679797463|      1.0|\n",
      "|  0.0159563890501342|      1.0|\n",
      "|0.020459156402843792|      1.0|\n",
      "| 0.02468288954153328|      1.0|\n",
      "| 0.02910749283017133|      1.0|\n",
      "| 0.03329789129447736|      1.0|\n",
      "|0.037323915329927064|      1.0|\n",
      "|0.041504255918341545|      1.0|\n",
      "| 0.04545843798313725|      1.0|\n",
      "| 0.04916232262217445|      1.0|\n",
      "|0.052762180087704676|      1.0|\n",
      "| 0.05620657152873966|      1.0|\n",
      "| 0.05973544912726374|      1.0|\n",
      "| 0.06312466593483645|      1.0|\n",
      "| 0.06659606995683734|      1.0|\n",
      "| 0.06998499939652744|      1.0|\n",
      "| 0.07327335007730196|      1.0|\n",
      "| 0.07663383009661308|      1.0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "pr is None\n"
     ]
    }
   ],
   "source": [
    "# model summary\n",
    "\n",
    "summary_lr = model_LR.summary\n",
    "\n",
    "print('The area under ROC:',summary_lr.areaUnderROC)\n",
    "print('The ROC is:',summary_lr.roc.show())\n",
    "print('pr is',summary_lr.pr.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6857318254055379,\n",
       " 0.2690071886860956,\n",
       " 0.14569401531672682,\n",
       " 0.0633077848905353,\n",
       " 0.030850318141735108,\n",
       " 0.014861588066495502,\n",
       " 0.007239337949729511,\n",
       " 0.003568679224882079,\n",
       " 0.0017445779069082733,\n",
       " 0.0008377751491601997,\n",
       " 0.0003956771712826265,\n",
       " 0.00020442638481504822,\n",
       " 0.00010052991846869381,\n",
       " 4.878542277309884e-05,\n",
       " 2.350608874400407e-05,\n",
       " 1.1786821109645318e-05,\n",
       " 5.7719100272044e-06,\n",
       " 2.8424634171807145e-06,\n",
       " 1.4042934235611005e-06,\n",
       " 6.982315769676967e-07,\n",
       " 3.466621216250634e-07,\n",
       " 1.5532829279736542e-07,\n",
       " 7.94559581226837e-08,\n",
       " 3.946691898244228e-08,\n",
       " 1.977006227513949e-08,\n",
       " 9.859897226488465e-09,\n",
       " 4.915916074409834e-09]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_lr.objectiveHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B. Evaluators of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_LR.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 147:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+--------------------+----------+\n",
      "|step|amount|type_CASH_IN|type_CASH_OUT|type_DEBIT|type_PAYMENT|type_TRANSFER|type2_CC|type2_CM|isFraud|         my_features|       rawPrediction|         probability|prediction|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+--------------------+----------+\n",
      "|   1|  6.93|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[21.3560649965366...|[0.99999999946889...|       0.0|\n",
      "|   1| 15.06|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[21.3560650364077...|[0.99999999946889...|       0.0|\n",
      "|   1| 27.55|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[21.3560650976612...|[0.99999999946889...|       0.0|\n",
      "|   1| 96.32|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[21.3560654349231...|[0.99999999946889...|       0.0|\n",
      "|   1|163.72|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[21.3560657654662...|[0.99999999946889...|       0.0|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_LR.transform(test).select(\"prediction\", \"isFraud\").rdd.map(lambda x: (float(x[0]),float(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "24/05/23 16:57:26 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "\n",
    "metrics = BinaryClassificationMetrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.mllib.evaluation.BinaryClassificationMetrics"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.evaluation.BinaryClassificationMetrics at 0x10709d390>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 16:57:36 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC: 1.0\n",
      "The area under PR is: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('The area under ROC:',metrics.areaUnderROC)\n",
    "print('The area under PR is:',metrics.areaUnderPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C. Tunnig of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "\n",
    "stages = [assembler,lr]\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "\n",
    "params = ParamGridBuilder().addGrid(lr.elasticNetParam,[0.0,0.5,1.0])\\\n",
    "        .addGrid(lr.regParam, [0.1,2.0]) \\\n",
    "        .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluators: TrainValidationSplit ########################\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "            .setMetricName(\"areaUnderROC\")\\\n",
    "            .setRawPredictionCol(\"prediction\") \\\n",
    "            .setLabelCol(\"isFraud\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossValidator\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=params, \\\n",
    "                          evaluator=evaluator,numFolds=3\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 22:26:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 22:26:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/23 22:26:12 WARN MemoryStore: Not enough space to cache rdd_241_9 in memory! (computed 24.4 MiB so far)\n",
      "24/05/23 22:26:12 WARN MemoryStore: Not enough space to cache rdd_241_5 in memory! (computed 24.3 MiB so far)\n",
      "24/05/23 22:26:12 WARN MemoryStore: Not enough space to cache rdd_241_6 in memory! (computed 24.3 MiB so far)\n",
      "24/05/23 22:26:12 WARN MemoryStore: Not enough space to cache rdd_241_8 in memory! (computed 24.4 MiB so far)\n",
      "24/05/23 22:26:12 WARN MemoryStore: Not enough space to cache rdd_241_7 in memory! (computed 24.4 MiB so far)\n",
      "24/05/23 22:26:12 WARN BlockManager: Persisting block rdd_241_8 to disk instead.\n",
      "24/05/23 22:26:12 WARN BlockManager: Persisting block rdd_241_7 to disk instead.\n",
      "24/05/23 22:26:12 WARN BlockManager: Persisting block rdd_241_5 to disk instead.\n",
      "24/05/23 22:26:12 WARN BlockManager: Persisting block rdd_241_6 to disk instead.\n",
      "24/05/23 22:26:12 WARN BlockManager: Persisting block rdd_241_9 to disk instead.\n",
      "24/05/23 22:26:15 WARN MemoryStore: Not enough space to cache rdd_241_1 in memory! (computed 23.7 MiB so far)\n",
      "24/05/23 22:26:15 WARN MemoryStore: Not enough space to cache rdd_241_4 in memory! (computed 45.9 MiB so far)\n",
      "24/05/23 22:26:15 WARN MemoryStore: Not enough space to cache rdd_241_0 in memory! (computed 45.8 MiB so far)\n",
      "24/05/23 22:26:15 WARN MemoryStore: Not enough space to cache rdd_241_3 in memory! (computed 45.9 MiB so far)\n",
      "24/05/23 22:26:15 WARN BlockManager: Persisting block rdd_241_4 to disk instead.\n",
      "24/05/23 22:26:15 WARN BlockManager: Persisting block rdd_241_1 to disk instead.\n",
      "24/05/23 22:26:15 WARN BlockManager: Persisting block rdd_241_0 to disk instead.\n",
      "24/05/23 22:26:15 WARN BlockManager: Persisting block rdd_241_3 to disk instead.\n",
      "24/05/23 22:26:16 WARN MemoryStore: Not enough space to cache rdd_241_2 in memory! (computed 46.0 MiB so far)\n",
      "24/05/23 22:26:16 WARN BlockManager: Persisting block rdd_241_2 to disk instead.\n",
      "24/05/23 22:26:33 WARN BlockManager: Block rdd_241_7 could not be removed as it was not found on disk or in memory\n",
      "24/05/23 22:26:35 ERROR Executor: Exception in task 7.0 in stage 91.0 (TID 310)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5898/1433368263.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2295/1547901075.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2658/1544716998.apply(Unknown Source)\n",
      "24/05/23 22:26:44 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 91.0 (TID 310),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5898/1433368263.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2295/1547901075.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2658/1544716998.apply(Unknown Source)\n",
      "24/05/23 22:26:44 WARN MemoryStore: Not enough space to cache rdd_241_2 in memory! (computed 46.0 MiB so far)\n",
      "24/05/23 22:26:45 WARN TaskSetManager: Lost task 7.0 in stage 91.0 (TID 310) (192.168.1.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5898/1433368263.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2295/1547901075.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2658/1544716998.apply(Unknown Source)\n",
      "\n",
      "24/05/23 22:26:45 ERROR TaskSetManager: Task 7 in stage 91.0 failed 1 times; aborting job\n",
      "24/05/23 22:26:45 WARN BlockManager: Putting block rdd_241_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/23 22:26:45 WARN BlockManager: Putting block rdd_241_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/23 22:26:45 WARN BlockManager: Putting block rdd_241_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/23 22:26:45 WARN BlockManager: Block rdd_241_8 could not be removed as it was not found on disk or in memory\n",
      "24/05/23 22:26:45 WARN BlockManager: Block rdd_241_5 could not be removed as it was not found on disk or in memory\n",
      "24/05/23 22:26:45 WARN BlockManager: Block rdd_241_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/23 22:26:45 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 91.0 failed 1 times, most recent failure: Lost task 7.0 in stage 91.0 (TID 310) (192.168.1.4 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5898/1433368263.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2295/1547901075.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2658/1544716998.apply(Unknown Source)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5898/1433368263.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2295/1547901075.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2658/1544716998.apply(Unknown Source)\n",
      "\n",
      "24/05/23 22:26:45 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$5948/209295414@6b18a01 rejected from java.util.concurrent.ThreadPoolExecutor@14640c8[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 309]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 51273)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/0n/zwbn7zz13l7c9kw1ccdb5qb80000gn/T/ipykernel_62814/170167318.py\", line 3, in <module>\n",
      "    cvModel = crossval.fit(train)\n",
      "              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 847, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 873, in next\n",
      "    raise value\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 847, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "                                                             ^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[109], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cross Validation Model: train\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m crossval\u001b[38;5;241m.\u001b[39mfit(train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitSingleModel(index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39mfit(dataset, paramMaps[index])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_showtraceback(etype, value, stb)\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(evalue),\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m gateway_client\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Model: train\n",
    "\n",
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPred = cvModel.transform(df_bank_par)\n",
    "\n",
    "rfResult = rfPred.toPandas()\n",
    "\n",
    "mpt.plot(rfResult.isFraud, rfResult.prediction,'bo')\n",
    "mpt.xlabel('isFraud')\n",
    "mpt.ylabel('Prediction')\n",
    "mpt.suptitle(\"Model Performance RMSE\")\n",
    "mpt.show()\n",
    "\n",
    "# Cross Validation Model: feature importance \n",
    "\n",
    "bestPipeline = cvModel.bestModel\n",
    "bestModel = bestPipeline.stages[1] # type: ignore\n",
    "\n",
    "importances = bestModel.featureImportances\n",
    "\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "mpt.bar(x_values,importances, orientation='vertical')\n",
    "mpt.xticks(x_values, columns, rotation = 40)\n",
    "mpt.ylabel('Importance')\n",
    "mpt.xlabel('Feature')\n",
    "mpt.title('Feature Importances')\n",
    "\n",
    "# Cross Validation Model: best hyperparameters\n",
    "\n",
    "print('numTrees - ', bestModel.getNumTrees)\n",
    "print('maxDepth - ', bestModel.getOrDefault('maxDepth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "            .addGrid(hashingTF.numFeatures,[10,100,1000])\\\n",
    "            .addGrid(lr.regParam,[0.1,0.01])\\\n",
    "            .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid,evaluator=BinaryClassificationEvaluator(), numFolds=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 00:10:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:10:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/20 00:10:44 WARN MemoryStore: Not enough space to cache rdd_2013_3 in memory! (computed 43.8 MiB so far)\n",
      "24/05/20 00:10:44 WARN MemoryStore: Not enough space to cache rdd_2013_9 in memory! (computed 45.8 MiB so far)\n",
      "24/05/20 00:10:45 WARN BlockManager: Persisting block rdd_2013_3 to disk instead.\n",
      "24/05/20 00:10:45 WARN BlockManager: Persisting block rdd_2013_9 to disk instead.\n",
      "24/05/20 00:10:45 WARN MemoryStore: Not enough space to cache rdd_2013_4 in memory! (computed 43.8 MiB so far)\n",
      "24/05/20 00:10:45 WARN BlockManager: Persisting block rdd_2013_4 to disk instead.\n",
      "24/05/20 00:10:48 WARN MemoryStore: Not enough space to cache rdd_2013_6 in memory! (computed 45.8 MiB so far)\n",
      "24/05/20 00:10:49 WARN BlockManager: Persisting block rdd_2013_6 to disk instead.\n",
      "24/05/20 00:10:49 WARN MemoryStore: Not enough space to cache rdd_2013_7 in memory! (computed 45.8 MiB so far)\n",
      "24/05/20 00:10:49 WARN BlockManager: Persisting block rdd_2013_7 to disk instead.\n",
      "24/05/20 00:11:00 WARN BlockManager: Block rdd_2013_5 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 00:11:01 WARN MemoryStore: Not enough space to cache rdd_2013_8 in memory! (computed 45.8 MiB so far)\n",
      "24/05/20 00:11:02 WARN BlockManager: Persisting block rdd_2013_8 to disk instead.\n",
      "24/05/20 00:11:04 ERROR Executor: Exception in task 5.0 in stage 908.0 (TID 7759)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/20 00:11:21 WARN BlockManager: Block rdd_2013_2 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 00:11:22 ERROR Executor: Exception in task 2.0 in stage 908.0 (TID 7756)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/20 00:11:27 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 908.0 (TID 7756),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/20 00:11:27 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 908.0 (TID 7759),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/20 00:11:41 WARN TaskSetManager: Lost task 5.0 in stage 908.0 (TID 7759) (192.168.1.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "24/05/20 00:11:46 WARN BlockManager: Block rdd_2013_4 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 00:11:46 ERROR Executor: Exception in task 4.0 in stage 908.0 (TID 7758)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/20 00:11:47 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 908.0 (TID 7758),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/20 00:11:49 ERROR TaskSetManager: Task 5 in stage 908.0 failed 1 times; aborting job\n",
      "24/05/20 00:12:00 WARN ShutdownHookManager: ShutdownHook '$anon$2' timeout, java.util.concurrent.TimeoutException\n",
      "java.util.concurrent.TimeoutException\n",
      "\tat java.util.concurrent.FutureTask.get(FutureTask.java:205)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.executeShutdown(ShutdownHookManager.java:124)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:95)\n",
      "24/05/20 00:12:01 WARN BlockManager: Putting block rdd_2013_9 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 00:12:01 WARN BlockManager: Putting block rdd_2013_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 00:12:01 WARN BlockManager: Putting block rdd_2013_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 00:12:01 WARN BlockManager: Putting block rdd_2013_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 00:12:01 WARN BlockManager: Putting block rdd_2013_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 00:12:01 WARN BlockManager: Block rdd_2013_7 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 00:12:01 WARN BlockManager: Block rdd_2013_8 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 00:12:01 WARN BlockManager: Block rdd_2013_0 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 00:12:01 WARN BlockManager: Block rdd_2013_9 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 00:12:01 WARN BlockManager: Putting block rdd_2013_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 00:12:01 WARN BlockManager: Putting block rdd_2013_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/20 00:12:01 WARN BlockManager: Block rdd_2013_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 00:12:01 WARN BlockManager: Block rdd_2013_3 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 00:12:01 WARN BlockManager: Block rdd_2013_6 could not be removed as it was not found on disk or in memory\n",
      "24/05/20 00:12:01 ERROR Instrumentation: org.apache.spark.SparkException: Job 513 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:91)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/05/20 00:12:01 WARN JobWaiter: Ignore failure\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 908.0 failed 1 times, most recent failure: Lost task 5.0 in stage 908.0 (TID 7759) (192.168.1.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "24/05/20 00:12:01 WARN SparkContext: Ignoring Exception while stopping SparkContext from shutdown hook\n",
      "java.lang.InterruptedException\n",
      "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1220)\n",
      "\tat java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335)\n",
      "\tat java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:339)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.stop(AsyncEventQueue.scala:143)\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.$anonfun$stop$1(LiveListenerBus.scala:224)\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.$anonfun$stop$1$adapted(LiveListenerBus.scala:224)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.LiveListenerBus.stop(LiveListenerBus.scala:224)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$13(SparkContext.scala:2269)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/20 00:12:02 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6520/863594140@4bbd6cb9 rejected from java.util.concurrent.ThreadPoolExecutor@4a3a5b9d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 7756]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/20 00:12:02 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6520/863594140@4e518ea8 rejected from java.util.concurrent.ThreadPoolExecutor@4a3a5b9d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 7756]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/20 00:12:02 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6520/863594140@766422d5 rejected from java.util.concurrent.ThreadPoolExecutor@4a3a5b9d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 7756]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/20 00:12:02 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6520/863594140@1571e192 rejected from java.util.concurrent.ThreadPoolExecutor@4a3a5b9d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 7756]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/20 00:12:02 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6520/863594140@6ac19331 rejected from java.util.concurrent.ThreadPoolExecutor@4a3a5b9d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 7756]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/20 00:12:02 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6520/863594140@1fd62c0b rejected from java.util.concurrent.ThreadPoolExecutor@4a3a5b9d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 7756]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/05/20 00:12:02 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$6520/863594140@67f4924c rejected from java.util.concurrent.ThreadPoolExecutor@4a3a5b9d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 7756]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:838)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 49369)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/0n/zwbn7zz13l7c9kw1ccdb5qb80000gn/T/ipykernel_1442/278333903.py\", line 1, in <module>\n",
      "    cvModel = crossval.fit(train)\n",
      "              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 847, in _fit\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 873, in next\n",
      "    raise value\n",
      "  File \"/opt/anaconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 847, in <lambda>\n",
      "    for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):\n",
      "                                                             ^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/util.py\", line 342, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py\", line 113, in singleTask\n",
      "    index, model = next(modelIter)\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 98, in __next__\n",
      "    return index, self.fitSingleModel(index)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 156, in fitSingleModel\n",
      "    return estimator.fit(dataset, paramMaps[index])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 203, in fit\n",
      "    return self.copy(params)._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/pipeline.py\", line 134, in _fit\n",
      "    model = stage.fit(dataset)\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[234], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m crossval\u001b[38;5;241m.\u001b[39mfit(train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitSingleModel(index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39mfit(dataset, paramMaps[index])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mfit(dataset)\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2155\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2152\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_showtraceback(etype, value, stb)\n\u001b[1;32m   2156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2157\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py:559\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    553\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    554\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    556\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(evalue),\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    562\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m gateway_client\u001b[38;5;241m.\u001b[39msend_command(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception_cmd)\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "cvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.LogisticRegressionModel"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To better understand the model, we can examine its coefficients and intercept. The values represent the weights assigned to each feature and the bias term, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = model_LR.coefficients\n",
    "\n",
    "intercept = model_LR.intercept\n",
    "\n",
    "print(\"Coefficients: \", coefficients)\n",
    "\n",
    "print(\"Intercept: \", intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n",
      "|isfraud|prediction|         probability|\n",
      "+-------+----------+--------------------+\n",
      "|      0|       0.0|[0.99995463738199...|\n",
      "|      0|       0.0|[0.99995463721974...|\n",
      "|      0|       0.0|[0.99995463708139...|\n",
      "|      0|       0.0|[0.99995463697014...|\n",
      "|      0|       0.0|[0.99995463691691...|\n",
      "|      0|       0.0|[0.9999546368187,...|\n",
      "|      0|       0.0|[0.99995463681835...|\n",
      "|      0|       0.0|[0.99995463669316...|\n",
      "|      0|       0.0|[0.99995463663495...|\n",
      "|      0|       0.0|[0.99995463655208...|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions of the logistic regression model using the test dataset\n",
    "\n",
    "predictions = model_LR.transform(test)\n",
    "\n",
    "predictions.select('isfraud','prediction','probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC - ROC Curve (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 17:03:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:43 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:54 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:54 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:54 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:54 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:54 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:54 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:54 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:54 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:54 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 17:03:54 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"isFraud\")\n",
    "lr_eval2 = MulticlassClassificationEvaluator(probabilityCol=\"probability\", labelCol=\"isFraud\")\n",
    "\n",
    "lr_AUC = lr_eval.evaluate(predictions) # AUC performance\n",
    "lr_ACC = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"accuracy\"}) # Accuracy performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 17:06:55 ERROR Inbox: An error happened while processing message in the inbox for HeartbeatReceiver\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "24/05/19 17:06:55 ERROR Utils: Uncaught exception in thread executor-heartbeater\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "Exception in thread \"dispatcher-HeartbeatReceiver\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o738.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ROC graphic\u001b[39;00m\n\u001b[1;32m      3\u001b[0m preds_and_labels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misFraud\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m preds_and_labels_collect \u001b[38;5;241m=\u001b[39m preds_and_labels\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      6\u001b[0m preds_and_labels_list \u001b[38;5;241m=\u001b[39m [ (\u001b[38;5;28mfloat\u001b[39m(i[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(i[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m preds_and_labels_collect  ]\n\u001b[1;32m      7\u001b[0m preds_and_labels \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize(preds_and_labels_list)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1261\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1261\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o738.collectToPython.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 17:07:02 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 163888 ms exceeds timeout 120000 ms\n",
      "24/05/19 17:07:02 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# ROC graphic\n",
    "\n",
    "preds_and_labels = predictions.select(\"probability\", \"isFraud\")\n",
    "preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(preds_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Area Under ROC:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 02:31:06 WARN TaskSetManager: Stage 322 contains a task of very large size (6636 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/19 02:31:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 02:31:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 02:31:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 02:31:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 02:31:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 02:31:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 02:31:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 02:31:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 02:31:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/19 02:31:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999998806070997"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Area under ROC\n",
    "\n",
    "print(\"Logistic Regression Area Under ROC:\")\n",
    "\n",
    "metrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "y_test = [i[1] for i in preds_and_labels_list]\n",
    "y_score = [i[0] for i in preds_and_labels_list]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "mpt.figure(figsize=(5,4))\n",
    "mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "mpt.plot([0,1],[0,1],'k--')\n",
    "mpt.xlim([0.0,1.0])\n",
    "mpt.ylim([0.0,1.05])\n",
    "mpt.xlabel('False Positive Rate')\n",
    "mpt.ylabel('True Positive Rate')\n",
    "mpt.title('ROC Curve - Logistic Regression')\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to this graphic we have overfitting in our model. Therefore, we need to adjunts some hyperparameters in the model, which is in previos steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC - ROC\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"Prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_lr = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_lr = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_lr = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_lr = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "# let´s store the results of this model: Random Forest\n",
    "\n",
    "accuracy.append(accuracy_lr)\n",
    "\n",
    "precision.append(precision_lr)\n",
    "\n",
    "recall.append(recall_lr)\n",
    "\n",
    "auc_roc.append(auc_lr)\n",
    "\n",
    "# let´s store the name of the model: Logistic Regression \n",
    "name_model_ = \"Logistic Regression\"\n",
    "\n",
    "name_model.append(name_model_)\n",
    "\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_lr)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_lr)\n",
    "\n",
    "print(f\"Precsion: \", precision_lr)\n",
    "\n",
    "print(f\"Recall: \", recall_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "24/05/17 23:20:05 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confusion Matrix is:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/17 23:20:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 23:20:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1903276.,       0.],\n",
       "       [      0., 1491116.]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_bank_par\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:623\u001b[0m, in \u001b[0;36mDataFrame.printSchema\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mschema()\u001b[38;5;241m.\u001b[39mtreeString(level))\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 623\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mschema()\u001b[38;5;241m.\u001b[39mtreeString())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/17 23:21:34 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 23:21:34 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 23:21:40 WARN MemoryStore: Not enough space to cache rdd_791_8 in memory! (computed 24.4 MiB so far)\n",
      "24/05/17 23:21:40 WARN MemoryStore: Not enough space to cache rdd_791_0 in memory! (computed 23.6 MiB so far)\n",
      "24/05/17 23:21:40 WARN BlockManager: Persisting block rdd_791_8 to disk instead.\n",
      "24/05/17 23:21:40 WARN BlockManager: Persisting block rdd_791_0 to disk instead.\n",
      "24/05/17 23:21:40 WARN MemoryStore: Not enough space to cache rdd_791_5 in memory! (computed 24.3 MiB so far)\n",
      "24/05/17 23:21:40 WARN MemoryStore: Not enough space to cache rdd_791_7 in memory! (computed 24.4 MiB so far)\n",
      "24/05/17 23:21:40 WARN BlockManager: Persisting block rdd_791_5 to disk instead.\n",
      "24/05/17 23:21:40 WARN BlockManager: Persisting block rdd_791_7 to disk instead.\n",
      "24/05/17 23:21:40 WARN MemoryStore: Not enough space to cache rdd_791_6 in memory! (computed 24.3 MiB so far)\n",
      "24/05/17 23:21:40 WARN BlockManager: Persisting block rdd_791_6 to disk instead.\n",
      "24/05/17 23:21:42 WARN MemoryStore: Not enough space to cache rdd_791_2 in memory! (computed 46.0 MiB so far)\n",
      "24/05/17 23:21:42 WARN MemoryStore: Not enough space to cache rdd_791_4 in memory! (computed 45.9 MiB so far)\n",
      "24/05/17 23:21:42 WARN BlockManager: Persisting block rdd_791_2 to disk instead.\n",
      "24/05/17 23:21:42 WARN BlockManager: Persisting block rdd_791_4 to disk instead.\n",
      "24/05/17 23:21:45 WARN MemoryStore: Not enough space to cache rdd_791_1 in memory! (computed 23.7 MiB so far)\n",
      "24/05/17 23:21:45 WARN BlockManager: Persisting block rdd_791_1 to disk instead.\n",
      "24/05/17 23:22:13 WARN BlockManager: Block rdd_791_1 could not be removed as it was not found on disk or in memory\n",
      "24/05/17 23:22:16 ERROR Executor: Exception in task 1.0 in stage 282.0 (TID 1588)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
      "24/05/17 23:22:17 WARN MemoryStore: Not enough space to cache rdd_791_2 in memory! (computed 46.0 MiB so far)\n",
      "24/05/17 23:22:17 WARN MemoryStore: Not enough space to cache rdd_791_0 in memory! (computed 45.8 MiB so far)\n",
      "24/05/17 23:22:18 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 282.0 (TID 1588),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
      "24/05/17 23:22:18 WARN MemoryStore: Not enough space to cache rdd_791_6 in memory! (computed 47.3 MiB so far)\n",
      "24/05/17 23:22:18 WARN MemoryStore: Not enough space to cache rdd_791_8 in memory! (computed 47.3 MiB so far)\n",
      "24/05/17 23:22:18 WARN TaskSetManager: Lost task 1.0 in stage 282.0 (TID 1588) (192.168.1.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
      "\n",
      "24/05/17 23:22:18 ERROR TaskSetManager: Task 1 in stage 282.0 failed 1 times; aborting job\n",
      "24/05/17 23:22:19 WARN MemoryStore: Not enough space to cache rdd_791_5 in memory! (computed 24.3 MiB so far)\n",
      "24/05/17 23:22:19 WARN MemoryStore: Not enough space to cache rdd_791_8 in memory! (computed 24.4 MiB so far)\n",
      "24/05/17 23:22:19 WARN MemoryStore: Not enough space to cache rdd_791_2 in memory! (computed 23.7 MiB so far)\n",
      "24/05/17 23:22:19 WARN MemoryStore: Not enough space to cache rdd_791_6 in memory! (computed 24.3 MiB so far)\n",
      "24/05/17 23:22:19 WARN MemoryStore: Not enough space to cache rdd_791_5 in memory! (computed 24.3 MiB so far)\n",
      "24/05/17 23:22:19 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 282.0 failed 1 times, most recent failure: Lost task 1.0 in stage 282.0 (TID 1588) (192.168.1.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
      "\n",
      "24/05/17 23:22:19 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 282.0 failed 1 times, most recent failure: Lost task 1.0 in stage 282.0 (TID 1588) (192.168.1.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
      "\n",
      "24/05/17 23:22:19 WARN TaskSetManager: Lost task 5.0 in stage 283.0 (TID 1602) (192.168.1.3 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 282.0 failed 1 times, most recent failure: Lost task 1.0 in stage 282.0 (TID 1588) (192.168.1.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/17 23:22:19 WARN TaskSetManager: Lost task 6.0 in stage 283.0 (TID 1603) (192.168.1.3 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 282.0 failed 1 times, most recent failure: Lost task 1.0 in stage 282.0 (TID 1588) (192.168.1.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/17 23:22:19 WARN TaskSetManager: Lost task 2.0 in stage 283.0 (TID 1599) (192.168.1.3 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 282.0 failed 1 times, most recent failure: Lost task 1.0 in stage 282.0 (TID 1588) (192.168.1.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n",
      "\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n",
      "\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n",
      "\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/05/17 23:22:19 WARN BlockManager: Putting block rdd_791_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "24/05/17 23:22:19 WARN BlockManager: Block rdd_791_1 could not be removed as it was not found on disk or in memory\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4336.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 282.0 failed 1 times, most recent failure: Lost task 1.0 in stage 282.0 (TID 1588) (192.168.1.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m grid \u001b[38;5;241m=\u001b[39m ParamGridBuilder()\u001b[38;5;241m.\u001b[39maddGrid(lr\u001b[38;5;241m.\u001b[39mmaxIter,[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[1;32m      7\u001b[0m cv \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mlr,  estimatorParamMaps\u001b[38;5;241m=\u001b[39mgrid,evaluator\u001b[38;5;241m=\u001b[39mevaluator, parallelism\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mfit(train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitSingleModel(index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39mfit(dataset, paramMaps[index])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4336.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 282.0 failed 1 times, most recent failure: Lost task 1.0 in stage 282.0 (TID 1588) (192.168.1.3 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:335)\n\tat org.apache.spark.sql.execution.columnar.ColumnBuilder$.ensureFreeSpace(ColumnBuilder.scala:167)\n\tat org.apache.spark.sql.execution.columnar.BasicColumnBuilder.appendFrom(ColumnBuilder.scala:73)\n\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.org$apache$spark$sql$execution$columnar$NullableColumnBuilder$$super$appendFrom(ColumnBuilder.scala:93)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom(NullableColumnBuilder.scala:61)\n\tat org.apache.spark.sql.execution.columnar.NullableColumnBuilder.appendFrom$(NullableColumnBuilder.scala:54)\n\tat org.apache.spark.sql.execution.columnar.ComplexColumnBuilder.appendFrom(ColumnBuilder.scala:93)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:105)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:80)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:290)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$2.next(InMemoryRelation.scala:287)\n\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:785)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1606)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1604)\n\tat org.apache.spark.storage.BlockManager$$Lambda$5940/1082377634.apply(Unknown Source)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:88)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1604)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2258/29328023.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2622/165740715.apply(Unknown Source)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50613)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# it does not work yet.....\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(lr.maxIter,[0,1]).build()\n",
    "\n",
    "cv = CrossValidator(estimator=lr,  estimatorParamMaps=grid,evaluator=evaluator, parallelism=2)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "#cvModel.avgMetrics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train the model Decision Tree (dt)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dt \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misFraud\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m dt\u001b[38;5;241m.\u001b[39mexplainParams\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/classification.py:1807\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.__init__\u001b[0;34m(self, featuresCol, labelCol, predictionCol, probabilityCol, rawPredictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, checkpointInterval, impurity, seed, weightCol, leafCol, minWeightFractionPerNode)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;124;03m__init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;124;03m         probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;124;03m         seed=None, weightCol=None, leafCol=\"\", minWeightFractionPerNode=0.0)\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;28msuper\u001b[39m(DecisionTreeClassifier, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m-> 1807\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.classification.DecisionTreeClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid\n\u001b[1;32m   1809\u001b[0m )\n\u001b[1;32m   1810\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:84\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     82\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m _jvm()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m java_class\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_obj(\u001b[38;5;241m*\u001b[39mjava_args)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(\n\u001b[1;32m   1713\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFLECTION_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   1714\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART)\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# train the model Decision Tree (dt)\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='isFraud')\n",
    "dt.explainParams\n",
    "#model_dt = dt.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 171:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+---------------+-----------+----------+\n",
      "|step|amount|type_CASH_IN|type_CASH_OUT|type_DEBIT|type_PAYMENT|type_TRANSFER|type2_CC|type2_CM|isFraud|            features|  rawPrediction|probability|prediction|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+---------------+-----------+----------+\n",
      "| 1.0|  8.73|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0| 23.31|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0| 38.66|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0| 96.32|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|106.81|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|137.88|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0| 151.2|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0| 181.0|           0|            0|         0|           0|            1|       1|       0|      1|(10,[0,1,5,7,9],[...|[0.0,3564025.0]|  [0.0,1.0]|       1.0|\n",
      "| 1.0| 181.0|           0|            1|         0|           0|            0|       1|       0|      1|(10,[0,1,2,7,9],[...|[0.0,3564025.0]|  [0.0,1.0]|       1.0|\n",
      "| 1.0|186.22|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|207.75|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|226.29|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|244.87|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|270.78|           1|            0|         0|           0|            0|       1|       0|      0|(10,[0,1,4,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0| 272.8|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|351.26|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0| 353.9|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|360.13|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|399.66|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|406.11|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|436.15|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|449.97|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|454.51|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|484.57|           1|            0|         0|           0|            0|       1|       0|      0|(10,[0,1,4,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|502.61|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|507.12|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|525.22|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|553.25|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|577.01|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|588.33|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0| 593.4|           0|            1|         0|           0|            0|       1|       0|      0|(10,[0,1,2,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|622.15|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|624.61|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|625.92|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|635.55|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|642.26|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|700.86|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|704.74|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|708.59|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|727.25|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|748.42|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|749.26|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|793.64|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|818.99|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|842.33|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|846.91|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|863.08|           1|            0|         0|           0|            0|       1|       0|      0|(10,[0,1,4,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|885.95|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|891.93|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "| 1.0|909.33|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[4441370.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+---------------+-----------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# make predictions of the decision tree model using the test dataset\n",
    "\n",
    "predictions = model_dt.transform(test)\n",
    "\n",
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC - ROC Curve (Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"isFraud\")\n",
    "lr_eval2 = MulticlassClassificationEvaluator(probabilityCol=\"probability\", labelCol=\"isFraud\")\n",
    "\n",
    "lr_AUC = lr_eval.evaluate(predictions) # AUC performance\n",
    "lr_ACC = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"accuracy\"}) # Accuracy performance\n",
    "\n",
    "# ROC graphic\n",
    "\n",
    "preds_and_labels = predictions.select(\"probability\", \"isFraud\")\n",
    "preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "# Area under ROC\n",
    "\n",
    "print(\"Logistic Regression Area Under ROC:\")\n",
    "\n",
    "metrics.areaUnderROC\n",
    "\n",
    "# Visualization\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "y_test = [i[1] for i in preds_and_labels_list]\n",
    "y_score = [i[0] for i in preds_and_labels_list]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "mpt.figure(figsize=(5,4))\n",
    "mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "mpt.plot([0,1],[0,1],'k--')\n",
    "mpt.xlim([0.0,1.0])\n",
    "mpt.ylim([0.0,1.05])\n",
    "mpt.xlabel('False Positive Rate')\n",
    "mpt.ylabel('True Positive Rate')\n",
    "mpt.title('ROC Curve - Logistic Regression')\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/17 20:15:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:06 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:12 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:18 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:15:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "[Stage 187:=======================================>                (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC:  1.0\n",
      "Accuracy:  1.0\n",
      "Precsion:  1.0\n",
      "Recall:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# AUC - ROC\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_dt = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\",)\n",
    "\n",
    "accuracy_dt = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "# let´s store the results of this model: Decision Tree\n",
    "\n",
    "accuracy.append(accuracy_dt)\n",
    "\n",
    "precision.append(precision_dt)\n",
    "\n",
    "recall.append(recall_dt)\n",
    "\n",
    "auc_roc.append(auc_dt)\n",
    "\n",
    "\n",
    "# let´s store the name of the model: Decision Tree\n",
    "name_model_ = \"Decision Tree\"\n",
    "\n",
    "name_model.append(name_model_)\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_dt)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_dt)\n",
    "\n",
    "print(f\"Precsion: \", precision_dt)\n",
    "\n",
    "print(f\"Recall: \", recall_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/17 20:15:49 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confusion Matrix is:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/17 20:16:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:16:19 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1905550.,       0.],\n",
       "       [      0., 1527301.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train the model Naive Bayes (nb)\n",
    "\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='isFraud')\n",
    "\n",
    "model_nb = nb.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+--------------------+----------+\n",
      "|step|amount|type_CASH_IN|type_CASH_OUT|type_DEBIT|type_PAYMENT|type_TRANSFER|type2_CC|type2_CM|isFraud|            features|       rawPrediction|         probability|prediction|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+--------------------+----------+\n",
      "| 1.0|  8.73|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.476857610643...|[0.99999999999999...|       0.0|\n",
      "| 1.0| 23.31|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.497407161157...|[0.99999999999999...|       0.0|\n",
      "| 1.0| 38.66|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.519041975998...|[0.99999999999999...|       0.0|\n",
      "| 1.0| 96.32|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.600309951486...|[0.99999999999999...|       0.0|\n",
      "| 1.0|106.81|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.615094916156...|[0.99999999999999...|       0.0|\n",
      "| 1.0|137.88|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.658886036489...|[0.99999999999999...|       0.0|\n",
      "| 1.0| 151.2|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.677659699921...|[0.99999999999999...|       0.0|\n",
      "| 1.0| 181.0|           0|            0|         0|           0|            1|       1|       0|      1|(10,[0,1,5,7,9],[...|[-61.829135031202...|[1.21755003135222...|       1.0|\n",
      "| 1.0| 181.0|           0|            1|         0|           0|            0|       1|       0|      1|(10,[0,1,2,7,9],[...|[-60.384545006022...|[5.09362655241750...|       1.0|\n",
      "| 1.0|186.22|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.727018085311...|[0.99999999999999...|       0.0|\n",
      "| 1.0|207.75|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.757363203456...|[0.99999999999999...|       0.0|\n",
      "| 1.0|226.29|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.783494113368...|[0.99999999999999...|       0.0|\n",
      "| 1.0|244.87|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.809681400649...|[0.99999999999999...|       0.0|\n",
      "| 1.0|270.78|           1|            0|         0|           0|            0|       1|       0|      0|(10,[0,1,4,7],[1....|[-33.604983765420...|[0.99999999327957...|       0.0|\n",
      "| 1.0| 272.8|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.849046897620...|[0.99999999999999...|       0.0|\n",
      "| 1.0|351.26|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.959631104293...|[0.99999999999999...|       0.0|\n",
      "| 1.0| 353.9|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.963352010559...|[0.99999999999999...|       0.0|\n",
      "| 1.0|360.13|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-33.972132785573...|[0.99999999999999...|       0.0|\n",
      "| 1.0|399.66|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.027847719167...|[0.99999999999999...|       0.0|\n",
      "| 1.0|406.11|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.036938569703...|[0.99999999999999...|       0.0|\n",
      "| 1.0|436.15|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.079277972819...|[0.99999999999999...|       0.0|\n",
      "| 1.0|449.97|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.098756353347...|[0.99999999999999...|       0.0|\n",
      "| 1.0|454.51|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[-37.379878282453...|[0.99999972134824...|       0.0|\n",
      "| 1.0|484.57|           1|            0|         0|           0|            0|       1|       0|      0|(10,[0,1,4,7],[1....|[-33.906306701244...|[0.99999999143060...|       0.0|\n",
      "| 1.0|502.61|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.172948969193...|[0.99999999999999...|       0.0|\n",
      "| 1.0|507.12|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.179305517397...|[0.99999999999999...|       0.0|\n",
      "| 1.0|525.22|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[-37.479539374141...|[0.99999969802354...|       0.0|\n",
      "| 1.0|553.25|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.244322716655...|[0.99999999999999...|       0.0|\n",
      "| 1.0|577.01|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.277810873048...|[0.99999999999999...|       0.0|\n",
      "| 1.0|588.33|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[-37.568488765973...|[0.99999967556173...|       0.0|\n",
      "| 1.0| 593.4|           0|            1|         0|           0|            0|       1|       0|      0|(10,[0,1,2,7],[1....|[-33.591845410752...|[0.99453377342915...|       0.0|\n",
      "| 1.0|622.15|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.341432732457...|[0.99999999999999...|       0.0|\n",
      "| 1.0|624.61|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.344899940568...|[0.99999999999999...|       0.0|\n",
      "| 1.0|625.92|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[-37.621469397236...|[0.99999966139669...|       0.0|\n",
      "| 1.0|635.55|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.360319150624...|[0.99999999999999...|       0.0|\n",
      "| 1.0|642.26|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[-37.644499551927...|[0.99999965504800...|       0.0|\n",
      "| 1.0|700.86|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.452369297678...|[0.99999999999999...|       0.0|\n",
      "| 1.0|704.74|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.457837902342...|[0.99999999999999...|       0.0|\n",
      "| 1.0|708.59|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.463264223979...|[0.99999999999999...|       0.0|\n",
      "| 1.0|727.25|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.489564265995...|[0.99999999999999...|       0.0|\n",
      "| 1.0|748.42|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.519401987831...|[0.99999999999999...|       0.0|\n",
      "| 1.0|749.26|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.520585912552...|[0.99999999999999...|       0.0|\n",
      "| 1.0|793.64|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.583136601975...|[0.99999999999999...|       0.0|\n",
      "| 1.0|818.99|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[-37.893588856609...|[0.99999957828923...|       0.0|\n",
      "| 1.0|842.33|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.651761952764...|[0.99999999999999...|       0.0|\n",
      "| 1.0|846.91|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.658217161362...|[0.99999999999999...|       0.0|\n",
      "| 1.0|863.08|           1|            0|         0|           0|            0|       1|       0|      0|(10,[0,1,4,7],[1....|[-34.439791637108...|[0.99999998682258...|       0.0|\n",
      "| 1.0|885.95|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.713241472202...|[0.99999999999999...|       0.0|\n",
      "| 1.0|891.93|           0|            0|         1|           0|            0|       1|       0|      0|(10,[0,1,6,7],[1....|[-37.996392986544...|[0.99999954182964...|       0.0|\n",
      "| 1.0|909.33|           0|            0|         0|           1|            0|       0|       1|      0|(10,[0,1,3,8],[1....|[-34.746194043602...|[0.99999999999999...|       0.0|\n",
      "+----+------+------------+-------------+----------+------------+-------------+--------+--------+-------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# make predictions of the naive bayes model using the test dataset\n",
    "\n",
    "predictions = model_nb.transform(test)\n",
    "\n",
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC - ROC Curve (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"isFraud\")\n",
    "lr_eval2 = MulticlassClassificationEvaluator(probabilityCol=\"probability\", labelCol=\"isFraud\")\n",
    "\n",
    "lr_AUC = lr_eval.evaluate(predictions) # AUC performance\n",
    "lr_ACC = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"accuracy\"}) # Accuracy performance\n",
    "\n",
    "# ROC graphic\n",
    "\n",
    "preds_and_labels = predictions.select(\"probability\", \"isFraud\")\n",
    "preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "# Area under ROC\n",
    "\n",
    "print(\"Logistic Regression Area Under ROC:\")\n",
    "\n",
    "metrics.areaUnderROC\n",
    "\n",
    "# Visualization\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "y_test = [i[1] for i in preds_and_labels_list]\n",
    "y_score = [i[0] for i in preds_and_labels_list]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "mpt.figure(figsize=(5,4))\n",
    "mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "mpt.plot([0,1],[0,1],'k--')\n",
    "mpt.xlim([0.0,1.0])\n",
    "mpt.ylim([0.0,1.05])\n",
    "mpt.xlabel('False Positive Rate')\n",
    "mpt.ylabel('True Positive Rate')\n",
    "mpt.title('ROC Curve - Logistic Regression')\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/17 20:17:00 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:00 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:00 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:00 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:00 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:00 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:00 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:00 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:00 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:00 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:17 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:24 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "[Stage 215:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC:  0.26641219845130204\n",
      "Accuracy:  0.6753345834118638\n",
      "Precsion:  0.6772445219431403\n",
      "Recall:  0.6753345834118638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# AUC - ROC\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_nb = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\",)\n",
    "\n",
    "accuracy_nb = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_nb = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_nb = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "# let´s store the results of this model: Naive Bayes\n",
    "\n",
    "accuracy.append(accuracy_nb)\n",
    "\n",
    "precision.append(precision_nb)\n",
    "\n",
    "recall.append(recall_nb)\n",
    "\n",
    "auc_roc.append(auc_nb)\n",
    "\n",
    "# let´s store the name of the model: Naive Bayes\n",
    "name_model_ = \"Naive Bayes\"\n",
    "\n",
    "name_model.append(name_model_)\n",
    "\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_nb)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_nb)\n",
    "\n",
    "print(f\"Precsion: \", precision_nb)\n",
    "\n",
    "print(f\"Recall: \", recall_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/17 20:17:58 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confusion Matrix is:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/17 20:17:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/05/17 20:17:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1557022.,  348528.],\n",
       "       [ 766000.,  761301.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Evaluation and Selection of the model\n",
    "\n",
    "We´ll evaluate the models using the metrics used in the previous step and we´ll select the model with the best performance. As first step, let´s create a dictionary with the results of every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Name_Model': name_model,\n",
    "    'Accuracy':accuracy,\n",
    "    'Precision':precision,\n",
    "    'Recall':recall,\n",
    "    'AUC_ROC':auc_roc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name_Model': ['Random Forest'],\n",
       " 'Accuracy': [0.3136234694614361],\n",
       " 'Precision': [1.0],\n",
       " 'Recall': [0.3136234694614361],\n",
       " 'AUC_ROC': [1.0]}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "type(results['Accuracy'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let´s create a pandas dataframe with the results dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('Name_Model', inplace=True)\n",
    "#results_df.set_index(\"Name_Model\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC_ROC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name_Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.313623</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.313623</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy  Precision    Recall  AUC_ROC\n",
       "Name_Model                                           \n",
       "Random Forest  0.313623        1.0  0.313623      1.0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1 entries, Random Forest to Random Forest\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Accuracy   1 non-null      float64\n",
      " 1   Precision  1 non-null      float64\n",
      " 2   Recall     1 non-null      float64\n",
      " 3   AUC_ROC    1 non-null      float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 40.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s visualize these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"results_df\" dataframe\n",
    "\n",
    "colors = ['#0077b6','#CDDBF3','#9370DB','#DDA0DD']\n",
    "results_df.plot(kind='bar', figsize=(12,6), colormap='viridis', rot=0)\n",
    "mpt.title('Comparison of metrics per model')\n",
    "mpt.xlabel('Models')\n",
    "mpt.ylabel('Score')\n",
    "mpt.legend(title = 'Metrics')\n",
    "mpt.tight_layout\n",
    "mpt.show()\n",
    "\n",
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transpose of the \"results_df\" dataframe\n",
    "\n",
    "results_df = results_df.T\n",
    "colors = ['#0077b6','#CDDBF3','#9370DB','#DDA0DD']\n",
    "results_df.plot(kind='bar', figsize=(12,6), colormap='viridis', rot=0)\n",
    "mpt.title('Comparison of metrics per model')\n",
    "mpt.xlabel('Metrics')\n",
    "mpt.ylabel('Score')\n",
    "mpt.legend(title = 'Models')\n",
    "mpt.tight_layout\n",
    "mpt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: Random Forest\n",
    "\n",
    "model_RF.save(\"randomF_model\")\n",
    "\n",
    "# model: Logistic Regression\n",
    "\n",
    "model_LR.save(\"logit_model\")\n",
    "\n",
    "# model: Decision Tree\n",
    "\n",
    "model_dt.save(\"decisionT_model\")\n",
    "\n",
    "# model: Naive Bayes\n",
    "\n",
    "model_nb.save(\"naiveB_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: Random Forest\n",
    "\n",
    "loaded_model_RF = RandomForestClassifier.load(\"randomF_model\")\n",
    "\n",
    "# model: Logistic Regression\n",
    "\n",
    "loaded_model_LR = LogisticRegression.load(\"logit_model\")\n",
    "\n",
    "# model: Decision Tree\n",
    "\n",
    "loaded_model_LR = DecisionTreeClassifier.load(\"decisionT_model\")\n",
    "\n",
    "# model: Naive Bayes\n",
    "\n",
    "loaded_model_LR = NaiveBayes.load(\"naiveB_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
