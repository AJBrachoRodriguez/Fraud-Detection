{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Fraud Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PaySim simulates mobile money transactions based on a sample of real transacions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world. The objective of the project is to predict if a transaction is fraudulent or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ll use PySpark to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from  pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import functools\n",
    "import seaborn as sb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as mpt\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "#from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "global df_bank, results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark dataframe \n",
    "\n",
    "df = spark.read.csv('fraudDetection.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s take a look to the data with the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 11 columns, some of them are numerical and others are categorical. Let´s count the number of registers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total number of registers is:\",df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have more than six miliions of transactions in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firstly, we´ll create a function to create a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.1.- creation of a new variable: type2\n",
    "\n",
    "df_type2 = df.withColumn(\"type2\",f.concat(f.substring(\"nameOrig\",1,1),f.substring(\"nameDest\",1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve created a new column named \"type2\" which is composed by the first character of the column \"nameOrig\" and the first character of the column \"nameDest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.2.1.- One Hot Encoding: column \"type\"\n",
    "\n",
    "df_type2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ll use some libraries of Spark for Machine Learning (SparkML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### StringIndexer Initialization\n",
    "### column: type\n",
    "\n",
    "indexer_type = StringIndexer(inputCol=\"type\",outputCol=\"types_indexed\")\n",
    "indexerModel_type = indexer_type.fit(df_type2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform the DataFrame using the fitted StringIndexer model\n",
    "\n",
    "indexed_df_type2 = indexerModel_type.transform(df_type2)\n",
    "indexed_df_type2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we´ve set each of the elements of the \"type\" column into indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply One-Hot-Encoding to the indexed column, that is, \n",
    "### \"types_indexed\"\n",
    "\n",
    "encoder_type = OneHotEncoder(dropLast=False, inputCol=\"types_indexed\", outputCol=\"types_onehot\")\n",
    "encoder_type_df = encoder_type.fit(indexed_df_type2).transform(indexed_df_type2)\n",
    "encoder_type_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type_df_split = encoder_type_df.select('*',vector_to_array('types_onehot').alias('types_onehot_split'))\n",
    "encoder_type_df_split.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now, we´ll split the \"types_onehot_split\" into five columns, one per category\n",
    "\n",
    "num_categories = len(encoder_type_df_split.first()['types_onehot_split'])\n",
    "cols_expanded = [(f.col('types_onehot_split')[i].alias(f\"{indexerModel_type.labels[i]}\")) for i in range(num_categories)]\n",
    "type_df = encoder_type_df_split.select('*',*cols_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve applied One-Hot-Encoding to the column \"type\" resulting in five new columns:\n",
    "+ CASH_OUT\n",
    "+ CASH_IN\n",
    "+ PAYMENT\n",
    "+ TRANSFER \n",
    "+ DEBIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll apply this procedure to the column \"type2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.2.2.- One Hot Encoding: column \"type2\"\n",
    "\n",
    "type_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### StringIndexer Initialization\n",
    "### column: type2\n",
    "\n",
    "indexer_type = StringIndexer(inputCol=\"type2\",outputCol=\"types_indexed2\")\n",
    "indexerModel_type = indexer_type.fit(type_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform the DataFrame using the fitted StringIndexer model\n",
    "\n",
    "indexed_df_type = indexerModel_type.transform(type_df)\n",
    "indexed_df_type.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply One-Hot-Encoding to the indexed column, that is, \n",
    "### \"types_indexed2\"\n",
    "\n",
    "encoder_type2 = OneHotEncoder(dropLast=False, inputCol=\"types_indexed2\", outputCol=\"types_onehot2\")\n",
    "encoder_type2_df = encoder_type2.fit(indexed_df_type).transform(indexed_df_type)\n",
    "encoder_type2_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df_split = encoder_type2_df.select('*',vector_to_array('types_onehot2').alias('types_onehot_split2'))\n",
    "encoder_type2_df_split.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now, we´ll split the \"types_onehot_split2\" into two columns, one per category\n",
    "\n",
    "num_categories = len(encoder_type2_df_split.first()['types_onehot_split2'])\n",
    "cols_expanded = [(f.col('types_onehot_split2')[i].alias(f\"{indexerModel_type.labels[i]}\")) for i in range(num_categories)]\n",
    "encoder_type2_df_split = encoder_type2_df_split.select('*',*cols_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df_split.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve split the \"type2\" column into two columns based on One-Hot-Encoding. Now, we´ll eliminate some unnecessaruy columns. Let´s check out all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.3.- Eliminate unneccesary columns: \"nameOrig\",\"nameDest\",\"isFlaggedFraud\",\"newbalanceDest\",\n",
    "### \"oldbalanceDest\",\"oldbalanceOrg\",\"newbalanceOrig\",\"types_indexed\",\"types_onehot\",\"types_onehot_split\",\n",
    "### \"types_indexed2\",\"types_onehot2\", \"types_onehot_split2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll eliminate the unnecessary columns:\n",
    "+ nameOrig\n",
    "+ nameDest\n",
    "+ isFlaggedFraud\n",
    "+ newbalanceDest\n",
    "+ oldbalanceDest\n",
    "+ oldbalanceOrg\n",
    "+ newbalanceOrig \n",
    "+ types_indexed\n",
    "+ types_onehot\n",
    "+ types_onehot_split\n",
    "+ types_indexed2\n",
    "+ types_onehot2\n",
    "+ types_onehot_split2\n",
    "+ type\n",
    "+ type2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank = encoder_type2_df_split.drop(\"nameOrig\",\"nameDest\",\"isFlaggedFraud\",\"newbalanceDest\",\"oldbalanceDest\",\n",
    "                       \"oldbalanceOrg\",\"newbalanceOrig\",\"type\",\"types_indexed\",\"types_onehot\",\n",
    "                       \"types_onehot_split\",\"type2\",\"types_indexed2\",\"types_onehot2\",\"types_onehot_split2\" )\n",
    "df_bank.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are the same quantity of registers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2.1.- Eliminate duplicated\n",
    "\n",
    "num_all_rows = df_bank.count()\n",
    "num_all_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicated_rows = df_bank.distinct().count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total number of duplicated rows is:\",num_all_rows - num_duplicated_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are 7597 duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank = df_bank.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the duplicated registers have been removed because there fewer registers than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2.2.- Eliminate null values\n",
    "\n",
    "df_bank.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there were no null values in the dataset because the number of registers is the same. Let´s take a look at the \"clean\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The visualization will be done using a functions which leverages the method histogram() of pyspark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the \"histogram\" function\n",
    "\n",
    "def histogram(df, col, bins=10, xname=None, yname=None):\n",
    "    \n",
    "    '''\n",
    "    This function makes a histogram from spark dataframe named \n",
    "    df for column name col. \n",
    "    '''\n",
    "    \n",
    "    # Calculating histogram in Spark \n",
    "    vals = df.select(col).rdd.flatMap(lambda x: x).histogram(bins)\n",
    "    \n",
    "    # Preprocessing histogram points and locations \n",
    "    width = vals[0][1] - vals[0][0]\n",
    "    loc = [vals[0][0] + (i+1) * width for i in range(len(vals[1]))]\n",
    "    \n",
    "    # Making a bar plot \n",
    "    mpt.bar(loc, vals[1], width=width)\n",
    "    mpt.xlabel(col)\n",
    "    mpt.ylabel(yname)\n",
    "    mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are some features that need to be converted to integers such as \"step\",\"amount\" and \"isFraud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string columns into integer columns\n",
    "\n",
    "df_bank = df_bank.withColumn(\"step\",df_bank[\"step\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank = df_bank.withColumn(\"amount\",df_bank[\"amount\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank = df_bank.withColumn(\"isFraud\",df_bank[\"isFraud\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve seen that all the features are \"integer\" types now. Therefore, we´re able to perform various visualizations with the histogram method. That´s what we´ll do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"step\"\n",
    "\n",
    "histogram(df_bank, 'step', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"amount\"\n",
    "\n",
    "histogram(df_bank, 'amount', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"Debit\"\n",
    "\n",
    "histogram(df_bank, 'Debit', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"Payment\"\n",
    "\n",
    "histogram(df_bank, 'Payment', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CASH_OUT\"\n",
    "\n",
    "histogram(df_bank, 'CASH_OUT', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CASH_IN\"\n",
    "\n",
    "histogram(df_bank, 'CASH_IN', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"TRANSFER\"\n",
    "\n",
    "histogram(df_bank, 'TRANSFER', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CC\"\n",
    "\n",
    "histogram(df_bank, 'CC', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CM\"\n",
    "\n",
    "histogram(df_bank, 'CM', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"isFraud\"\n",
    "\n",
    "histogram(df_bank, 'isFraud', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember that our label is \"isFraud\", therefore, we can see that this class is unbalanced as we can see from the previous graphic. We need to perform an **Oversampling** through ***Data Balancing*** using *pyspark*. In this part, we´ll transform the dataset into a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we \"write\" a parquet file, that is, create a parquet file with the same data\n",
    "\n",
    "df_bank.write.parquet(\"/Users/alexangelbracho/Desktop/GitHub_projects/FraudDetection/Fraud-Detection-Project/fraudDetection.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we read the parquet file\n",
    "\n",
    "df_bank_par = spark.read.parquet(\"fraudDetection.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### oversampling with \"pysaprk\"\n",
    "\n",
    "minor_df = df_bank_par.filter(f.col(\"isFraud\")==1)\n",
    "major_df = df_bank_par.filter(f.col(\"isFraud\")==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df_bank_par = df_bank_par.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df_bank_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_major_df = major_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_major_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = int(major_df.count()/minor_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ratio is:\",ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let´s duplicate the minoriry rows\n",
    "\n",
    "oversampled_df = minor_df.withColumn(\"dummy\",f.explode(f.array([f.lit(x) for x in a]))).drop(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the unnecessary columns in the \"oversampled_df\" dataframe\n",
    "\n",
    "oversampled_df = oversampled_df.drop(\"step\",\"amount\",\"CASH_OUT\",\"CASH_IN\",\"PAYMENT\",\"TRANSFER\",\"DEBIT\",\"CC\",\"CM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oversampled_df = oversampled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oversampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oversampled_df + num_major_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can realize that suming \"oversampled_df\" and \"major_df\" exceeds the total number of samples. Therefore, we need to low them down to the half at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we need to aggregate indexes to the \"oversampled_df\" dataframe\n",
    "\n",
    "oversampled_df = oversampled_df.withColumn(\"index\",monotonically_increasing_id())\n",
    "oversampled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a view of the \"oversampled_df\" dataframe to use sparkSQL\n",
    "\n",
    "oversampled_df.createOrReplaceTempView(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_major_df = num_major_df / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_major_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_oversampled_df = num_df_bank_par - limit_major_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_oversampled_df = int(limit_oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_oversampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(limit_oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this query to select some rows of the \"oversampled_df\" dataframe\n",
    "\n",
    "query = f\"SELECT * FROM isFraud2 LIMIT {limit_oversampled_df}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the unnecessary columns in the \"major_df\" dataframe\n",
    "\n",
    "major_df = major_df.drop(\"step\",\"amount\",\"CASH_OUT\",\"CASH_IN\",\"PAYMENT\",\"TRANSFER\",\"DEBIT\",\"CC\",\"CM\")\n",
    "major_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we need to aggregate indexes to the \"major_df\" dataframe\n",
    "\n",
    "major_df = major_df.withColumn(\"index\",monotonically_increasing_id())\n",
    "major_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_major_df = int(limit_major_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a view from \"major_df\" dataframe to do some queries\n",
    "\n",
    "major_df.createOrReplaceTempView(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this query to select some rows of the \"major_df\" dataframe\n",
    "\n",
    "query = f\"SELECT * FROM isFraud LIMIT {limit_major_df}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = major_df.unionAll(oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The previous table contains the former unbalanced data in the feature \"isFraud\"; this result says that we have the same number of registers than the original dataset. Let´s check out if the the class is already balanced in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1 = combined_df.filter(f.col(\"isFraud\")==1)\n",
    "class_0 = combined_df.filter(f.col(\"isFraud\")==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the class \"isFraud\" is almost the same in this dataframe, resulting in the same number of samples in the original dataset. Now, we need to merge the original dataframe \"df_bank_par\" with \"combined_pd\" dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.drop(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.count(), df_bank_par.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we need to aggregate indexes to the \"df_bank_par\" dataframe\n",
    "\n",
    "df_bank_par = df_bank_par.withColumn(\"index\",monotonically_increasing_id())\n",
    "df_bank_par.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.join(combined_df,on=['index']).drop('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out again the number of samples of each class in the feature \"isFraud\" (label) in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1 = df_bank_par.filter(f.col(\"isFraud\")==1)\n",
    "class_0 = df_bank_par.filter(f.col(\"isFraud\")==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(class_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our latest valid and \"clean\" dataframe is *df_bank_par* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have a balanced class in \"isFraud\". Let´s check out with a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"isFraud\"\n",
    "\n",
    "histogram(df_bank_par, 'isFraud', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
