{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Fraud Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PaySim simulates mobile money transactions based on a sample of real transacions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world. The objective of the project is to predict if a transaction is fraudulent or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries: mathematical computing \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# libraries: sklearn\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# libraries: pyspark sql\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from  pyspark.sql.functions import monotonically_increasing_id, desc, row_number\n",
    "\n",
    "# libraries: pyspark machine learning\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, DecisionTreeClassifier, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "# libraries: visualization\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as mpt\n",
    "import functools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "global df_bank, results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ll use PySpark to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of the SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FraudDetection\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark dataframe \n",
    "\n",
    "df = spark.read.csv('fraudDetection.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll convert this \"df\" dataframe into a parquet file using the following method of pyspark. The file will be named \"fraudDetection.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"/Users/alexangelbracho/Desktop/GitHub_projects/FraudDetection/Fraud-Detection-Project/fraudDetection.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll read the file as a parquet file. The calculation will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = spark.read.parquet(\"fraudDetection.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s take a look to the data with the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 11 columns, some of them are numerical and others are categorical. Let´s count the number of registers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total number of registers is:\",df_bank_par.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have more than six miliions of transactions in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firstly, we´ll create a function to create a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.1.- creation of a new variable: type2\n",
    "\n",
    "df_type2 = df_bank_par.withColumn(\"type2\",f.concat(f.substring(\"nameOrig\",1,1),f.substring(\"nameDest\",1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve created a new column named \"type2\" which is composed by the first character of the column \"nameOrig\" and the first character of the column \"nameDest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.2.1.- One Hot Encoding: column \"type\"\n",
    "\n",
    "df_type2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ll use some libraries of Spark for Machine Learning (SparkML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### StringIndexer Initialization\n",
    "### column: type\n",
    "\n",
    "indexer_type = StringIndexer(inputCol=\"type\",outputCol=\"types_indexed\")\n",
    "indexerModel_type = indexer_type.fit(df_type2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform the DataFrame using the fitted StringIndexer model\n",
    "\n",
    "indexed_df_type2 = indexerModel_type.transform(df_type2)\n",
    "indexed_df_type2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we´ve set each of the elements of the \"type\" column into indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply One-Hot-Encoding to the indexed column, that is, \n",
    "### \"types_indexed\"\n",
    "\n",
    "encoder_type = OneHotEncoder(dropLast=False, inputCol=\"types_indexed\", outputCol=\"types_onehot\")\n",
    "encoder_type_df = encoder_type.fit(indexed_df_type2).transform(indexed_df_type2)\n",
    "encoder_type_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type_df_split = encoder_type_df.select('*',vector_to_array('types_onehot').alias('types_onehot_split'))\n",
    "encoder_type_df_split.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now, we´ll split the \"types_onehot_split\" into five columns, one per category\n",
    "\n",
    "num_categories = len(encoder_type_df_split.first()['types_onehot_split'])\n",
    "cols_expanded = [(f.col('types_onehot_split')[i].alias(f\"{indexerModel_type.labels[i]}\")) for i in range(num_categories)]\n",
    "type_df = encoder_type_df_split.select('*',*cols_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve applied One-Hot-Encoding to the column \"type\" resulting in five new columns:\n",
    "+ CASH_OUT\n",
    "+ CASH_IN\n",
    "+ PAYMENT\n",
    "+ TRANSFER \n",
    "+ DEBIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll apply this procedure to the column \"type2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.2.2.- One Hot Encoding: column \"type2\"\n",
    "\n",
    "type_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### StringIndexer Initialization\n",
    "### column: type2\n",
    "\n",
    "indexer_type = StringIndexer(inputCol=\"type2\",outputCol=\"types_indexed2\")\n",
    "indexerModel_type = indexer_type.fit(type_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform the DataFrame using the fitted StringIndexer model\n",
    "\n",
    "indexed_df_type = indexerModel_type.transform(type_df)\n",
    "indexed_df_type.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply One-Hot-Encoding to the indexed column, that is, \n",
    "### \"types_indexed2\"\n",
    "\n",
    "encoder_type2 = OneHotEncoder(dropLast=False, inputCol=\"types_indexed2\", outputCol=\"types_onehot2\")\n",
    "encoder_type2_df = encoder_type2.fit(indexed_df_type).transform(indexed_df_type)\n",
    "encoder_type2_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df_split = encoder_type2_df.select('*',vector_to_array('types_onehot2').alias('types_onehot_split2'))\n",
    "encoder_type2_df_split.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now, we´ll split the \"types_onehot_split2\" into two columns, one per category\n",
    "\n",
    "num_categories = len(encoder_type2_df_split.first()['types_onehot_split2'])\n",
    "cols_expanded = [(f.col('types_onehot_split2')[i].alias(f\"{indexerModel_type.labels[i]}\")) for i in range(num_categories)]\n",
    "encoder_type2_df_split = encoder_type2_df_split.select('*',*cols_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df_split.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve split the \"type2\" column into two columns based on One-Hot-Encoding. Now, we´ll eliminate some unnecessaruy columns. Let´s check out all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df_split.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll eliminate the unnecessary columns:\n",
    "+ nameOrig\n",
    "+ nameDest\n",
    "+ isFlaggedFraud\n",
    "+ newbalanceDest\n",
    "+ oldbalanceDest\n",
    "+ oldbalanceOrg\n",
    "+ newbalanceOrig \n",
    "+ types_indexed\n",
    "+ types_onehot\n",
    "+ types_onehot_split\n",
    "+ types_indexed2\n",
    "+ types_onehot2\n",
    "+ types_onehot_split2\n",
    "+ type\n",
    "+ type2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = encoder_type2_df_split.drop(\"nameOrig\",\"nameDest\",\"isFlaggedFraud\",\"newbalanceDest\",\"oldbalanceDest\",\n",
    "                       \"oldbalanceOrg\",\"newbalanceOrig\",\"type\",\"types_indexed\",\"types_onehot\",\n",
    "                       \"types_onehot_split\",\"type2\",\"types_indexed2\",\"types_onehot2\",\"types_onehot_split2\" )\n",
    "df_bank_par.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are the same quantity of registers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2.1.- Eliminate duplicated\n",
    "\n",
    "num_all_rows = df_bank_par.count()\n",
    "num_all_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicated_rows = df_bank_par.distinct().count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total number of duplicated rows is:\",num_all_rows - num_duplicated_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are 7597 duplicated rows. Let´s remove the null values and duplicated values from the df_bank_par dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.dropna()\n",
    "\n",
    "df_bank_par = df_bank_par.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the duplicated registers have been removed because there are fewer registers than before. Let´s take a look at the \"clean\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The visualization will be done using a functions which leverages the method histogram() of pyspark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the \"histogram\" function\n",
    "\n",
    "def histogram(df, col, bins=10, xname=None, yname=None):\n",
    "    \n",
    "    '''\n",
    "    This function makes a histogram from spark dataframe named \n",
    "    df for column name col. \n",
    "    '''\n",
    "    \n",
    "    # Calculating histogram in Spark \n",
    "    vals = df.select(col).rdd.flatMap(lambda x: x).histogram(bins)\n",
    "    \n",
    "    # Preprocessing histogram points and locations \n",
    "    width = vals[0][1] - vals[0][0]\n",
    "    loc = [vals[0][0] + (i+1) * width for i in range(len(vals[1]))]\n",
    "    \n",
    "    # Making a bar plot \n",
    "    mpt.bar(loc, vals[1], width=width)\n",
    "    mpt.xlabel(col)\n",
    "    mpt.ylabel(yname)\n",
    "    mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are some features that need to be converted to integers such as \"step\",\"amount\" and \"isFraud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string columns into integer columns\n",
    "\n",
    "df_bank_par = df_bank_par.withColumn(\"step\",df_bank_par[\"step\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.withColumn(\"amount\",df_bank_par[\"amount\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.withColumn(\"isFraud\",df_bank_par[\"isFraud\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve seen that all the features are \"integer\" types now. Therefore, we´re able to perform various visualizations with the histogram method. That´s what we´ll do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"step\"\n",
    "\n",
    "histogram(df_bank_par, 'step', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"amount\"\n",
    "\n",
    "histogram(df_bank_par, 'amount', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"Debit\"\n",
    "\n",
    "histogram(df_bank_par, 'Debit', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"Payment\"\n",
    "\n",
    "histogram(df_bank_par, 'Payment', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CASH_OUT\"\n",
    "\n",
    "histogram(df_bank_par, 'CASH_OUT', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CASH_IN\"\n",
    "\n",
    "histogram(df_bank_par, 'CASH_IN', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"TRANSFER\"\n",
    "\n",
    "histogram(df_bank_par, 'TRANSFER', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CC\"\n",
    "\n",
    "histogram(df_bank_par, 'CC', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CM\"\n",
    "\n",
    "histogram(df_bank_par, 'CM', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"isFraud\"\n",
    "\n",
    "histogram(df_bank_par, 'isFraud', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember that our label is \"isFraud\", therefore, we can see that this class is unbalanced as we can see from the previous graphic. We need to perform an **Oversampling** through ***Data Balancing*** using *pyspark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### oversampling with \"pysaprk\"\n",
    "\n",
    "minor_df = df_bank_par.filter(f.col(\"isFraud\")==1)\n",
    "major_df = df_bank_par.filter(f.col(\"isFraud\")==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df_bank_par = df_bank_par.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df_bank_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_major_df = major_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_major_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = int(major_df.count()/minor_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The ratio is:\",ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let´s duplicate the minoriry rows\n",
    "\n",
    "oversampled_df = minor_df.withColumn(\"dummy\",f.explode(f.array([f.lit(x) for x in a]))).drop(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the unnecessary columns in the \"oversampled_df\" dataframe\n",
    "\n",
    "oversampled_df = oversampled_df.drop(\"step\",\"amount\",\"CASH_OUT\",\"CASH_IN\",\"PAYMENT\",\"TRANSFER\",\"DEBIT\",\"CC\",\"CM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oversampled_df = oversampled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oversampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oversampled_df + num_major_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can realize that suming \"oversampled_df\" and \"major_df\" exceeds the total number of samples. Therefore, we need to low them down to the half at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we need to aggregate indexes to the \"oversampled_df\" dataframe\n",
    "\n",
    "oversampled_df = oversampled_df.withColumn(\"index\",monotonically_increasing_id())\n",
    "oversampled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a view of the \"oversampled_df\" dataframe to use sparkSQL\n",
    "\n",
    "oversampled_df.createOrReplaceTempView(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_major_df = num_major_df / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_major_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_oversampled_df = num_df_bank_par - limit_major_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_oversampled_df = int(limit_oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_oversampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(limit_oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this query to select some rows of the \"oversampled_df\" dataframe\n",
    "\n",
    "query = f\"SELECT * FROM isFraud LIMIT {limit_oversampled_df}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampled_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the unnecessary columns in the \"major_df\" dataframe\n",
    "\n",
    "major_df = major_df.drop(\"step\",\"amount\",\"CASH_OUT\",\"CASH_IN\",\"PAYMENT\",\"TRANSFER\",\"DEBIT\",\"CC\",\"CM\")\n",
    "major_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we need to aggregate indexes to the \"major_df\" dataframe\n",
    "\n",
    "major_df = major_df.withColumn(\"index\",monotonically_increasing_id())\n",
    "major_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_major_df = int(limit_major_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_major_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a view from \"major_df\" dataframe to do some queries\n",
    "\n",
    "major_df.createOrReplaceTempView(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this query to select some rows of the \"major_df\" dataframe\n",
    "\n",
    "query = f\"SELECT * FROM isFraud LIMIT {limit_major_df}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = major_df.unionAll(oversampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The previous table contains the former unbalanced data in the feature \"isFraud\"; this result says that we have the same number of registers than the original dataset. Let´s check out if the the class is already balanced in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1 = combined_df.filter(f.col(\"isFraud\")==1)\n",
    "class_0 = combined_df.filter(f.col(\"isFraud\")==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the class \"isFraud\" is almost the same in this dataframe, resulting in the same number of samples in the original dataset. Now, we need to merge the original dataframe \"df_bank_par\" with \"combined_pd\" dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.drop(\"isFraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.count(), df_bank_par.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we need to aggregate indexes to the \"df_bank_par\" dataframe\n",
    "\n",
    "df_bank_par = df_bank_par.withColumn(\"index\",monotonically_increasing_id())\n",
    "df_bank_par.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.join(combined_df,on=['index']).drop('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out again the number of samples of each class in the feature \"isFraud\" (label) in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1 = df_bank_par.filter(f.col(\"isFraud\")==1)\n",
    "class_0 = df_bank_par.filter(f.col(\"isFraud\")==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_bank_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(class_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our latest valid and \"clean\" dataframe is *df_bank_par* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have a balanced class in \"isFraud\". Let´s check out with a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(df_bank_par, 'isFraud', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we want to transform this pyspark \"dataframe\" df_bank_par into a pandas dataframe we can use the method to_pandas_on_spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas dataframe\n",
    "\n",
    "df_bank_par_pandas = df_bank_par.to_pandas_on_spark()\n",
    "df_bank_par_pandas.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par_pandas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_bank_par_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s create a function to find a correlation between the target variable \"isFraud\" and the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the function \"correlation_df\"\n",
    "\n",
    "def correlation_df(df,target_var,feature_cols, method):\n",
    "    # assemble features into a vector\n",
    "    target_var = [target_var]\n",
    "    feature_cols = feature_cols\n",
    "    df_cor = df.select(target_var + feature_cols)\n",
    "    assembler = VectorAssembler(inputCols=target_var + feature_cols, outputCol=\"features\")\n",
    "    df_cor = assembler.transform(df_cor)\n",
    "\n",
    "    # calculate correlation matrix\n",
    "    correlation_matrix = Correlation.corr(df_cor, \"features\", method =method).head()[0]\n",
    "\n",
    "    # extract the correlation coefficient between target and each feature\n",
    "    target_corr_list = [correlation_matrix[i,0] for i in range(len(feature_cols)+1)][1:]\n",
    "\n",
    "    # create a Dataframe with target variable, feature names and correlation coefficients\n",
    "    correlation_data = [(feature_cols[i],float(target_corr_list[i])) for i in range(len(feature_cols))]\n",
    "\n",
    "    correlation_df = spark.createDataFrame(correlation_data, [\"feature\",\"correlation\"] )\n",
    "\n",
    "    correlation_df = correlation_df.withColumn(\"abs_correlation\",f.abs(\"correlation\"))\n",
    "\n",
    "    # print the result\n",
    "    return correlation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"isFraud\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"amount\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"step\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construction of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = df_bank_par.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train) , type(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s assemble these datasets \"train\" and \"test\" into a single feature vector using VectorAssembler class per each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let´s assemble the train dataset as a single feature vector using VectorAssembler class\n",
    "\n",
    "columns = ['step','amount','CASH_OUT','PAYMENT','CASH_IN','TRANSFER','DEBIT','CC','CM','isFraud']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "\n",
    "train = assembler.transform(train)\n",
    "\n",
    "train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let´s assemble the test dataset as a single feature vector using VectorAssembler class\n",
    "\n",
    "columns = ['step','amount','CASH_OUT','PAYMENT','CASH_IN','TRANSFER','DEBIT','CC','CM','isFraud']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "\n",
    "test = assembler.transform(test)\n",
    "\n",
    "test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Models\n",
    "\n",
    "We´ll use several machine learning algorithms to evaluate all of them and to select the best one. We´ll start with Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model \"random forest\" (rf)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='isFraud')\n",
    "model_RF = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of the random forest model using the test dataset\n",
    "\n",
    "predictions = model_RF.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are three more columns: rawPrediction, probability and prediction. We can clearly compare the actual values and predicted values with the output below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"isFraud\",\"prediction\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At a glance we can see that the predicted values are the same of the actual values, at least for the first fifty registers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to evaluate our random forest machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The accuracy is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Error = (1 - accuracy)\n",
    "print(f\"The Test Error is {Test_Error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out the Consufion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to the confusion matrix, all the actual values will be correctly predicted. It may mean an Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model Logistic Regression (lr)\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='isFraud')\n",
    "\n",
    "model_LR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To better understand the model, we can examine its coefficients and intercept. The values represent the weights assigned to each feature and the bias term, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = model_LR.coefficients\n",
    "\n",
    "intercept = model_LR.intercept\n",
    "\n",
    "print(\"Coefficients: \", coefficients)\n",
    "\n",
    "print(\"Intercept: \", intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of the logistic regression model using the test dataset\n",
    "\n",
    "predictions = model_LR.transform(test)\n",
    "\n",
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC - ROC\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\",)\n",
    "\n",
    "accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy)\n",
    "\n",
    "print(f\"Precsion: \", precision)\n",
    "\n",
    "print(f\"Recall: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model Decision Tree (dt)\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='isFraud')\n",
    "\n",
    "model_dt = dt.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of the decision tree model using the test dataset\n",
    "\n",
    "predictions = model_dt.transform(test)\n",
    "\n",
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC - ROC\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\",)\n",
    "\n",
    "accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy)\n",
    "\n",
    "print(f\"Precsion: \", precision)\n",
    "\n",
    "print(f\"Recall: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model Naive Bayes (nb)\n",
    "\n",
    "nb = NaiveBayes(featuresCol='features', labelCol='isFraud')\n",
    "\n",
    "model_nb = nb.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of the naive bayes model using the test dataset\n",
    "\n",
    "predictions = model_nb.transform(test)\n",
    "\n",
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC - ROC\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(labelCol=\"isFraud\", predictionCol=\"prediction\",)\n",
    "\n",
    "accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy)\n",
    "\n",
    "print(f\"Precsion: \", precision)\n",
    "\n",
    "print(f\"Recall: \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: Random Forest\n",
    "\n",
    "model_RF.save(\"randomF_model\")\n",
    "\n",
    "# model: Logistic Regression\n",
    "\n",
    "model_LR.save(\"logit_model\")\n",
    "\n",
    "# model: Decision Tree\n",
    "\n",
    "model_dt.save(\"decisionT_model\")\n",
    "\n",
    "# model: Naive Bayes\n",
    "\n",
    "model_nb.save(\"naiveB_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: Random Forest\n",
    "\n",
    "loaded_model_RF = RandomForestClassifier.load(\"randomF_model\")\n",
    "\n",
    "# model: Logistic Regression\n",
    "\n",
    "loaded_model_LR = LogisticRegression.load(\"logit_model\")\n",
    "\n",
    "# model: Decision Tree\n",
    "\n",
    "loaded_model_LR = DecisionTreeClassifier.load(\"decisionT_model\")\n",
    "\n",
    "# model: Naive Bayes\n",
    "\n",
    "loaded_model_LR = NaiveBayes.load(\"naiveB_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
