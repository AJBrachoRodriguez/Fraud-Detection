{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Fraud Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PaySim simulates mobile money transactions based on a sample of real transacions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world. The objective of the project is to predict if a transaction is fraudulent or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries: mathematical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# libraries: sklearn\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# libraries: pyspark SparkContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# libraries: pyspark sql\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from  pyspark.sql.functions import monotonically_increasing_id, desc, row_number\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# libraries: pyspark machine learning\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, DecisionTreeClassifier, NaiveBayes, RandomForestClassificationModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel, TrainValidationSplit,TrainValidationSplitModel\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, RFormula\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# libraries: visualization\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as mpt\n",
    "import functools\n",
    "from collections import Counter\n",
    "from ydata_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark-Context\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Builder\n",
    "\n",
    "spark = SparkSession.builder.appName(\"fraudDetection\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "global df_bank, results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ll use PySpark to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark dataframe \n",
    "\n",
    "df = spark.read.csv('fraudDetection.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll convert this \"df\" dataframe into a parquet file using the following method of pyspark. The file will be named \"fraudDetection.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"fraudDetection.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll read the file as a parquet file. The calculation will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = spark.read.parquet(\"fraudDetection.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s take a look to the data with the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 11 columns, none of them is numerical (they are categorical). Let´s count the number of registers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total number of registers is:\",df_bank_par.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have more than six miliions of transactions in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firstly, we´ll create a function to create a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.1.- creation of a new variable: type2\n",
    "\n",
    "df_type2 = df_bank_par.withColumn(\"type2\",f.concat(f.substring(\"nameOrig\",1,1),f.substring(\"nameDest\",1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve created a new column named \"type2\" which is composed by the first character of the column \"nameOrig\" and the first character of the column \"nameDest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.2.1.- One Hot Encoding: column \"type\"\n",
    "\n",
    "df_type2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ll use some libraries of Spark for Machine Learning (SparkML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### StringIndexer Initialization\n",
    "### column: type\n",
    "\n",
    "indexer_type = StringIndexer(inputCol=\"type\",outputCol=\"types_indexed\")\n",
    "indexerModel_type = indexer_type.fit(df_type2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform the DataFrame using the fitted StringIndexer model\n",
    "\n",
    "indexed_df_type2 = indexerModel_type.transform(df_type2)\n",
    "indexed_df_type2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we´ve set each of the elements of the \"type\" column into indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply One-Hot-Encoding to the indexed column, that is, \n",
    "### \"types_indexed\"\n",
    "\n",
    "encoder_type = OneHotEncoder(dropLast=False, inputCol=\"types_indexed\", outputCol=\"types_onehot\")\n",
    "encoder_type_df = encoder_type.fit(indexed_df_type2).transform(indexed_df_type2)\n",
    "encoder_type_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type_df_split = encoder_type_df.select('*',vector_to_array('types_onehot').alias('types_onehot_split'))\n",
    "encoder_type_df_split.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now, we´ll split the \"types_onehot_split\" into five columns, one per category\n",
    "\n",
    "num_categories = len(encoder_type_df_split.first()['types_onehot_split'])\n",
    "cols_expanded = [(f.col('types_onehot_split')[i].alias(f\"{indexerModel_type.labels[i]}\")) for i in range(num_categories)]\n",
    "type_df = encoder_type_df_split.select('*',*cols_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_df.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve applied One-Hot-Encoding to the column \"type\" resulting in five new columns:\n",
    "+ CASH_OUT\n",
    "+ CASH_IN\n",
    "+ PAYMENT\n",
    "+ TRANSFER \n",
    "+ DEBIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll apply this procedure to the column \"type2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.1.2.2.- One Hot Encoding: column \"type2\"\n",
    "\n",
    "type_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### StringIndexer Initialization\n",
    "### column: type2\n",
    "\n",
    "indexer_type = StringIndexer(inputCol=\"type2\",outputCol=\"types_indexed2\")\n",
    "indexerModel_type = indexer_type.fit(type_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform the DataFrame using the fitted StringIndexer model\n",
    "\n",
    "indexed_df_type = indexerModel_type.transform(type_df)\n",
    "indexed_df_type.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### apply One-Hot-Encoding to the indexed column, that is, \n",
    "### \"types_indexed2\"\n",
    "\n",
    "encoder_type2 = OneHotEncoder(dropLast=False, inputCol=\"types_indexed2\", outputCol=\"types_onehot2\")\n",
    "encoder_type2_df = encoder_type2.fit(indexed_df_type).transform(indexed_df_type)\n",
    "encoder_type2_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df_split = encoder_type2_df.select('*',vector_to_array('types_onehot2').alias('types_onehot_split2'))\n",
    "encoder_type2_df_split.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now, we´ll split the \"types_onehot_split2\" into two columns, one per category\n",
    "\n",
    "num_categories = len(encoder_type2_df_split.first()['types_onehot_split2'])\n",
    "cols_expanded = [(f.col('types_onehot_split2')[i].alias(f\"{indexerModel_type.labels[i]}\")) for i in range(num_categories)]\n",
    "encoder_type2_df_split = encoder_type2_df_split.select('*',*cols_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df_split.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve split the \"type2\" column into two columns based on One-Hot-Encoding. Now, we´ll eliminate some unnecessaruy columns. Let´s check out all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_type2_df_split.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll eliminate the unnecessary columns:\n",
    "+ nameOrig\n",
    "+ nameDest\n",
    "+ isFlaggedFraud\n",
    "+ newbalanceDest\n",
    "+ oldbalanceDest\n",
    "+ oldbalanceOrg\n",
    "+ newbalanceOrig \n",
    "+ types_indexed\n",
    "+ types_onehot\n",
    "+ types_onehot_split\n",
    "+ types_indexed2\n",
    "+ types_onehot2\n",
    "+ types_onehot_split2\n",
    "+ type\n",
    "+ type2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = encoder_type2_df_split.drop(\"nameOrig\",\"nameDest\",\"isFlaggedFraud\",\"newbalanceDest\",\"oldbalanceDest\",\n",
    "                       \"oldbalanceOrg\",\"newbalanceOrig\",\"type\",\"types_indexed\",\"types_onehot\",\n",
    "                       \"types_onehot_split\",\"type2\",\"types_indexed2\",\"types_onehot2\",\"types_onehot_split2\",\"types_indexed3\",\"types_onehot3\",\"types_onehot_split3\" )\n",
    "df_bank_par.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_bank_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are the same quantity of registers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2.1.- Eliminate duplicated\n",
    "\n",
    "num_all_rows = df_bank_par.count()\n",
    "num_all_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicated_rows = df_bank_par.distinct().count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The total number of duplicated rows is:\",num_all_rows - num_duplicated_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are 7597 duplicated rows. Let´s remove the null values and duplicated values from the df_bank_par dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.dropna()\n",
    "\n",
    "df_bank_par = df_bank_par.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the duplicated registers have been removed because there are fewer registers than before. Let´s take a look at the \"clean\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The visualization will be done using a functions which leverages the method histogram() of pyspark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the \"histogram\" function\n",
    "\n",
    "def histogram(df, col, bins=10, xname=None, yname=None):\n",
    "    \n",
    "    '''\n",
    "    This function makes a histogram from spark dataframe named \n",
    "    df for column name col. \n",
    "    '''\n",
    "    \n",
    "    # Calculating histogram in Spark \n",
    "    vals = df.select(col).rdd.flatMap(lambda x: x).histogram(bins)\n",
    "    \n",
    "    # Preprocessing histogram points and locations \n",
    "    width = vals[0][1] - vals[0][0]\n",
    "    loc = [vals[0][0] + (i+1) * width for i in range(len(vals[1]))]\n",
    "    \n",
    "    # Making a bar plot \n",
    "    mpt.bar(loc, vals[1], width=width)\n",
    "    mpt.xlabel(col)\n",
    "    mpt.ylabel(yname)\n",
    "    mpt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are some features that need to be converted to integers such as \"step\",\"amount\" and \"isFraud\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string columns into integer columns\n",
    "\n",
    "df_bank_par = df_bank_par.withColumn(\"step\",df_bank_par[\"step\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.withColumn(\"amount\",df_bank_par[\"amount\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = df_bank_par.withColumn(\"isFraud\",df_bank_par[\"isFraud\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We´ve seen that all the features are \"integer\" types now. Therefore, we´re able to perform various visualizations with the histogram method. That´s what we´ll do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"step\"\n",
    "\n",
    "histogram(df_bank_par, 'step', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"amount\"\n",
    "\n",
    "histogram(df_bank_par, 'amount', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"Debit\"\n",
    "\n",
    "histogram(df_bank_par, 'Debit', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"Payment\"\n",
    "\n",
    "histogram(df_bank_par, 'Payment', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CASH_OUT\"\n",
    "\n",
    "histogram(df_bank_par, 'CASH_OUT', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CASH_IN\"\n",
    "\n",
    "histogram(df_bank_par, 'CASH_IN', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"TRANSFER\"\n",
    "\n",
    "histogram(df_bank_par, 'TRANSFER', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CC\"\n",
    "\n",
    "histogram(df_bank_par, 'CC', bins=15, yname='frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"CM\"\n",
    "\n",
    "histogram(df_bank_par, 'CM', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram: \"isFraud\"\n",
    "\n",
    "histogram(df_bank_par, 'isFraud', bins=15, yname='frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember that our label is \"isFraud\", therefore, we can see that this class is unbalanced as we can see from the previous graphic. We need to perform an **Oversampling** through ***Data Balancing***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this case, there are two options to do the ***Data Balancing***: with **PySpark** and with **Sklearn**. You can uncomment the pyspark section, otherwise it will be done with Sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################ Oversampling with PySpark ##############################################################\n",
    "\n",
    "# Create undersampling function\n",
    "#def oversample_minority(df, ratio=1):\n",
    "#    '''\n",
    "#    ratio is the ratio of majority to minority\n",
    "#    Eg. ratio 1 is equivalent to majority:minority = 1:1\n",
    "#    ratio 5 is equivalent to majority:minority = 5:1\n",
    "#    '''\n",
    "#    minority_count = df.filter(f.col('isFraud')==1).count()\n",
    "#    majority_count = df.filter(f.col('isFraud')==0).count()\n",
    "#    \n",
    "#    balance_ratio = majority_count / minority_count\n",
    "#    \n",
    "#    print(f\"Initial Majority:Minority ratio is {balance_ratio:.2f}:1\")\n",
    "#    if ratio >= balance_ratio:\n",
    "#        print(\"No oversampling of minority was done as the input ratio was more than or equal to the initial ratio.\")\n",
    "#    else:\n",
    "#        print(f\"Oversampling of minority done such that Majority:Minority ratio is {ratio}:1\")\n",
    "#    \n",
    "#    oversampled_minority = df.filter(f.col('isFraud')==1).sample(withReplacement=True, fraction=(balance_ratio/ratio),seed=88)\n",
    "#    oversampled_df = df.filter(f.col('isFraud')==0).union(oversampled_minority)\n",
    "#    \n",
    "#    return oversampled_df\n",
    "\n",
    "#oversampled_df = oversample_minority(df_bank_par,ratio=1)\n",
    "\n",
    "#minority_count = oversampled_df.filter(f.col('isFraud')==1).count()\n",
    "#majority_count = oversampled_df.filter(f.col('isFraud')==0).count()\n",
    "#minority_count, majority_count\n",
    "#oversampled_df = oversampled_df.dropna()\n",
    "#oversampled_df = oversampled_df.dropDuplicates()\n",
    "#df_bank_par = oversampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################# Oversampling with Sklearn  #############################################################\n",
    "\n",
    "df_banco = pd.read_csv(\"fraudDetection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def procesar_datos():\n",
    "  global df_banco, resultados\n",
    "  df_banco=df_banco.copy()\n",
    "  # Crea la nueva variable type2 con la combinación de la primera letra de las columnas nameOrig y nameDest\n",
    "  df_banco['type2'] = df_banco['nameOrig'].str[0] + df_banco['nameDest'].str[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procesar_datos()\n",
    "df_banco.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding: \"type\" and \"type2\"\n",
    "df_encoded = pd.get_dummies(df_banco, columns=['type', 'type2'], dtype=int)\n",
    "df_encoded.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to eliminate\n",
    "columns_to_drop = ['nameOrig', 'nameDest', 'isFlaggedFraud', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# eliminate those columns\n",
    "df_encoded.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# reset the index\n",
    "df_encoded.reset_index(drop=True, inplace=True)\n",
    "df_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate the duplicated registers and save the result in df_banco\n",
    "\n",
    "df_banco = df_encoded.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate the duplicated registers and reset the index in df_banco\n",
    "df_banco.dropna(inplace=True)\n",
    "df_banco.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_banco.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_banco.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the values in the column \"isFraud\"\n",
    "conteo_isfraud = df_banco['isFraud'].value_counts()\n",
    "\n",
    "# create the \"bar\" graphic \n",
    "mpt.figure(figsize=(8, 6))\n",
    "conteo_isfraud.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "mpt.title('Distribution of the column \"isFraud\" ')\n",
    "mpt.xlabel('isFraud')\n",
    "mpt.ylabel('quantity')\n",
    "mpt.xticks([0, 1], ['NoFraud', 'Fraud'], rotation=0)\n",
    "\n",
    "# aggregate the values over the bar\n",
    "for i, valor in enumerate(conteo_isfraud):\n",
    "    mpt.text(i, valor + 0.01 * max(conteo_isfraud), str(valor), ha='center', va='bottom')\n",
    "\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_histogramas():\n",
    "\n",
    "  # define the attributes to plot\n",
    "  attributes = ['isFraud', 'amount', 'step']  # Puedes agregar más attributes aquí si lo deseas\n",
    "\n",
    "  # generate the boxplot graphic\n",
    "  mpt.figure(figsize=(12, 6))\n",
    "  for i, attribute in enumerate(attributes, 1):\n",
    "      mpt.subplot(1, len(attributes), i)\n",
    "      sb.boxplot(x=df_banco[attribute])\n",
    "      mpt.title(f'Boxplot de {attribute}')\n",
    "  mpt.tight_layout()\n",
    "  mpt.show()\n",
    "\n",
    "  # generate histograms\n",
    "  mpt.figure(figsize=(12, 6))\n",
    "  for i, attribute in enumerate(attributes, 1):\n",
    "      mpt.subplot(1, len(attributes), i)\n",
    "      sb.histplot(df_banco[attribute], kde=True)\n",
    "      mpt.title(f'Histograma de {attribute}')\n",
    "  mpt.tight_layout()\n",
    "  mpt.show()\n",
    "boxplot_histogramas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the histogram of the column 'amount'\n",
    "mpt.figure(figsize=(10, 6))\n",
    "mpt.hist(df_banco['amount'], bins=140, color='skyblue', edgecolor='black')\n",
    "mpt.title('Histogram of the column \"amount\" ')\n",
    "mpt.xlabel('Amount')\n",
    "mpt.ylabel('Frequency')\n",
    "mpt.grid(True)\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProfileReport(df_banco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "def balancing_class():\n",
    "    global df_banco, resultados\n",
    "\n",
    "    # Instancia SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Balanceo de clases\n",
    "    X_res, y_res = smote.fit_resample(df_banco.drop(columns=['isFraud']), df_banco['isFraud'])\n",
    "\n",
    "    # Reconstrucción del DataFrame balanceado\n",
    "    df_banco = pd.DataFrame(X_res, columns=df_banco.drop(columns=['isFraud']).columns)\n",
    "    df_banco['isFraud'] = y_res\n",
    "\n",
    "    # Elimina registros duplicados\n",
    "    df_banco.drop_duplicates(inplace=True)\n",
    "    df_banco.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Llama a la función balanceo_clases\n",
    "balancing_class()\n",
    "\n",
    "# Imprime el resultado final\n",
    "df_banco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the values of the column \"isFraud\"\n",
    "count_isfraud = df_banco['isFraud'].value_counts()\n",
    "\n",
    "# create the graphic\n",
    "mpt.figure(figsize=(8, 6))\n",
    "count_isfraud.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "mpt.title('Distribution of the column isFraud')\n",
    "mpt.xlabel('isFraud')\n",
    "mpt.ylabel('Quantity')\n",
    "mpt.xticks([0, 1], ['NoFraud', 'Fraud'], rotation=0)\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balancing_class()\n",
    "ProfileReport(df_banco)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll convert this pandas dataframe into a PySpark dataframe to leverage, but first to a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_banco.to_parquet('df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we´ll convert read the parquet file into a PySpark dataframe to make the calculations faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par = spark.read.parquet('df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string columns into integer columns\n",
    "\n",
    "df_bank_par = df_bank_par.withColumn(\"isFraud\",df_bank_par[\"isFraud\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_par.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = df_bank_par.filter(f.col(\"isFraud\")==0)\n",
    "class_1 = df_bank_par.filter(f.col(\"isFraud\")==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################## Convert parquet file into Pandas ###########################################\n",
    "#############################################################  (optional section)   #################################################\n",
    "\n",
    "##df_bank_par_pandas = df_bank_par.to_pandas_on_spark()\n",
    "##df_bank_par_pandas.head(10)\n",
    "##df_bank_par_pandas.describe()\n",
    "##type(df_bank_par_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s create a function to find a correlation between the target variable \"isFraud\" and the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of the function \"correlation_df\"\n",
    "\n",
    "def correlation_df(df,target_var,feature_cols, method):\n",
    "    # assemble features into a vector\n",
    "    target_var = [target_var]\n",
    "    feature_cols = feature_cols\n",
    "    df_cor = df.select(target_var + feature_cols)\n",
    "    assembler = VectorAssembler(inputCols=target_var + feature_cols, outputCol=\"features\")\n",
    "    df_cor = assembler.transform(df_cor)\n",
    "\n",
    "    # calculate correlation matrix\n",
    "    correlation_matrix = Correlation.corr(df_cor, \"features\", method =method).head()[0]\n",
    "\n",
    "    # extract the correlation coefficient between target and each feature\n",
    "    target_corr_list = [correlation_matrix[i,0] for i in range(len(feature_cols)+1)][1:]\n",
    "\n",
    "    # create a Dataframe with target variable, feature names and correlation coefficients\n",
    "    correlation_data = [(feature_cols[i],float(target_corr_list[i])) for i in range(len(feature_cols))]\n",
    "\n",
    "    correlation_df = spark.createDataFrame(correlation_data, [\"feature\",\"correlation\"] )\n",
    "\n",
    "    correlation_df = correlation_df.withColumn(\"abs_correlation\",f.abs(\"correlation\"))\n",
    "\n",
    "    # print the result\n",
    "    return correlation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let´s calculate the correlation among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"isFraud\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"amount\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"step\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"type_CASH_IN\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"type_CASH_OUT\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"type_DEBIT\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"type_PAYMENT\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"type_TRANSFER\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"type2_CC\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"type2_CM\"\n",
    "\n",
    "indep_cols = [x for x in df_bank_par.columns if x not in [target] ]\n",
    "\n",
    "corr_values_df = correlation_df(df=df_bank_par, target_var= target, feature_cols= indep_cols, method='pearson')\n",
    "\n",
    "print(f\"The corelation between {target} and the other features is: \")\n",
    "\n",
    "corr_values_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construction of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = df_bank_par.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s assemble these datasets \"train\" and \"test\" into a single feature vector using VectorAssembler class per each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let´s assemble the train dataset as a single feature vector using VectorAssembler class\n",
    "\n",
    "columns = ['step','amount','type_CASH_OUT','type_PAYMENT','type_CASH_IN','type_TRANSFER','type_DEBIT','type2_CC','type2_CM','isFraud']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "\n",
    "train = assembler.transform(train).withColumnRenamed(\"features\", \"my_features\")\n",
    "\n",
    "train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let´s assemble the test dataset as a single feature vector using VectorAssembler class\n",
    "\n",
    "columns = ['step','amount','type_CASH_OUT','type_PAYMENT','type_CASH_IN','type_TRANSFER','type_DEBIT','type2_CC','type2_CM','isFraud']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol='features')\n",
    "\n",
    "test = assembler.transform(test).withColumnRenamed(\"features\", \"my_features\")\n",
    "\n",
    "test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Models\n",
    "\n",
    "We´ll use several machine learning algorithms to evaluate all of them and to select the best one. We´ll start with Random Forest. However, it´s important to create some lists where to store the results of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = []\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "precision = []\n",
    "\n",
    "recall = []\n",
    "\n",
    "auc_roc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### model from the cross validation ##############################################\n",
    "################################################### (uncomment if you want to use this model ) #######################################\n",
    "\n",
    "#modelRF = CrossValidatorModel.load(\"tuningRF\")\n",
    "#print(modelRF.explainParams())\n",
    "#type(modelRF)\n",
    "#modelRF.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"isFraud\", featuresCol=\"my_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRF = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = modelRF.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_rf = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_rf)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the \"values list\" you can change the values range of the hyperparameters to test with. Besides, you need to set the hyperparameter name in the argument of the \"RandomForestClassifier\" model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of the overfitting\n",
    "\n",
    "# define the tree depths to evaluate\n",
    "values = [i for i in range(0, 11)]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    " # configure the model\n",
    " rf = RandomForestClassifier(labelCol=\"isFraud\", featuresCol=\"my_features\", maxDepth=i)\n",
    " # configure the evaluators and metrics\n",
    " evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    " metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    " # fit model on the training dataset\n",
    " modelRF = rf.fit(train)\n",
    " # predictions on the training dataset\n",
    " predictions = modelRF.transform(train)\n",
    " # evaluate on the train dataset\n",
    " train_accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    " train_scores.append(train_accuracy)\n",
    " # evaluate on the test dataset\n",
    " predictions = modelRF.transform(test)\n",
    " test_accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    " test_scores.append(test_accuracy)\n",
    " # summarize progress\n",
    " print('>%d, train: %.3f, test: %.3f' % (i, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of train and test scores vs \"<hyperparameter>\"\n",
    "\n",
    "mpt.plot(values, train_scores, '-o', label='Train')\n",
    "mpt.plot(values, test_scores, '-o', label='Test')\n",
    "mpt.legend()\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in the training dataset\n",
    "\n",
    "predictions = modelRF.transform(train)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_rf = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_rf = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_rf = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_rf)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_rf)\n",
    "\n",
    "print(f\"Precsion: \", precision_rf)\n",
    "\n",
    "print(f\"Recall: \", recall_rf)\n",
    "\n",
    "\n",
    "# ROC curve : visualization\n",
    "\n",
    "print(\"The ROC curve is:\")\n",
    "trainingSummary = modelRF.summary\n",
    "lrROC = trainingSummary.roc.toPandas()\n",
    "\n",
    "mpt.plot(lrROC['FPR'],lrROC['TPR'])\n",
    "mpt.ylabel('False Positive Rate')\n",
    "mpt.xlabel('True Positive Rate')\n",
    "mpt.title('ROC Curve - Random Forest')\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n",
    "mpt.show()\n",
    "\n",
    "\n",
    "# Precision Recall curve : visualization\n",
    "\n",
    "print(\"The Precision Recall curve is:\")\n",
    "pr = trainingSummary.pr.toPandas()\n",
    "mpt.plot(pr['recall'],pr['precision'])\n",
    "mpt.ylabel('Precision')\n",
    "mpt.xlabel('Recall')\n",
    "mpt.title('Precision Recall curve - Random Forest')\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################  Visualization ROC CURVE using Sklearn ############################################################\n",
    "\n",
    "########################### ROC graphic ###########################\n",
    "\n",
    "#preds_and_labels = predictions.select(\"prediction\", \"isFraud\")\n",
    "#preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "#preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "#preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "#metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "\n",
    "######################### Visualization ###########################\n",
    "\n",
    "#fpr = dict()\n",
    "#tpr = dict()\n",
    "#roc_auc = dict()\n",
    "\n",
    "#y_test = [i[1] for i in preds_and_labels_collect]\n",
    "#y_score = [i[0] for i in preds_and_labels_collect]\n",
    "\n",
    "#fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "#roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#print(\"The graphic ROC curve is:\")\n",
    "\n",
    "#mpt.figure(figsize=(5,4))\n",
    "#mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "#mpt.plot([0,1],[0,1],'k--')\n",
    "#mpt.xlim([0.0,1.0])\n",
    "#mpt.ylim([0.0,1.05])\n",
    "#mpt.xlabel('False Positive Rate')\n",
    "#mpt.ylabel('True Positive Rate')\n",
    "#mpt.title('ROC Curve - Random Forest')\n",
    "#mpt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in the test dataset\n",
    "\n",
    "predictions = modelRF.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_rf = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_rf = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_rf = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_rf)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_rf)\n",
    "\n",
    "print(f\"Precsion: \", precision_rf)\n",
    "\n",
    "print(f\"Recall: \", recall_rf)\n",
    "\n",
    "# ROC curve : visualization\n",
    "\n",
    "trainingSummary = modelRF.summary\n",
    "lrROC = trainingSummary.roc.toPandas()\n",
    "\n",
    "mpt.plot(lrROC['FPR'],lrROC['TPR'])\n",
    "mpt.ylabel('False Positive Rate')\n",
    "mpt.xlabel('True Positive Rate')\n",
    "mpt.title('ROC Curve - Random Forest')\n",
    "mpt.show()\n",
    "\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# fill the results list\n",
    "\n",
    "auc_roc.append(auc_rf)\n",
    "\n",
    "accuracy.append(accuracy_rf)\n",
    "\n",
    "recall.append(recall_rf)\n",
    "\n",
    "precision.append(precision_rf)\n",
    "\n",
    "name_model.append(\"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################  Visualization ROC curve using Sklearn ############################################################\n",
    "\n",
    "########################### ROC graphic ###########################\n",
    "\n",
    "#preds_and_labels = predictions.select(\"prediction\", \"isFraud\")\n",
    "#preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "#preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "#preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "#metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "\n",
    "######################### Visualization ###########################\n",
    "\n",
    "#fpr = dict()\n",
    "#tpr = dict()\n",
    "#roc_auc = dict()\n",
    "\n",
    "#y_test = [i[1] for i in preds_and_labels_collect]\n",
    "#y_score = [i[0] for i in preds_and_labels_collect]\n",
    "\n",
    "#fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "#roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#print(\"The graphic ROC curve is:\")\n",
    "\n",
    "#mpt.figure(figsize=(5,4))\n",
    "#mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "#mpt.plot([0,1],[0,1],'k--')\n",
    "#mpt.xlim([0.0,1.0])\n",
    "#mpt.ylim([0.0,1.05])\n",
    "#mpt.xlabel('False Positive Rate')\n",
    "#mpt.ylabel('True Positive Rate')\n",
    "#mpt.title('ROC Curve - Random Forest')\n",
    "#mpt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we see the feature importance graphic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Model: feature importance \n",
    "\n",
    "modelRF = CrossValidatorModel.load(\"tuningRFProof\")\n",
    "\n",
    "bestPipeline = modelRF.bestModel\n",
    "bestModel = bestPipeline.stages[1] # type: ignore\n",
    "\n",
    "importances = bestModel.featureImportances\n",
    "\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "mpt.bar(x_values,importances, orientation='vertical')\n",
    "mpt.xticks(x_values, columns, rotation = 40)\n",
    "mpt.ylabel('Importance')\n",
    "mpt.xlabel('Feature')\n",
    "mpt.title('Feature Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions of the model\n",
    "\n",
    "predictions = modelRF.transform(test)\n",
    "\n",
    "print('The predictions of the model are:')\n",
    "\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that there are three more columns: rawPrediction, probability and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out the Consufion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### modelLRrom the cross validation ##############################################\n",
    "################################################### (uncomment LR you want to use this model ) #######################################\n",
    "\n",
    "#modelLR = CrossValidatorModel.load(\"tuningLR\")\n",
    "#print(modelLR.explainParams())\n",
    "#type(modelLR)\n",
    "#modelLR.extractParamMap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"my_features\", labelCol=\"isFraud\", elasticNetParam=0.5, regParam=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR = lr.fit(train)\n",
    "\n",
    "predictions = modelLR.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_rf = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_rf)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the \"values list\" you can change the values range of the hyperparameters to test with. Besides, you need to set the hyperparameter name in the argument of the \"Logistic Regression\" model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of the overfitting\n",
    "\n",
    "# define the tree depths to evaluate\n",
    "values = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    " # configure the model\n",
    " lr = LogisticRegression(labelCol=\"isFraud\", featuresCol=\"my_features\", elasticNetParam=i)\n",
    " # configure the evaluators and metrics\n",
    " evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    " metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    " # fit model on the training dataset\n",
    " modelLR = lr.fit(train)\n",
    " # predictions on the training dataset\n",
    " predictions = modelLR.transform(train)\n",
    " # evaluate on the train dataset\n",
    " train_accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    " train_scores.append(train_accuracy)\n",
    " # evaluate on the test dataset\n",
    " predictions = modelLR.transform(test)\n",
    " test_accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    " test_scores.append(test_accuracy)\n",
    " # summarize progress\n",
    " print('>%d, train: %.3f, test: %.3f' % (i, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of train and test scores vs tree depth\n",
    "\n",
    "mpt.plot(values, train_scores, '-o', label='Train')\n",
    "mpt.plot(values, test_scores, '-o', label='Test')\n",
    "mpt.legend()\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in the training dataset\n",
    "\n",
    "predictions = modelLR.transform(train)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_rf = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_rf = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_rf = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_rf)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_rf)\n",
    "\n",
    "print(f\"Precsion: \", precision_rf)\n",
    "\n",
    "print(f\"Recall: \", recall_rf)\n",
    "\n",
    "\n",
    "# ROC curve : visualization\n",
    "\n",
    "\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n",
    "print(\"The ROC curve is:\")\n",
    "trainingSummary = modelLR.summary\n",
    "lrROC = trainingSummary.roc.toPandas()\n",
    "\n",
    "mpt.plot(lrROC['FPR'],lrROC['TPR'])\n",
    "mpt.ylabel('False Positive Rate')\n",
    "mpt.xlabel('True Positive Rate')\n",
    "mpt.title('ROC Curve - Logistic Regression')\n",
    "mpt.show()\n",
    "\n",
    "\n",
    "# Precision Recall curve : visualization\n",
    "\n",
    "print(\"The Precision Recall curve is:\")\n",
    "pr = trainingSummary.pr.toPandas()\n",
    "mpt.plot(pr['recall'],pr['precision'])\n",
    "mpt.ylabel('Precision')\n",
    "mpt.xlabel('Recall')\n",
    "mpt.title('Precision Recall curve - Logistic Regression')\n",
    "mpt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################  Visualization ROC CURVE using Sklearn ############################################################\n",
    "\n",
    "########################### ROC graphic ###########################\n",
    "\n",
    "#preds_and_labels = predictions.select(\"prediction\", \"isFraud\")\n",
    "#preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "#preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "#preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "#metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "\n",
    "######################### Visualization ###########################\n",
    "\n",
    "#fpr = dict()\n",
    "#tpr = dict()\n",
    "#roc_auc = dict()\n",
    "\n",
    "#y_test = [i[1] for i in preds_and_labels_collect]\n",
    "#y_score = [i[0] for i in preds_and_labels_collect]\n",
    "\n",
    "#fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "#roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#print(\"The graphic ROC curve is:\")\n",
    "\n",
    "#mpt.figure(figsize=(5,4))\n",
    "#mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "#mpt.plot([0,1],[0,1],'k--')\n",
    "#mpt.xlim([0.0,1.0])\n",
    "#mpt.ylim([0.0,1.05])\n",
    "#mpt.xlabel('False Positive Rate')\n",
    "#mpt.ylabel('True Positive Rate')\n",
    "#mpt.title('ROC Curve - Logistic Regression')\n",
    "#mpt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in the test dataset\n",
    "\n",
    "predictions = modelLR.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_rf = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_rf = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_rf = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_rf)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_rf)\n",
    "\n",
    "print(f\"Precsion: \", precision_rf)\n",
    "\n",
    "print(f\"Recall: \", recall_rf)\n",
    "\n",
    "\n",
    "# fill the results list\n",
    "\n",
    "auc_roc.append(auc_rf)\n",
    "\n",
    "accuracy.append(accuracy_rf)\n",
    "\n",
    "recall.append(recall_rf)\n",
    "\n",
    "precision.append(precision_rf)\n",
    "\n",
    "name_model.append(\"Logistic Regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################  Visualization ROC CURVE using Sklearn ############################################################\n",
    "\n",
    "########################### ROC graphic ###########################\n",
    "\n",
    "#preds_and_labels = predictions.select(\"prediction\", \"isFraud\")\n",
    "#preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "#preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "#preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "#metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "\n",
    "######################### Visualization ###########################\n",
    "\n",
    "#fpr = dict()\n",
    "#tpr = dict()\n",
    "#roc_auc = dict()\n",
    "\n",
    "#y_test = [i[1] for i in preds_and_labels_collect]\n",
    "#y_score = [i[0] for i in preds_and_labels_collect]\n",
    "\n",
    "#fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "#roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#print(\"The graphic ROC curve is:\")\n",
    "\n",
    "#mpt.figure(figsize=(5,4))\n",
    "#mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "#mpt.plot([0,1],[0,1],'k--')\n",
    "#mpt.xlim([0.0,1.0])\n",
    "#mpt.ylim([0.0,1.05])\n",
    "#mpt.xlabel('False Positive Rate')\n",
    "#mpt.ylabel('True Positive Rate')\n",
    "#mpt.title('ROC Curve - Logistic Regression')\n",
    "#mpt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "predictions = modelLR.transform(test)\n",
    "\n",
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients and intercept\n",
    "\n",
    "##print('The coefficients are:', modelLR.coefficients)\n",
    "##print('The independent term is:', modelLR.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "\n",
    "##model_LR = CrossValidatorModel.load(\"tuningLRProof\")\n",
    "##summary_lr = model_LR.bestModel\n",
    "\n",
    "##print('The area under ROC:',summary_lr)\n",
    "##print('The ROC is:',summary_lr.roc.show())\n",
    "##print('pr is',summary_lr.pr.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##summary_lr.objectiveHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of the logistic regression model using the test dataset\n",
    "\n",
    "predictions.select('isfraud','prediction','probability').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### model from the cross validation ##############################################\n",
    "################################################### (uncomment if you want to use this model ) #######################################\n",
    "\n",
    "#modelDT = CrossValidatorModel.load(\"tuningDT\")\n",
    "#print(modelDT.explainParams())\n",
    "#type(modelDT)\n",
    "#modelDT.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol=\"my_features\", labelCol=\"isFraud\")\n",
    "\n",
    "modelDT = dt.fit(train)\n",
    "\n",
    "predictions = modelDT.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminary evaluation\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_rf = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_rf)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_rf)\n",
    "\n",
    "print(f\"Precsion: \", precision_dt)\n",
    "\n",
    "print(f\"Recall: \", recall_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the \"values list\" you can change the values range of the hyperparameters to test with. Besides, you need to set the hyperparameter name in the argument of the \"Decision Tree\" model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of the overfitting\n",
    "\n",
    "\n",
    "# define the tree depths to evaluate\n",
    "values = [i for i in range(1,16)]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    " # configure the model\n",
    " dt = DecisionTreeClassifier(featuresCol=\"my_features\",labelCol=\"isFraud\",maxDepth=i)\n",
    " # configure the evaluators and metrics\n",
    " evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    " metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    " # fit model on the training dataset\n",
    " modelDT = dt.fit(train)\n",
    " # predictions on the training dataset\n",
    " predictions = modelDT.transform(train)\n",
    " # evaluate on the train dataset\n",
    " train_accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    " train_scores.append(train_accuracy)\n",
    " # evaluate on the test dataset\n",
    " predictions = modelDT.transform(test)\n",
    " test_accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    " test_scores.append(test_accuracy)\n",
    " # summarize progress\n",
    " print('>%d, train: %.3f, test: %.3f' % (i, train_accuracy, test_accuracy))\n",
    "\n",
    "\n",
    "# plot of train and test scores vs tree depth\n",
    "mpt.plot(values, train_scores, '-o', label='Train')\n",
    "mpt.plot(values, test_scores, '-o', label='Test')\n",
    "mpt.legend()\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in the training dataset\n",
    "\n",
    "predictions = modelDT.transform(train)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_dt = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_dt = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_dt)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_dt)\n",
    "\n",
    "print(f\"Precsion: \", precision_dt)\n",
    "\n",
    "print(f\"Recall: \", recall_dt)\n",
    "\n",
    "# ROC curve : visualization\n",
    "\n",
    "print(\"The ROC curve is:\")\n",
    "trainingSummary = modelDT.summary\n",
    "lrROC = trainingSummary.roc.toPandas()\n",
    "\n",
    "mpt.plot(lrROC['FPR'],lrROC['TPR'])\n",
    "mpt.ylabel('False Positive Rate')\n",
    "mpt.xlabel('True Positive Rate')\n",
    "mpt.title('ROC Curve - Decision Tree')\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n",
    "mpt.show()\n",
    "\n",
    "\n",
    "# Precision Recall curve : visualization\n",
    "\n",
    "print(\"The Precision Recall curve is:\")\n",
    "pr = trainingSummary.pr.toPandas()\n",
    "mpt.plot(pr['recall'],pr['precision'])\n",
    "mpt.ylabel('Precision')\n",
    "mpt.xlabel('Recall')\n",
    "mpt.title('Precision Recall curve - Decision Tree')\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################  Visualization ROC CURVE using Sklearn ############################################################\n",
    "\n",
    "########################### ROC graphic ###########################\n",
    "\n",
    "#preds_and_labels = predictions.select(\"prediction\", \"isFraud\")\n",
    "#preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "#preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "#preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "#metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "\n",
    "######################### Visualization ###########################\n",
    "\n",
    "#fpr = dict()\n",
    "#tpr = dict()\n",
    "#roc_auc = dict()\n",
    "\n",
    "#y_test = [i[1] for i in preds_and_labels_collect]\n",
    "#y_score = [i[0] for i in preds_and_labels_collect]\n",
    "\n",
    "#fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "#roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#print(\"The graphic ROC curve is:\")\n",
    "\n",
    "#mpt.figure(figsize=(5,4))\n",
    "#mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "#mpt.plot([0,1],[0,1],'k--')\n",
    "#mpt.xlim([0.0,1.0])\n",
    "#mpt.ylim([0.0,1.05])\n",
    "#mpt.xlabel('False Positive Rate')\n",
    "#mpt.ylabel('True Positive Rate')\n",
    "#mpt.title('ROC Curve - Decision Tree')\n",
    "#mpt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in the test dataset\n",
    "\n",
    "predictions = modelDT.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_dt = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_dt = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_dt)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_dt)\n",
    "\n",
    "print(f\"Precision: \", precision_dt)\n",
    "\n",
    "print(f\"Recall: \", recall_dt)\n",
    "\n",
    "# full the results list\n",
    "\n",
    "auc_roc.append(auc_dt)\n",
    "\n",
    "accuracy.append(accuracy_dt)\n",
    "\n",
    "recall.append(recall_dt)\n",
    "\n",
    "precision.append(precision_dt)\n",
    "\n",
    "name_model.append(\"Decision Tree\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################  Visualization ROC CURVE using Sklearn ############################################################\n",
    "\n",
    "########################### ROC graphic ###########################\n",
    "\n",
    "#preds_and_labels = predictions.select(\"prediction\", \"isFraud\")\n",
    "#preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "#preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "#preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "#metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "\n",
    "######################### Visualization ###########################\n",
    "\n",
    "#fpr = dict()\n",
    "#tpr = dict()\n",
    "#roc_auc = dict()\n",
    "\n",
    "#y_test = [i[1] for i in preds_and_labels_collect]\n",
    "#y_score = [i[0] for i in preds_and_labels_collect]\n",
    "\n",
    "#fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "#roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#print(\"The graphic ROC curve is:\")\n",
    "\n",
    "#mpt.figure(figsize=(5,4))\n",
    "#mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "#mpt.plot([0,1],[0,1],'k--')\n",
    "#mpt.xlim([0.0,1.0])\n",
    "#mpt.ylim([0.0,1.05])\n",
    "#mpt.xlabel('False Positive Rate')\n",
    "#mpt.ylabel('True Positive Rate')\n",
    "#mpt.title('ROC Curve - Logistic Regression')\n",
    "#mpt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of the decision tree model using the test dataset\n",
    "\n",
    "predictions = modelDT.transform(test)\n",
    "\n",
    "print(\"The predictions are:\")\n",
    "\n",
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### model from the cross validation ##############################################\n",
    "################################################### (uncomment if you want to use this model ) #######################################\n",
    "\n",
    "#modelNB = CrossValidatorModel.load(\"tuningNB\")\n",
    "#print(modelNB.explainParams())\n",
    "#type(modelNB)\n",
    "#modelNB.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol=\"my_features\", labelCol=\"isFraud\")\n",
    "\n",
    "modelNB = nb.fit(train)\n",
    "\n",
    "predictions = modelNB.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nb.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminary evaluation\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_rf = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_rf = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_rf)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the \"values list\" you can change the values range of the hyperparameters to test with. Besides, you need to set the hyperparameter name in the argument of the \"Naive Bayes\" model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of the overfitting\n",
    "\n",
    "# define the tree depths to evaluate\n",
    "values = [i for i in range(0, 11)]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "# evaluate a decision tree for each depth\n",
    "for i in values:\n",
    " # configure the model\n",
    " lr = NaiveBayes(labelCol=\"isFraud\", featuresCol=\"my_features\", smoothing=i)\n",
    " # configure the evaluators and metrics\n",
    " evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    " metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    " # fit model on the training dataset\n",
    " modelLR = lr.fit(train)\n",
    " # predictions on the training dataset\n",
    " predictions = modelLR.transform(train)\n",
    " # evaluate on the train dataset\n",
    " train_accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    " train_scores.append(train_accuracy)\n",
    " # evaluate on the test dataset\n",
    " predictions = modelLR.transform(test)\n",
    " test_accuracy = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    " test_scores.append(test_accuracy)\n",
    " # summarize progress\n",
    " print('>%d, train: %.3f, test: %.3f' % (i, train_accuracy, test_accuracy))\n",
    "\n",
    " # plot of train and test scores vs tree depth\n",
    "\n",
    "mpt.plot(values, train_scores, '-o', label='Train')\n",
    "mpt.plot(values, test_scores, '-o', label='Test')\n",
    "mpt.legend()\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in the training dataset\n",
    "\n",
    "predictions = modelNB.transform(train)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_dt = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_dt = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_dt)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_dt)\n",
    "\n",
    "print(f\"Precsion: \", precision_dt)\n",
    "\n",
    "print(f\"Recall: \", recall_dt)\n",
    "\n",
    "\n",
    "# ROC curve : visualization\n",
    "\n",
    "print(\"The ROC curve is:\")\n",
    "trainingSummary = modelNB.summary\n",
    "lrROC = trainingSummary.roc.toPandas()\n",
    "\n",
    "mpt.plot(lrROC['FPR'],lrROC['TPR'])\n",
    "mpt.ylabel('False Positive Rate')\n",
    "mpt.xlabel('True Positive Rate')\n",
    "mpt.title('ROC Curve - Naive Bayes')\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n",
    "mpt.show()\n",
    "\n",
    "\n",
    "# Precision Recall curve : visualization\n",
    "\n",
    "print(\"The Precision Recall curve is:\")\n",
    "pr = trainingSummary.pr.toPandas()\n",
    "mpt.plot(pr['recall'],pr['precision'])\n",
    "mpt.ylabel('Precision')\n",
    "mpt.xlabel('Recall')\n",
    "mpt.title('Precision Recall curve - Naive Bayes')\n",
    "mpt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################  Visualization ROC CURVE using Sklearn ############################################################\n",
    "\n",
    "########################### ROC graphic ###########################\n",
    "\n",
    "#preds_and_labels = predictions.select(\"prediction\", \"isFraud\")\n",
    "#preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "#preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "#preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "#metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "\n",
    "######################### Visualization ###########################\n",
    "\n",
    "#fpr = dict()\n",
    "#tpr = dict()\n",
    "#roc_auc = dict()\n",
    "\n",
    "#y_test = [i[1] for i in preds_and_labels_collect]\n",
    "#y_score = [i[0] for i in preds_and_labels_collect]\n",
    "\n",
    "#fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "#roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#print(\"The graphic ROC curve is:\")\n",
    "\n",
    "#mpt.figure(figsize=(5,4))\n",
    "#mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "#mpt.plot([0,1],[0,1],'k--')\n",
    "#mpt.xlim([0.0,1.0])\n",
    "#mpt.ylim([0.0,1.05])\n",
    "#mpt.xlabel('False Positive Rate')\n",
    "#mpt.ylabel('True Positive Rate')\n",
    "#mpt.title('ROC Curve - Naive Bayes')\n",
    "#mpt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in the test dataset\n",
    "\n",
    "predictions = modelNB.transform(test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"isFraud\")\n",
    "\n",
    "auc_dt = evaluator.evaluate(predictions)\n",
    "\n",
    "# Accuracy, Precision and Recall\n",
    "\n",
    "metrics = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"isFraud\")\n",
    "\n",
    "accuracy_dt = metrics.evaluate(predictions, {metrics.metricName:\"accuracy\"})\n",
    "\n",
    "precision_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedPrecision\"})\n",
    "\n",
    "recall_dt = metrics.evaluate(predictions, {metrics.metricName:\"weightedRecall\"})\n",
    "\n",
    "print(f\"AUC-ROC: \", auc_dt)\n",
    "\n",
    "print(f\"Accuracy: \", accuracy_dt)\n",
    "\n",
    "print(f\"Precsion: \", precision_dt)\n",
    "\n",
    "print(f\"Recall: \", recall_dt)\n",
    "\n",
    "# full the results list\n",
    "\n",
    "auc_roc.append(auc_dt)\n",
    "\n",
    "accuracy.append(accuracy_dt)\n",
    "\n",
    "recall.append(recall_dt)\n",
    "\n",
    "precision.append(precision_dt)\n",
    "\n",
    "name_model.append(\"Naive Bayes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################  Visualization ROC CURVE using Sklearn ############################################################\n",
    "\n",
    "########################### ROC graphic ###########################\n",
    "\n",
    "#preds_and_labels = predictions.select(\"prediction\", \"isFraud\")\n",
    "#preds_and_labels_collect = preds_and_labels.collect()\n",
    "\n",
    "#preds_and_labels_list = [ (float(i[0][0]), 1.0 - float(i[1])) for i in preds_and_labels_collect  ]\n",
    "#preds_and_labels = sc.parallelize(preds_and_labels_list)\n",
    "\n",
    "#metrics = BinaryClassificationMetrics(preds_and_labels)\n",
    "\n",
    "\n",
    "######################### Visualization ###########################\n",
    "\n",
    "#fpr = dict()\n",
    "#tpr = dict()\n",
    "#roc_auc = dict()\n",
    "\n",
    "#y_test = [i[1] for i in preds_and_labels_collect]\n",
    "#y_score = [i[0] for i in preds_and_labels_collect]\n",
    "\n",
    "#fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "\n",
    "#roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#print(\"The graphic ROC curve is:\")\n",
    "\n",
    "#mpt.figure(figsize=(5,4))\n",
    "#mpt.plot(fpr, tpr, label='ROC curve' % roc_auc)\n",
    "#mpt.plot([0,1],[0,1],'k--')\n",
    "#mpt.xlim([0.0,1.0])\n",
    "#mpt.ylim([0.0,1.05])\n",
    "#mpt.xlabel('False Positive Rate')\n",
    "#mpt.ylabel('True Positive Rate')\n",
    "#mpt.title('ROC Curve - Naive Bayes')\n",
    "#mpt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of the naive bayes model using the test dataset\n",
    "\n",
    "predictions = modelNB.transform(test)\n",
    "\n",
    "predictions.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s check out the Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_and_labels = predictions.select([\"prediction\",\"isFraud\"])\n",
    "preds_and_labels = preds_and_labels.withColumn(\"isFraud\", f.col(\"isFraud\").cast(FloatType())).orderBy(\"prediction\")\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(\"The Confusion Matrix is:\")\n",
    "\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Evaluation and Selection of the model\n",
    "\n",
    "We´ll evaluate the models using the metrics used in the previous step and we´ll select the model with the best performance. As first step, let´s create a dictionary with the results of every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Name_Model': name_model,\n",
    "    'Accuracy':accuracy,\n",
    "    'Precision':precision,\n",
    "    'Recall':recall,\n",
    "    'AUC_ROC':auc_roc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let´s create a pandas dataframe with the results dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('Name_Model', inplace=True)\n",
    "#results_df.set_index(\"Name_Model\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let´s visualize these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"results_df\" dataframe\n",
    "\n",
    "colors = ['#0077b6','#CDDBF3','#9370DB','#DDA0DD']\n",
    "results_df.plot(kind='bar', figsize=(12,6), colormap='viridis', rot=0)\n",
    "mpt.title('Comparison of metrics per model')\n",
    "mpt.xlabel('Models')\n",
    "mpt.ylabel('Score')\n",
    "mpt.legend(title = 'Metrics')\n",
    "mpt.tight_layout\n",
    "mpt.show()\n",
    "\n",
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose of the \"results_df\" dataframe\n",
    "\n",
    "results_df = results_df.T\n",
    "colors = ['#0077b6','#CDDBF3','#9370DB','#DDA0DD']\n",
    "results_df.plot(kind='bar', figsize=(12,6), colormap='viridis', rot=0)\n",
    "mpt.title('Comparison of metrics per model')\n",
    "mpt.xlabel('Metrics')\n",
    "mpt.ylabel('Score')\n",
    "mpt.legend(title = 'Models')\n",
    "mpt.tight_layout\n",
    "mpt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### + The fraud is related with the variables \"Payment\", \"CC\" and \"CM\".\n",
    "### + Random Forest represents the machine learning algorithm with the highest reliability without \"overfitting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: Random Forest\n",
    "\n",
    "modelRF.save(\"modelRF\")\n",
    "\n",
    "# model: Logistic Regression\n",
    "\n",
    "modelLR.save(\"modelLR\")\n",
    "\n",
    "# model: Decision Tree\n",
    "\n",
    "modelDT.save(\"modelDT\")\n",
    "\n",
    "# model: Naive Bayes\n",
    "\n",
    "modelNB.save(\"modelNB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: Random Forest\n",
    "\n",
    "loaded_model_RF =  CrossValidatorModel.load(\"modelRF\")\n",
    "\n",
    "# model: Logistic Regression\n",
    "\n",
    "loaded_model_LR = CrossValidatorModel.load(\"modelLR\")\n",
    "\n",
    "# model: Decision Tree\n",
    "\n",
    "loaded_model_LR = CrossValidatorModel.load(\"modelDT\")\n",
    "\n",
    "# model: Naive Bayes\n",
    "\n",
    "loaded_model_LR = CrossValidatorModel.load(\"modelNB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
